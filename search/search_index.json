{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\/\\s\\-\\.]+"},"docs":[{"location":"","text":"","title":"\u9996\u9875"},{"location":"about-analytics-cookies/","text":"By using this website, you are confirming that you accept our use of cookies for the purposes of collecting anonymous usage data to improve the user experience and content on the website. Continue to sections below for details about knative.dev, or use the following resources to learn about cookies in general: Learn about basic site analytics usuage at: https://www.cookiechoices.org/ You can also watch a video about how Google uses cookies . What are cookies? \u00b6 Cookies are small pieces of data that is sent from a website and stored on the user's computer by the user's web browser while that user is browsing. Cookies were designed to be a reliable mechanism for websites to remember stateful information or to record the user's browsing activity. For example, a cookie can be used to determine if a user has already visited a page and whether or not that user has already been presented with a certain notice or announcement. As a rule, cookies will make your browsing experience better, like ensuring that those same notices or announcements don't pop-up over every page. Many sites use cookies and analytics. You might have agreed to their usage when you created an account, used their service, or viewed their webpage (cookie consent notice). However, you can choose to disable cookies on knative.dev, or any other website. The most effective way to do this is to disable cookies in your browser but you can loose some website behavior. What cookies are used on knative.dev \u00b6 We use Google Analytics tracking on knative.dev. These cookies are used to store information, such as what pages you visited and for how long, whether you have been to the site before, and what site referred you to the web page. We also learn about what types of content and topic areas you are interested in, including what content or sections never get viewed or used. These cookies contain no personally identifiable information (PII) but they will use your computer\u2019s IP address. For more information about the data collected, view our Privacy Policy . If you prefer to view the Knative docs from within the knative/docs GitHub repository, view details about their cookies and tracking at GitHub Privacy Statement . Options for opting out \u00b6 Use the following options to prevent your data from being shared with websites. Opt-out Browser Add-on \u00b6 You can use the Google Analytics Opt-out Browser Add-on to prevent your usage data from being sent to Google Analytics. Learn more, including how to install the add-on . Disabling cookies \u00b6 You can manually restrict the use of cookies in your web browser. General details about how to control cookie usage in your web browser are available at: Apple Safari Google Chrome Microsoft Internet Explorer Mozilla Firefox","title":"Learn about Google Analytics cookies"},{"location":"about-analytics-cookies/#what-are-cookies","text":"Cookies are small pieces of data that is sent from a website and stored on the user's computer by the user's web browser while that user is browsing. Cookies were designed to be a reliable mechanism for websites to remember stateful information or to record the user's browsing activity. For example, a cookie can be used to determine if a user has already visited a page and whether or not that user has already been presented with a certain notice or announcement. As a rule, cookies will make your browsing experience better, like ensuring that those same notices or announcements don't pop-up over every page. Many sites use cookies and analytics. You might have agreed to their usage when you created an account, used their service, or viewed their webpage (cookie consent notice). However, you can choose to disable cookies on knative.dev, or any other website. The most effective way to do this is to disable cookies in your browser but you can loose some website behavior.","title":"What are cookies?"},{"location":"about-analytics-cookies/#what-cookies-are-used-on-knativedev","text":"We use Google Analytics tracking on knative.dev. These cookies are used to store information, such as what pages you visited and for how long, whether you have been to the site before, and what site referred you to the web page. We also learn about what types of content and topic areas you are interested in, including what content or sections never get viewed or used. These cookies contain no personally identifiable information (PII) but they will use your computer\u2019s IP address. For more information about the data collected, view our Privacy Policy . If you prefer to view the Knative docs from within the knative/docs GitHub repository, view details about their cookies and tracking at GitHub Privacy Statement .","title":"What cookies are used on knative.dev"},{"location":"about-analytics-cookies/#options-for-opting-out","text":"Use the following options to prevent your data from being shared with websites.","title":"Options for opting out"},{"location":"about-analytics-cookies/#opt-out-browser-add-on","text":"You can use the Google Analytics Opt-out Browser Add-on to prevent your usage data from being sent to Google Analytics. Learn more, including how to install the add-on .","title":"Opt-out Browser Add-on"},{"location":"about-analytics-cookies/#disabling-cookies","text":"You can manually restrict the use of cookies in your web browser. General details about how to control cookie usage in your web browser are available at: Apple Safari Google Chrome Microsoft Internet Explorer Mozilla Firefox","title":"Disabling cookies"},{"location":"about/testimonials/","text":"Enterprise-grade Serverless on your own terms. Understanding Knative \"If Kubernetes is an electrical grid, then Knative is its light switch.\" \u2014Kelsey Hightower, Google Cloud Platform Knative is an automated system that helps development teams manage and maintain processes in Kubernetes. Its purpose is to simplify, automate, and monitor deployments of Kubernetes so teams spend less time on maintenance and more time on app development and projects. Knative takes over repetitive and time-intensive tasks while removing obstacles and delays. Knative does this through two features. The first is Knative Eventing. Eventing allows developers to set up detailed actions triggered by specific events within a broader environment. The second is Knative Serving, which automatically manages the creation and scaling of services through Kubernetes, including scaling down to zero. Each of these features aims to free up resources that teams would otherwise spend managing systems. They also save businesses money by reacting to conditions in real \u2014 time. Meaning, companies only pay for the resources they are using, not the ones they might use. Scale to Zero is a feature of Knative Serving that automatically turns off services running in containers when there is no demand for them. Instead of running programs on standby, they can be turned off and turned back on when needed again. Scale to zero reduces costs over time and helps manage technical resources. The core idea behind Knative is to allow teams to harness the power of serverless application deployment. Serverless refers to managing cloud-based servers and virtual machines, often hosted on platforms like AWS, Google Cloud, Microsoft Azure, and more. Serverless is a great option for companies looking to move away from the costly endeavor of managing their own servers and infrastructure. \"I often think of Knative as part of 'Serverless 2.0.' It combines the good things about serverless with a loosening of constraints around execution time and availability of resources.\" -Michael Behrendt, Distinguished Engineer and Chief Architect of Serverless and Cloud Functions for IBM. IBM is a committed sponsor of Knative Knative in the broader ecosystem To understand Knative more fully, it is important to know that it exists in a larger ecosystem of services that work together. For example, Knative acts as a framework on top of Kubernetes that helps build a serverless platform. Kubernetes itself is a system that orchestrates the creation and running of containers used in app deployment, scaling, and more. Those contain ers can run anything, from simple tools written in python to complex Al systems. Containers were developed to help tackle the problem of complexity. As development teams build software products, they create massive codebases. Left unorganized, those codebases can become gigantic and confusing-even for those who make them. Containers solve this problem by breaking codebases into small, self-contained processes that can interact to do work. They also help developers manage complex webs of dependencies like APIs and databases. These containers are easier to maintain for teams looking to work fast while maintaining best practices. Knative's value in DevOps DevOps promises effective application development processes with faster deployments and fewer bugs. While Kubernetes helps facilitate this, it can produce significant complexity. Achieving value at scale with Kubernetes traditionally involves teams developing specialized knowledge. Knative cuts down on that by providing a serverless experience that removes the need for all development team members to know or understand the ins and outs of Kubernetes. \"What we are doing with Knative is to provide a developer experience that makes it easier to focus on code. Cloud developers focus on the business problems they are solving without having to coordinate or wait for approvals from platform teams to scale their apps. Knative is a framework that helps automate platform capabilities so your apps can scale as if they were running on Serverless compute.\" -Aparna Sinha, Director of Product Management, Google Tangible benefits of Knative for teams It has always been true that organizations need to develop and innovate faster than their competition while deploying products with fewer flaws. However, being bogged down by configuring networks and operating systems harms developer productivity and morale. Developers want to create things, and Knative helps them do that. \"The amount of internal work needed to use Knative is minuscule\" -Tilen Kav\u010di\u010d, Backend Developer for Outfit7, which uses Knative for key backend system The advantage of Open Source Open source has been a powerful resource for creating business solutions for decades. Kubernetes and Knative are now paving the way for that relationship to become stronger. Each project has significant support from some of the biggest names in tech including IBM, Google, Redhat, and VMware. The Kubernetes and Knative ecosystem consists of widely adopt ed projects that are proven across many installations for a multitude of uses. The open-source foundation of Knative means that anyone using the platform can participate in the community to get help, solve problems, and influence the direction of deployment for future versions. Find out more > Case Studies: Read about organizations using Knative, from platform developers to proven companies to innovative startups > Check the getting started guide to get up and running with Knative in an afternoon > Join the Knative Slack to talk to the community","title":"Testimonials"},{"location":"about/case-studies/deepc/","text":"deepc Case Study \u201cIt should be possible for somebody with an algorithm to have it on the platform in an hour\u201d -- Andrew Webber, Senior Software Engineer for deepc AI Startup deepc Connects Researchers to Radiologists with Knative Eventing deepc is a startup at the cutting edge, bringing advanced data techniques and artificial intelligence to healthcare. The German company helps radiologists access better diagnostics and resources through cloud-based technology. Their goal is to elevate treatment quality across the board while creating more efficiency and improvement in medical settings. This revolution in technology comes with hefty challenges. Care systems are highly regulated, making patient privacy and safety a top priority for deepc. Doctors and medical staff also demand that new technologies be reliable and stable when lives are on the line. Rising to the challenge deepc has risen to meet these challenges through carefully architected solutions, using tools like Knative Eventing to their full potential. Their product helps radiologists access a wide selection of artificial intelligence (AI) programs that analyze imaging, like x-rays and MRIs. The data generated from these AI programs help radiologists make more accurate diagnoses. The deepc workflow The radiologist uploads the image into deepcOS, initially to a virtual machine within the hospital IT infrastructure containing the deepcOS client application. After making a series of selections, deepcOS identifies the proper AI to use. It then removes the patient information from the scans before encrypting the data. deepcOS sends that data to the cloud-based deepc AI platform. This platform does the heavy lifting in providing the computing power the AI algorithms need to do their work. After the program finishes, the results are sent back. Finally, the data is reassociated with the patient, and the radiologist can take action based on the results. Critically, patient information always remains on-premises in the hospital and is not transmitted to deepc servers. A Knative-powered process The deepcOS workflow builds on a sophisticated implementation of Knative Eventing. Knative Eventing allows teams to deploy event-driven architecture with serverless applications quickly. In conjunction with Knative Serving, deepc resources and programs scale up and down automatically based on specific event triggers laid out by the developers. Knative takes care of the management, so the process does not need to wait for a person to take action. When data is sent to deepc's cloud-based platform, Knative emits an event that triggers a specific AI. After one is selected, Knative starts a container environment for the program to run. Some AI programs may only need one container. Others may require multiple, running parallel or in sequence. In the case of multiple containers, the deepc team created workflows using Knative Eventing to coordinate the complex processes. After the process finishes and provides the output for the radiologist, Knative triggers stopping the active containers. \"Knative gives us a foundation of consistency,\" said Andrew Webber, Senior Software Engineer. Bridging between legacy and advanced systems The platform makes available AIs developed by leading global companies and researchers.. Knative has also allowed integration with the work of independent researchers through an SDK implementation to radiologists. They don\u2019t need to be Kubernetes experts or take days to bring their work to patients through deepc\u2019s platform. \u201cIt should be possible for somebody with an algorithm to have it on the platform in an hour\u201d said Webber. Some implementations are more complex. They use legacy technology that does not fit into a standard container or they have unique architectures that require OS-level configuration. deepc has built out APIs and virtual machines that connect those technologies to their own cloud-based platform and still integrate with the Knative Eventing workflow. This approach ensures those programs work flawlessly within the system. The case for business The choice to develop their platform around Knative has had several business benefits for the startup. One of the most complicated aspects of growing a company is scaling. Many technology companies find their developers start scrambling when more customers are onboarded, uncovering new bugs and other issues. However, because of the nature of Knative, this is less of a problem for deepc. Knative's combination of automation and serverless methods means as more customers are onboarded, deepc will not need to build out more resources - it will all happen automatically. Knative has also allowed the startup to add real value to customers using their technology. For example, because many applications used by radiologists are built by different companies, medical professionals have had to interact with disparate systems and procedures. deepc provides access to the work of many researchers on one platform, ending complicated processes for professionals on the ground. Healthcare systems get simple, unified billing. Knative has helped deepc create a robust case for customers to use their platform. Looking forward deepc has already done amazing things as a company, with many more features planned. The company is a model for how Knative can help any organization build an impressive technical architecture capable of addressing some of today's most complex problems. Using features provided by Knative has enabled them to pioneer what is possible. Find out more Getting started with Knative Knative Serving Knative Eventing","title":"deepc"},{"location":"about/case-studies/outfit7/","text":"Outfit7 Case Study \"The community support is really great. The hands-on experience with Knative was so impressive. On the Slack channel, we got actual engineers to answer our questions\" -- Tilen Kav\u010di\u010d, Software Engineer for Outfit7 Game maker Outfit7 automates high-performance ad bidding with Knative Serving Since its founding in 2009, the mobile gaming company Outfit7 has seen astronomical growth\u2014garnering over 17 billion downloads and over 85 billion video views last year. Outfit7 was in the Top 5 game publishers list on iOS and Google Play worldwide by the number of game downloads for 6 consecutive years (2015 - 2020). With their latest launch, My Talking Angela 2, they were number 1 by global games downloads in July, August, and September (more than 120 million downloads). The success of the well-known game developer behind massive hits like the Talking Tom & Friends franchise and stand-alone titles like Mythic Legends created large-scale challenges. With up to 470 million monthly active users, 20 thousand server requests per second, and terabytes of data generated daily, they needed a stable, high-performance solution. They turned to a Knative and Kubernetes solution to optimize real-time bidding ad sales in a way that can automatically scale up and down as needed. They were able to develop a system so easy to maintain that it freed up two software engineers who are now able to work on more important tasks, like optimizing backend costs and adding new game features. High performance in-app bidding Ad sales are an important revenue stream of Outfit7. The team needed to walk a careful balance: sell the space for the highest bid, use technical resources efficiently, and make sure ads are served to players quickly. To achieve this they decided to adopt an in-app bidding approach. The Outfit7 user base generates around 8,000 ad-related requests per second. With so many users spread worldwide, the amount of these requests can drop or surge depending on all sorts of factors. Not just predictable things like the time of day, but current events can suddenly create traffic. The pandemic saw their usage soar, for instance. To manage the process in-house, the team needed to be able to test and deploy very efficiently. \"There were two specific use-cases we wanted to cover,\" explained Luka Draksler, backend engineer in the ad technology department at Outfit7. \"One was to have the ability to do zero downtime canary deployments using an automatic gradual rollout process. This works in a way that the new version of the software is deployed using a continuous deployment pipeline with a small amount of traffic first. If everything checks out, all production traffic is migrated to the new version. In the worst case scenario (if requests start failing) traffic can be quickly migrated to the old version. The second use-case was the ability to have developers deploy versions to specific groups of users for instances of A/B testing and other use cases.\" The team decided to adopt Knative Serving as the backbone of their solution. Knative allowed Outfit7 to streamline deployments and cut down on development time. After being surprised at how easily they generated an internal proof of concept, the team saw that it could craft custom solutions tuned for their internal workflows\u2014without consuming valuable developer time. In addition, they could quickly configure A/B testing and deploy multiple versions of code simultaneously. Serverless solution Knative Serving gave Outfit7 access to a robust set of tools and features that allows their team to automate and monitor the deployment of applications to handle ad requests. When more requests are coming in, their system automatically spins up more containers that house the workers and tools. When these requests drop, unneeded containers shut down. Outfit7 only pays for the resources they require for the current load. Knative works as a layer installed on the top of Kubernetes. It brings the power of serverless workloads to the scalable capabilities of Kubernetes. Teams quickly spin up container-based applications without needing to consider the details of Kubernetes. Knative also simplifies project deployments to Kubernetes. Mitja Bezen\u0161ek, Lead Developer on Outfit7's backend team, estimated that the traditional development that Knative replaced would have required three full time engineers to maintain. Their new platform operates with minimal work and allows the developers to deploy updates at will. The open source community Outfit7's team was blown away by the supportive and helpful community around Knative. After discovering a problem with network scaling, the team was surprised by how easy it was to find answers and solutions. \"The community support is really great. The hands-on experience with Knative was so impressive. On the Slack channel, we got actual engineers to answer our questions\" -- Tilen Kav\u010di\u010d, Software Engineer for Outfit7 Sharing their story The great experience with Knative encouraged their team to share their experience with fellow companies and engineers at a local meetup. The presentation which included several live demos was a success, helping spawn another meet-up focused on the technology. \"Tilen showed them the demo and what it's all about,\" said Bezen\u0161ek. \"I hope we got them engaged going forward.\" Looking forward Outfit7 shows no signs of slowing down. \u201cAs we want to support our vision in expanding our games portfolio, we are always looking for new strategic partners who can accompany us on this path,\u201d added Helder Lopes, Head of R&D in Cyprus headquarters. The company plans to incorporate and adopt Knative into other back-end systems \u2013 taking advantage of the easier workflows that Knative offers. Find out more Getting started with Knative Knative Serving Knative Eventing","title":"Outfit7"},{"location":"about/case-studies/puppet/","text":"Puppet Case Study \"I'm a strong believer in working with open source projects. We've made contributions to numerous projects, including Tekton, Knative, Ambassador, and gVisor, all of which we depend on to make our product functional\" -- Noah Fontes, Senior Principal Software Engineer for Puppet Relay by Puppet Brings Workflows to Everything using Knative Puppet is a software company specializing in infrastructure automation. The company was founded in 2009 to solve some of the most complex problems for those who work in operations. Around 2019, the team noticed that cloud operations teams weren\u2019t able to effectively manage modern cloud-native applications because they were relying on manual workflow processes. The group saw an opportunity here to build out a platform to connect events triggered by modern architectures to ensure cloud environments remained secure, compliant, and cost effective. This is the story of how Relay , a cloud-native workflow automation platform, was created, and how Knative and Kubernetes modernize and super-charge business process automation. The glue for DevOps When Puppet first began exploring the power and flexibility of Knative to trigger their Tekton-based workflow execution engine, they weren't quite sure where their journey was going to take them. Knative offered an attractive feature set, so they began building and experimenting. They wanted to build an event-driven DevOps tool; they weren't interested in building just another continuous integration product. In addition, as they continued to explore, they realized that they wanted something flexible and not tied to just one vertical. Whatever they were building, it was not going to focus on just one market. As their target came into focus, they realized that the serverless applications and functions enabled by Knative Serving would be perfect for a cloud-based business process automation service. Out of this realization, they built Relay , a cloud workflow automation product that helps Cloud Ops teams solve the integration and eventing issues that arise as organizations adopt multiple clouds and SaaS products alongside legacy solutions. Containers and webhooks Containers and webhooks are key elements in the Relay architecture. Containers allow Puppet to offer a cloud-based solution where businesses can configure and deploy workflows as discrete business units. Since the containers provide self-contained environments, even legacy services and packages can be included. This proved to be an essential feature for business customers. Anything that can be contained in a Docker image, for example, can be part of a Relay workflow. \"We focused on containers because they provide isolation,\" explains Noah Fontes, Senior Principal Software Engineer for Puppet, \"Containers provide discrete units of execution, where users can decrease maintenance burden of complex systems.\" Allowing fully-configurable webhooks gives users the flexibility needed to incorporate business processes of all kinds. With webhooks, Relay can interact with nearly any web-based API to trigger rich, fully featured workflows across third party SaaS products, cloud services, web applications, and even system utilities. Knative Serving provides important infrastructure for Relay. It allows webhooks and services to scale automatically, even down to zero . This allows Relay to support pretty much any integration, including those used by only a small number of users. With autoscaling, those services don't consume resources while they are not being used. What is Knative Serving? Modern cloud-based applications deal with massive scaling challenges through several approaches. At the core of most of these is the use of containers: discrete computing units that run single applications, single services, or even just single functions. This approach is incredibly powerful, allowing services to scale the amount of resources they consume as demand dictates. However, while all of this sounds amazing, it can be difficult to manage and configure. One of the most successful solutions for delivering this advanced architecture is Knative Serving. This framework builds on top of Kubernetes to support the deployment and management of serverless applications, services, and functions. In particular, Knative Services focuses on being easy to configure, deploy, and manage. Workflow integrations The open architecture allows Relay to integrate dozens of different services and platforms into workflows. A look at the Relay integrations GitHub page provides a list of these integrations and demonstrates their commitment to the open source community. \"I'm a strong believer in working with open source projects. We've made contributions to numerous projects, including Tekton, Knative, Ambassador, and gVisor, all of which we depend on to make our product functional,\" says Fontes. Results: automated infrastructure management While Relay's infrastructure runs on Google Cloud Platform, its library of workflows, integrations, steps, and triggers includes services across all major cloud service providers. Relay customers can integrate across Microsoft Azure, AWS, and Oracle Cloud Infrastructure among others. By combining these integrations with SaaS offerings, it truly is becoming the Zapier of infrastructure management. \u201cOur customers have diverse needs for managing their workloads that are often best implemented as web APIs. Our product provides a serverless microservice environment powered by Knative that allows them to build this complex tooling without the management and maintenance overhead of traditional deployment architectures. We pass the cost savings on to them, and everyone is happier,\" said Fontes. Building and deploying Relay would not have been possible without the existing infrastructure offered by systems such as Knative and Tekton . Remarkably, Fontes' team never grew above eight engineers. Once they solidified their plan for Relay, they were able to bring it to production in just three months, says Fontes. \"Thanks to Knative, getting Relay out the door was easier than we thought it would be.\" said Noah Fontes, Senior Principal Software Engineer. Knative aims to make scalable, secure, stateless architectures available quickly by abstracting away the complex details of a Kubernetes installation and enabling developers to focus on what matters. Find out more Getting started with Knative Knative Serving Knative Eventing A Basic Introduction to Webhooks","title":"Puppet"},{"location":"community/","text":"Welcome to the Knative community! \u00b6 Knative is an open source project that anyone in the community can use, improve, and enjoy. We'd love you to join us! In this section: Contribute to Knative : helpful tips for how to get started contributing to Knative. About the Knative community : information about the Knative community and how it's run.","title":"Welcome to the community"},{"location":"community/#welcome-to-the-knative-community","text":"Knative is an open source project that anyone in the community can use, improve, and enjoy. We'd love you to join us! In this section: Contribute to Knative : helpful tips for how to get started contributing to Knative. About the Knative community : information about the Knative community and how it's run.","title":"Welcome to the Knative community!"},{"location":"community/about/","text":"About the Knative community \u00b6 This page provides links to documents for common Knative community practices and a description of Knative's audience. Community values \u00b6 This section links to documents about our values. Knative project values : shared goals and values for the community. Knative team values : the goals and values we hold as a team. Governance \u00b6 This section links to documents about how the Knative community is governed. Knative has public and recorded monthly community meetings. Each project has one or more working groups driving the project, and Knative has a single technical oversight committee monitoring the overall project. Governance : the Knative governance framework. Community roles : describes the roles individuals can assume within the Knative community such as member, approver, or working group lead. Working groups : provides information about our various working groups. Steering Committee (SC) : describes our steering committee. Technical Oversight Committee (TOC) : describes our technical oversight committee. Trademark Committee : describes our trademark committee. Annual reports : lists previous annual reports. Processes \u00b6 This section links to documents for common Knative community processes. At the moment, these practices (except for the formation of Working Groups and Lazy Consensus) are recommendations that individual working groups can choose to adopt, rather than requirements. Each working group should document their processes; either in their own repo or in a pointer to these docs. Reviewing and Merging Pull Requests : how we manage pull requests. Working group processes : how working groups operate. SC election process : elcection process for our steering committee. TOC election process : election process for our technical oversight committee. Repository Guidelines : how we create and remove core repositories. Sandbox repo process : how to create a repo in the knative-sandbox GitHub org. Feature tracks : outlines the process for adding non-trivial features. Golang policy : principles regarding the Golang version Knative tests and releases with. Release principles : release principles including information about support and feature phases. Release schedule : Knative past and future release dates. Sunsetting features : process to sunset features that are getting no apparent usage, but are time consuming to maintain. Community calendar \u00b6 The Knative community calendar ( iCal export file ) contains events that provide the opportunity to learn more about Knative and meet other users and contributors. This includes Working Group, Steering Committee, and other community meetings. Events don't have to be organized by the Knative project to be added to the calendar. If you want to add an event to the calendar please send an email to knative-steering@googlegroups.com or post to the #community channel in the Knative Slack workspace. Knative's audience \u00b6 Knative is designed for different personas: Developers \u00b6 Knative components offer developers Kubernetes-native APIs for deploying serverless-style functions, applications, and containers to an auto-scaling runtime. To join the conversation, head over to the Knative users Google group. Operators \u00b6 Knative components are intended to be integrated into more polished products that cloud service providers or in-house teams in large enterprises can then operate. Any enterprise or cloud provider can adopt Knative components into their own systems and pass the benefits along to their customers. Contributors \u00b6 With a clear project scope, lightweight governance model, and clean lines of separation between pluggable components, the Knative project establishes an efficient contributor workflow. Knative is a diverse, open, and inclusive community. Your own path to becoming a Knative contributor can begin in any of the following components: Knative authors \u00b6 Knative is an open source project with an active development community. The project was started by Google but has contributions from a growing number of industry-leading companies. For a current list of the authors, see Authors .","title":"About the community"},{"location":"community/about/#about-the-knative-community","text":"This page provides links to documents for common Knative community practices and a description of Knative's audience.","title":"About the Knative community"},{"location":"community/about/#community-values","text":"This section links to documents about our values. Knative project values : shared goals and values for the community. Knative team values : the goals and values we hold as a team.","title":"Community values"},{"location":"community/about/#governance","text":"This section links to documents about how the Knative community is governed. Knative has public and recorded monthly community meetings. Each project has one or more working groups driving the project, and Knative has a single technical oversight committee monitoring the overall project. Governance : the Knative governance framework. Community roles : describes the roles individuals can assume within the Knative community such as member, approver, or working group lead. Working groups : provides information about our various working groups. Steering Committee (SC) : describes our steering committee. Technical Oversight Committee (TOC) : describes our technical oversight committee. Trademark Committee : describes our trademark committee. Annual reports : lists previous annual reports.","title":"Governance"},{"location":"community/about/#processes","text":"This section links to documents for common Knative community processes. At the moment, these practices (except for the formation of Working Groups and Lazy Consensus) are recommendations that individual working groups can choose to adopt, rather than requirements. Each working group should document their processes; either in their own repo or in a pointer to these docs. Reviewing and Merging Pull Requests : how we manage pull requests. Working group processes : how working groups operate. SC election process : elcection process for our steering committee. TOC election process : election process for our technical oversight committee. Repository Guidelines : how we create and remove core repositories. Sandbox repo process : how to create a repo in the knative-sandbox GitHub org. Feature tracks : outlines the process for adding non-trivial features. Golang policy : principles regarding the Golang version Knative tests and releases with. Release principles : release principles including information about support and feature phases. Release schedule : Knative past and future release dates. Sunsetting features : process to sunset features that are getting no apparent usage, but are time consuming to maintain.","title":"Processes"},{"location":"community/about/#community-calendar","text":"The Knative community calendar ( iCal export file ) contains events that provide the opportunity to learn more about Knative and meet other users and contributors. This includes Working Group, Steering Committee, and other community meetings. Events don't have to be organized by the Knative project to be added to the calendar. If you want to add an event to the calendar please send an email to knative-steering@googlegroups.com or post to the #community channel in the Knative Slack workspace.","title":"Community calendar"},{"location":"community/about/#knatives-audience","text":"Knative is designed for different personas:","title":"Knative's audience"},{"location":"community/about/#developers","text":"Knative components offer developers Kubernetes-native APIs for deploying serverless-style functions, applications, and containers to an auto-scaling runtime. To join the conversation, head over to the Knative users Google group.","title":"Developers"},{"location":"community/about/#operators","text":"Knative components are intended to be integrated into more polished products that cloud service providers or in-house teams in large enterprises can then operate. Any enterprise or cloud provider can adopt Knative components into their own systems and pass the benefits along to their customers.","title":"Operators"},{"location":"community/about/#contributors","text":"With a clear project scope, lightweight governance model, and clean lines of separation between pluggable components, the Knative project establishes an efficient contributor workflow. Knative is a diverse, open, and inclusive community. Your own path to becoming a Knative contributor can begin in any of the following components:","title":"Contributors"},{"location":"community/about/#knative-authors","text":"Knative is an open source project with an active development community. The project was started by Google but has contributions from a growing number of industry-leading companies. For a current list of the authors, see Authors .","title":"Knative authors"},{"location":"community/contributing/","text":"Contribute to Knative \u00b6 This is the starting point for becoming a contributor - improving code, improving docs, giving talks, etc. Here are a few ways to get involved. Prerequisites \u00b6 If you want to contribute to Knative, you must do the following: Before you can make a contribution, you must fill out the Google Contributor License Agreement (CLA) using the same email address you used to register for Github. For more information, see Contributor license agreements . Read the Knative contributor guide . Read the Code of conduct . For more information about how the Knative community is run, see About the Knative community . Contribute to the code \u00b6 Knative is a diverse, open, and inclusive community. Development takes place in the Knative org on GitHub . Your own path to becoming a Knative contributor can begin in any of the following components, look for GitHub issues marked with the good first issue label. Knative Serving: For how to get started contributing, see the Serving development workflow . For good starter issues, see Serving issues . Knative Eventing: For how to get started contributing, see the Eventing development workflow . For good starter issues, see Eventing issues . Knative Client (kn): For how to get started contributing, see the Client development workflow . For good starter issues, see Client issues . Documentation: For how to get started contributing, see the Docs contributor guide . For good starter issues, see Documentation issues . Contribute code samples to the community \u00b6 Do you have a Knative code sample that demonstrates a use-case or product integration that will help someone learn about Knative? Beyond the official documentation there are endless possibilities for combining tools, platforms, languages, and products. By submitting a tutorial you can share your experience and help others who are solving similar problems. Community tutorials are stored in Markdown files under the code-samples/community . These documents are contributed, reviewed, and maintained by the community. Submit a Pull Request to the community sample directory under the Knative component folder that aligns with your document. For example, Knative Serving samples are under the serving folder. A reviewer will be assigned to review your submission. They\u2019ll work with you to ensure that your submission is clear, correct, and meets the style guide, but it helps if you follow it as you write your tutorial. Contribute code samples : Share your samples with the community. Link existing code samples : Link to your Knative samples that live on another site. Learn and connect \u00b6 Using or want to use Knative? Have any questions? Find out more here: Knative users group : Discussion and help from your fellow users. Knative developers group Discussion and help from Knative developers. Knative Slack : Ping @serving-help or @eventing-help if you run into issues using Knative Serving or Eventing and chat with other project developers. See also the Knative Slack guidelines . Twitter : Follow us on Twitter to get the latest news! Stack Overflow questions : Knative tagged questions and curated answers. Community Meetup \u00b6 This virtual event is designed for end users, a space for our community to meet, get to know each other, and learn about uses and applications of Knative. Catch up with past community meetups on our YouTube channel . Stay tuned for new events by subscribing to the calendar ( iCal export file ) and following us on Twitter . More ways to get involved \u00b6 Even if there\u2019s not an issue opened for it, we can always use more testing throughout the platform. Similarly, we can always use more docs, richer docs, insightful docs. Or maybe a cool blog post? And if you\u2019re a web developer, we could use your help in spiffing up our public-facing web site. Bug reports and friction logs from new developers are especially welcome.","title":"Contribute to Knative"},{"location":"community/contributing/#contribute-to-knative","text":"This is the starting point for becoming a contributor - improving code, improving docs, giving talks, etc. Here are a few ways to get involved.","title":"Contribute to Knative"},{"location":"community/contributing/#prerequisites","text":"If you want to contribute to Knative, you must do the following: Before you can make a contribution, you must fill out the Google Contributor License Agreement (CLA) using the same email address you used to register for Github. For more information, see Contributor license agreements . Read the Knative contributor guide . Read the Code of conduct . For more information about how the Knative community is run, see About the Knative community .","title":"Prerequisites"},{"location":"community/contributing/#contribute-to-the-code","text":"Knative is a diverse, open, and inclusive community. Development takes place in the Knative org on GitHub . Your own path to becoming a Knative contributor can begin in any of the following components, look for GitHub issues marked with the good first issue label. Knative Serving: For how to get started contributing, see the Serving development workflow . For good starter issues, see Serving issues . Knative Eventing: For how to get started contributing, see the Eventing development workflow . For good starter issues, see Eventing issues . Knative Client (kn): For how to get started contributing, see the Client development workflow . For good starter issues, see Client issues . Documentation: For how to get started contributing, see the Docs contributor guide . For good starter issues, see Documentation issues .","title":"Contribute to the code"},{"location":"community/contributing/#contribute-code-samples-to-the-community","text":"Do you have a Knative code sample that demonstrates a use-case or product integration that will help someone learn about Knative? Beyond the official documentation there are endless possibilities for combining tools, platforms, languages, and products. By submitting a tutorial you can share your experience and help others who are solving similar problems. Community tutorials are stored in Markdown files under the code-samples/community . These documents are contributed, reviewed, and maintained by the community. Submit a Pull Request to the community sample directory under the Knative component folder that aligns with your document. For example, Knative Serving samples are under the serving folder. A reviewer will be assigned to review your submission. They\u2019ll work with you to ensure that your submission is clear, correct, and meets the style guide, but it helps if you follow it as you write your tutorial. Contribute code samples : Share your samples with the community. Link existing code samples : Link to your Knative samples that live on another site.","title":"Contribute code samples to the community"},{"location":"community/contributing/#learn-and-connect","text":"Using or want to use Knative? Have any questions? Find out more here: Knative users group : Discussion and help from your fellow users. Knative developers group Discussion and help from Knative developers. Knative Slack : Ping @serving-help or @eventing-help if you run into issues using Knative Serving or Eventing and chat with other project developers. See also the Knative Slack guidelines . Twitter : Follow us on Twitter to get the latest news! Stack Overflow questions : Knative tagged questions and curated answers.","title":"Learn and connect"},{"location":"community/contributing/#community-meetup","text":"This virtual event is designed for end users, a space for our community to meet, get to know each other, and learn about uses and applications of Knative. Catch up with past community meetups on our YouTube channel . Stay tuned for new events by subscribing to the calendar ( iCal export file ) and following us on Twitter .","title":"Community Meetup"},{"location":"community/contributing/#more-ways-to-get-involved","text":"Even if there\u2019s not an issue opened for it, we can always use more testing throughout the platform. Similarly, we can always use more docs, richer docs, insightful docs. Or maybe a cool blog post? And if you\u2019re a web developer, we could use your help in spiffing up our public-facing web site. Bug reports and friction logs from new developers are especially welcome.","title":"More ways to get involved"},{"location":"eventing/","text":"Knative Eventing \u00b6 Knative Eventing provides tools for routing events from event producers to sinks, enabling developers to use an event-driven architecture with their applications. Knative Eventing resources are loosely coupled, and can be developed and deployed independently of each other. Any producer can generate events before there are active event consumers that are listening. Any event consumer can express interest in an event or class of events, before there are producers that are creating those events. Knative Eventing uses standard HTTP POST requests to send and receive events between event producers and sinks. These events conform to the CloudEvents specifications , which enables creating, parsing, sending, and receiving events in any programming language. Other services can be connected to the Knative Eventing system. These services can perform the following functions: Create new applications without modifying the event producer or event consumer. Select and target specific subsets of the events from their producers. Installation \u00b6 You can install Knative Eventing via the methods listed on the installation page . Common use cases \u00b6 Knative Eventing supports the following use cases: Publish an event without creating a consumer. You can send events to a broker as an HTTP POST, and use binding to decouple the destination configuration from your application that produces events. Consume an event without creating a publisher. You can use a trigger to consume events from a broker based on event attributes. The application receives events as an HTTP POST. Tip Multiple event producers and sinks can be used together to create more advanced Knative Eventing flows to solve complex use cases. Eventing components \u00b6 An event-driven architecture is based on the concept of decoupled relationships between event producers that create events, and event consumers, or sinks , that receive events. It builds on delivery over HTTP by providing configuration and management of pluggable event-routing components. A sink or subscriber can also be configured to respond to HTTP requests by sending a response event. Examples of sinks in a Knative Eventing deployment include Knative Services, Channels and Brokers. Event sources \u00b6 In a Knative Eventing deployment, event Sources are the primary event producers. Events are sent to a sink or subscriber . Broker and Trigger \u00b6 Brokers and Triggers provide an \"event mesh\" model, which allows an event producer to deliver events to a Broker, which then distributes them uniformly to consumers by using Triggers. This delivers the following benefits: Consumers can register for specific types of events without needing to negotiate directly with event producers. Event routing can be optimized by the underlying platform using the specified filter conditions. Channel and Subscription \u00b6 Channels and Subscriptions provide a \"event pipe\" model which transforms and routes events between Channels using Subscriptions. This model is appropriate for event pipelines where events from one system need to be transformed and then routed to another process. Event registry \u00b6 Knative Eventing defines an EventType object to make it easier for consumers to discover the types of events they can consume from Brokers. The registry consists of a collection of event types. The event types stored in the registry contain (all) the required information for a consumer to create a Trigger without resorting to some other out-of-band mechanism. To learn how to use the registry, see the Event Registry documentation .","title":"Knative Eventing\u6982\u8ff0"},{"location":"eventing/#knative-eventing","text":"Knative Eventing provides tools for routing events from event producers to sinks, enabling developers to use an event-driven architecture with their applications. Knative Eventing resources are loosely coupled, and can be developed and deployed independently of each other. Any producer can generate events before there are active event consumers that are listening. Any event consumer can express interest in an event or class of events, before there are producers that are creating those events. Knative Eventing uses standard HTTP POST requests to send and receive events between event producers and sinks. These events conform to the CloudEvents specifications , which enables creating, parsing, sending, and receiving events in any programming language. Other services can be connected to the Knative Eventing system. These services can perform the following functions: Create new applications without modifying the event producer or event consumer. Select and target specific subsets of the events from their producers.","title":"Knative Eventing"},{"location":"eventing/#installation","text":"You can install Knative Eventing via the methods listed on the installation page .","title":"Installation"},{"location":"eventing/#common-use-cases","text":"Knative Eventing supports the following use cases: Publish an event without creating a consumer. You can send events to a broker as an HTTP POST, and use binding to decouple the destination configuration from your application that produces events. Consume an event without creating a publisher. You can use a trigger to consume events from a broker based on event attributes. The application receives events as an HTTP POST. Tip Multiple event producers and sinks can be used together to create more advanced Knative Eventing flows to solve complex use cases.","title":"Common use cases"},{"location":"eventing/#eventing-components","text":"An event-driven architecture is based on the concept of decoupled relationships between event producers that create events, and event consumers, or sinks , that receive events. It builds on delivery over HTTP by providing configuration and management of pluggable event-routing components. A sink or subscriber can also be configured to respond to HTTP requests by sending a response event. Examples of sinks in a Knative Eventing deployment include Knative Services, Channels and Brokers.","title":"Eventing components"},{"location":"eventing/#event-sources","text":"In a Knative Eventing deployment, event Sources are the primary event producers. Events are sent to a sink or subscriber .","title":"Event sources"},{"location":"eventing/#broker-and-trigger","text":"Brokers and Triggers provide an \"event mesh\" model, which allows an event producer to deliver events to a Broker, which then distributes them uniformly to consumers by using Triggers. This delivers the following benefits: Consumers can register for specific types of events without needing to negotiate directly with event producers. Event routing can be optimized by the underlying platform using the specified filter conditions.","title":"Broker and Trigger"},{"location":"eventing/#channel-and-subscription","text":"Channels and Subscriptions provide a \"event pipe\" model which transforms and routes events between Channels using Subscriptions. This model is appropriate for event pipelines where events from one system need to be transformed and then routed to another process.","title":"Channel and Subscription"},{"location":"eventing/#event-registry","text":"Knative Eventing defines an EventType object to make it easier for consumers to discover the types of events they can consume from Brokers. The registry consists of a collection of event types. The event types stored in the registry contain (all) the required information for a consumer to create a Trigger without resorting to some other out-of-band mechanism. To learn how to use the registry, see the Event Registry documentation .","title":"Event registry"},{"location":"eventing/accessing-traces/","text":"Accessing CloudEvent traces \u00b6 Depending on the request tracing tool that you have installed on your Knative Eventing cluster, see the corresponding section for details about how to visualize and trace your requests. Before you begin \u00b6 You must have a Knative cluster running with the Eventing component installed. Learn more . Configuring tracing \u00b6 With the exception of importers, the Knative Eventing tracing is configured through the config-tracing ConfigMap in the knative-eventing namespace. Most importers do not use the ConfigMap and instead, use a static 1% sampling rate. You can use the config-tracing ConfigMap to configure the following Eventing components: Brokers Triggers InMemoryChannel ApiServerSource PingSource GitlabSource KafkaSource PrometheusSource Example: The following example config-tracing ConfigMap samples 10% of all CloudEvents: apiVersion : v1 kind : ConfigMap metadata : name : config-tracing namespace : knative-eventing data : backend : \"zipkin\" zipkin-endpoint : \"http://zipkin.istio-system.svc.cluster.local:9411/api/v2/spans\" sample-rate : \"0.1\" Configuration options \u00b6 You can configure your config-tracing with following options: backend : Valid values are zipkin , stackdriver , or none . The default is none . zipkin-endpoint : Specifies the URL to the zipkin collector where you want to send the traces. Must be set if backend is set to zipkin . stackdriver-project-id : Specifies the GCP project ID into which the Stackdriver traces are written. You must specify the backend as stackdriver . If backend is unspecified, the GCP project ID is read from GCP metadata when running on GCP. sample-rate : Specifies the sampling rate. Valid values are decimals from 0 to 1 (interpreted as a float64), which indicate the probability that any given request is sampled. An example value is 0.5 , which gives each request a 50% sampling probablity. debug : Enables debugging. Valid values are true or false . Defaults to false when not specified. Set to true to enable debug mode, which forces the sample-rate to 1.0 and sends all spans to the server. Viewing your config-tracing ConfigMap \u00b6 To view your current configuration: kubectl -n knative-eventing get configmap config-tracing -oyaml Editing and deploying your config-tracing ConfigMap \u00b6 To edit and then immediately deploy changes to your ConfigMap, run the following command: kubectl -n knative-eventing edit configmap config-tracing Accessing traces in Eventing \u00b6 To access the traces, you use either the Zipkin or Jaeger tool. Details about using these tools to access traces are provided in the Knative Serving observability section: Zipkin Jaeger Example \u00b6 The following demonstrates how to trace requests in Knative Eventing with Zipkin, using the TestBrokerTracing End-to-End test. For this example, assume the following details: Everything happens in the includes-incoming-trace-id-2qszn namespace. The Broker is named br . There are two Triggers that are associated with the Broker: transformer - Filters to only allow events whose type is transformer . Sends the event to the Kubernetes Service transformer , which will reply with an identical event, except the replied event's type will be logger . logger - Filters to only allow events whose type is logger . Sends the event to the Kubernetes Service logger . An event is sent to the Broker with the type transformer , by the Pod named sender . Given this scenario, the expected path and behavior of an event is as follows: sender Pod sends the request to the Broker. Go to the Broker's ingress Pod. Go to the imc-dispatcher Channel (imc stands for InMemoryChannel). Go to both Triggers. Go to the Broker's filter Pod for the Trigger logger . The Trigger's filter ignores this event. Go to the Broker's filter Pod for the Trigger transformer . The filter does pass, so it goes to the Kubernetes Service pointed at, also named transformer . transformer Pod replies with the modified event. Go to an InMemory dispatcher. Go to the Broker's ingress Pod. Go to the InMemory dispatcher. Go to both Triggers. Go to the Broker's filter Pod for the Trigger transformer . The Trigger's filter ignores the event. Go to the Broker's filter Pod for the Trigger logger . The filter passes. Go to the logger Pod. There is no reply. This is a screenshot of the trace view in Zipkin. All the red letters have been added to the screenshot and correspond to the expectations earlier in this section: This is the same screenshot without the annotations. If you are interested, here is the raw JSON of the trace.","title":"Accessing CloudEvent traces"},{"location":"eventing/accessing-traces/#accessing-cloudevent-traces","text":"Depending on the request tracing tool that you have installed on your Knative Eventing cluster, see the corresponding section for details about how to visualize and trace your requests.","title":"Accessing CloudEvent traces"},{"location":"eventing/accessing-traces/#before-you-begin","text":"You must have a Knative cluster running with the Eventing component installed. Learn more .","title":"Before you begin"},{"location":"eventing/accessing-traces/#configuring-tracing","text":"With the exception of importers, the Knative Eventing tracing is configured through the config-tracing ConfigMap in the knative-eventing namespace. Most importers do not use the ConfigMap and instead, use a static 1% sampling rate. You can use the config-tracing ConfigMap to configure the following Eventing components: Brokers Triggers InMemoryChannel ApiServerSource PingSource GitlabSource KafkaSource PrometheusSource Example: The following example config-tracing ConfigMap samples 10% of all CloudEvents: apiVersion : v1 kind : ConfigMap metadata : name : config-tracing namespace : knative-eventing data : backend : \"zipkin\" zipkin-endpoint : \"http://zipkin.istio-system.svc.cluster.local:9411/api/v2/spans\" sample-rate : \"0.1\"","title":"Configuring tracing"},{"location":"eventing/accessing-traces/#configuration-options","text":"You can configure your config-tracing with following options: backend : Valid values are zipkin , stackdriver , or none . The default is none . zipkin-endpoint : Specifies the URL to the zipkin collector where you want to send the traces. Must be set if backend is set to zipkin . stackdriver-project-id : Specifies the GCP project ID into which the Stackdriver traces are written. You must specify the backend as stackdriver . If backend is unspecified, the GCP project ID is read from GCP metadata when running on GCP. sample-rate : Specifies the sampling rate. Valid values are decimals from 0 to 1 (interpreted as a float64), which indicate the probability that any given request is sampled. An example value is 0.5 , which gives each request a 50% sampling probablity. debug : Enables debugging. Valid values are true or false . Defaults to false when not specified. Set to true to enable debug mode, which forces the sample-rate to 1.0 and sends all spans to the server.","title":"Configuration options"},{"location":"eventing/accessing-traces/#viewing-your-config-tracing-configmap","text":"To view your current configuration: kubectl -n knative-eventing get configmap config-tracing -oyaml","title":"Viewing your config-tracing ConfigMap"},{"location":"eventing/accessing-traces/#editing-and-deploying-your-config-tracing-configmap","text":"To edit and then immediately deploy changes to your ConfigMap, run the following command: kubectl -n knative-eventing edit configmap config-tracing","title":"Editing and deploying your config-tracing ConfigMap"},{"location":"eventing/accessing-traces/#accessing-traces-in-eventing","text":"To access the traces, you use either the Zipkin or Jaeger tool. Details about using these tools to access traces are provided in the Knative Serving observability section: Zipkin Jaeger","title":"Accessing traces in Eventing"},{"location":"eventing/accessing-traces/#example","text":"The following demonstrates how to trace requests in Knative Eventing with Zipkin, using the TestBrokerTracing End-to-End test. For this example, assume the following details: Everything happens in the includes-incoming-trace-id-2qszn namespace. The Broker is named br . There are two Triggers that are associated with the Broker: transformer - Filters to only allow events whose type is transformer . Sends the event to the Kubernetes Service transformer , which will reply with an identical event, except the replied event's type will be logger . logger - Filters to only allow events whose type is logger . Sends the event to the Kubernetes Service logger . An event is sent to the Broker with the type transformer , by the Pod named sender . Given this scenario, the expected path and behavior of an event is as follows: sender Pod sends the request to the Broker. Go to the Broker's ingress Pod. Go to the imc-dispatcher Channel (imc stands for InMemoryChannel). Go to both Triggers. Go to the Broker's filter Pod for the Trigger logger . The Trigger's filter ignores this event. Go to the Broker's filter Pod for the Trigger transformer . The filter does pass, so it goes to the Kubernetes Service pointed at, also named transformer . transformer Pod replies with the modified event. Go to an InMemory dispatcher. Go to the Broker's ingress Pod. Go to the InMemory dispatcher. Go to both Triggers. Go to the Broker's filter Pod for the Trigger transformer . The Trigger's filter ignores the event. Go to the Broker's filter Pod for the Trigger logger . The filter passes. Go to the logger Pod. There is no reply. This is a screenshot of the trace view in Zipkin. All the red letters have been added to the screenshot and correspond to the expectations earlier in this section: This is the same screenshot without the annotations. If you are interested, here is the raw JSON of the trace.","title":"Example"},{"location":"eventing/event-delivery/","text":"Handling Delivery Failure \u00b6 You can configure event delivery parameters for Knative Eventing components that are applied in cases where an event fails to be delivered Configuring Subscription event delivery \u00b6 You can configure how events are delivered for each Subscription by adding a delivery spec to the Subscription object, as shown in the following example: apiVersion : messaging.knative.dev/v1 kind : Subscription metadata : name : example-subscription namespace : example-namespace spec : delivery : deadLetterSink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : example-sink backoffDelay : <duration> backoffPolicy : <policy-type> retry : <integer> Where The deadLetterSink spec contains configuration settings to enable using a dead letter sink. This tells the Subscription what happens to events that cannot be delivered to the subscriber. When this is configured, events that fail to be delivered are sent to the dead letter sink destination. The destination can be a Knative Service or a URI. In the example, the destination is a Service object, or Knative Service, named example-sink . The backoffDelay delivery parameter specifies the time delay before an event delivery retry is attempted after a failure. The duration of the backoffDelay parameter is specified using the ISO 8601 format. For example, PT1S specifies a 1 second delay. The backoffPolicy delivery parameter can be used to specify the retry back off policy. The policy can be specified as either linear or exponential . When using the linear back off policy, the back off delay is the time interval specified between retries. When using the exponential back off policy, the back off delay is equal to backoffDelay*2^<numberOfRetries> . retry specifies the number of times that event delivery is retried before the event is sent to the dead letter sink. Configuring Broker event delivery \u00b6 You can configure how events are delivered for each Broker by adding a delivery spec, as shown in the following example: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : with-dead-letter-sink spec : delivery : deadLetterSink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : example-sink backoffDelay : <duration> backoffPolicy : <policy-type> retry : <integer> Where The deadLetterSink spec contains configuration settings to enable using a dead letter sink. This tells the Subscription what happens to events that cannot be delivered to the subscriber. When this is configured, events that fail to be delivered are sent to the dead letter sink destination. The destination can be any Addressable object that conforms to the Knative Eventing sink contract, such as a Knative Service, a Kubernetes Service, or a URI. In the example, the destination is a Service object, or Knative Service, named example-sink . The backoffDelay delivery parameter specifies the time delay before an event delivery retry is attempted after a failure. The duration of the backoffDelay parameter is specified using the ISO 8601 format. For example, PT1S specifies a 1 second delay. The backoffPolicy delivery parameter can be used to specify the retry back off policy. The policy can be specified as either linear or exponential . When using the linear back off policy, the back off delay is the time interval specified between retries. This is a linearly increasing delay, which means that the back off delay increases by the given interval for each retry. When using the exponential back off policy, the back off delay increases by a multiplier of the given interval for each retry. retry specifies the number of times that event delivery is retried before the event is sent to the dead letter sink. The initial delivery attempt is not included in the retry count, so the total number of delivery attempts is equal to the retry value +1. Broker support \u00b6 The following table summarizes which delivery parameters are supported for each Broker implementation type: Broker Class Supported Delivery Parameters googlecloud deadLetterSink , retry , backoffPolicy , backoffDelay Kafka deadLetterSink , retry , backoffPolicy , backoffDelay MTChannelBasedBroker depends on the underlying Channel RabbitMQBroker deadLetterSink , retry , backoffPolicy , backoffDelay Note deadLetterSink must be a GCP Pub/Sub topic URI. googlecloud Broker only supports the exponential back off policy. Configuring Channel event delivery \u00b6 Failed events may, depending on the specific Channel implementation in use, be enhanced with extension attributes prior to forwarding to the deadLetterSink . These extension attributes are as follows: knativeerrordest Type: String Description: The original destination URL to which the failed event was sent. This could be either a delivery or reply URL based on which operation encountered the failed event. Constraints: Always present because every HTTP Request has a destination URL. Examples: \"http://myservice.mynamespace.svc.cluster.local:3000/mypath\" ...any deadLetterSink URL... knativeerrorcode Type: Int Description: The HTTP Response StatusCode from the final event dispatch attempt. Constraints: Always present because every HTTP Response contains a StatusCode . Examples: \"500\" ...any HTTP StatusCode... knativeerrordata Type: String Description: The HTTP Response Body from the final event dispatch attempt. Constraints: Empty if the HTTP Response Body is empty, and may be truncated if the length is excessive. Examples: 'Internal Server Error: Failed to process event.' '{\"key\": \"value\"}' ...any HTTP Response Body... Channel support \u00b6 The following table summarizes which delivery parameters are supported for each Channel implementation. Channel Type Supported Delivery Parameters GCP PubSub none In-Memory deadLetterSink , retry , backoffPolicy , backoffDelay Kafka deadLetterSink , retry , backoffPolicy , backoffDelay Natss none","title":"Handling delivery failure"},{"location":"eventing/event-delivery/#handling-delivery-failure","text":"You can configure event delivery parameters for Knative Eventing components that are applied in cases where an event fails to be delivered","title":"Handling Delivery Failure"},{"location":"eventing/event-delivery/#configuring-subscription-event-delivery","text":"You can configure how events are delivered for each Subscription by adding a delivery spec to the Subscription object, as shown in the following example: apiVersion : messaging.knative.dev/v1 kind : Subscription metadata : name : example-subscription namespace : example-namespace spec : delivery : deadLetterSink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : example-sink backoffDelay : <duration> backoffPolicy : <policy-type> retry : <integer> Where The deadLetterSink spec contains configuration settings to enable using a dead letter sink. This tells the Subscription what happens to events that cannot be delivered to the subscriber. When this is configured, events that fail to be delivered are sent to the dead letter sink destination. The destination can be a Knative Service or a URI. In the example, the destination is a Service object, or Knative Service, named example-sink . The backoffDelay delivery parameter specifies the time delay before an event delivery retry is attempted after a failure. The duration of the backoffDelay parameter is specified using the ISO 8601 format. For example, PT1S specifies a 1 second delay. The backoffPolicy delivery parameter can be used to specify the retry back off policy. The policy can be specified as either linear or exponential . When using the linear back off policy, the back off delay is the time interval specified between retries. When using the exponential back off policy, the back off delay is equal to backoffDelay*2^<numberOfRetries> . retry specifies the number of times that event delivery is retried before the event is sent to the dead letter sink.","title":"Configuring Subscription event delivery"},{"location":"eventing/event-delivery/#configuring-broker-event-delivery","text":"You can configure how events are delivered for each Broker by adding a delivery spec, as shown in the following example: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : with-dead-letter-sink spec : delivery : deadLetterSink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : example-sink backoffDelay : <duration> backoffPolicy : <policy-type> retry : <integer> Where The deadLetterSink spec contains configuration settings to enable using a dead letter sink. This tells the Subscription what happens to events that cannot be delivered to the subscriber. When this is configured, events that fail to be delivered are sent to the dead letter sink destination. The destination can be any Addressable object that conforms to the Knative Eventing sink contract, such as a Knative Service, a Kubernetes Service, or a URI. In the example, the destination is a Service object, or Knative Service, named example-sink . The backoffDelay delivery parameter specifies the time delay before an event delivery retry is attempted after a failure. The duration of the backoffDelay parameter is specified using the ISO 8601 format. For example, PT1S specifies a 1 second delay. The backoffPolicy delivery parameter can be used to specify the retry back off policy. The policy can be specified as either linear or exponential . When using the linear back off policy, the back off delay is the time interval specified between retries. This is a linearly increasing delay, which means that the back off delay increases by the given interval for each retry. When using the exponential back off policy, the back off delay increases by a multiplier of the given interval for each retry. retry specifies the number of times that event delivery is retried before the event is sent to the dead letter sink. The initial delivery attempt is not included in the retry count, so the total number of delivery attempts is equal to the retry value +1.","title":"Configuring Broker event delivery"},{"location":"eventing/event-delivery/#broker-support","text":"The following table summarizes which delivery parameters are supported for each Broker implementation type: Broker Class Supported Delivery Parameters googlecloud deadLetterSink , retry , backoffPolicy , backoffDelay Kafka deadLetterSink , retry , backoffPolicy , backoffDelay MTChannelBasedBroker depends on the underlying Channel RabbitMQBroker deadLetterSink , retry , backoffPolicy , backoffDelay Note deadLetterSink must be a GCP Pub/Sub topic URI. googlecloud Broker only supports the exponential back off policy.","title":"Broker support"},{"location":"eventing/event-delivery/#configuring-channel-event-delivery","text":"Failed events may, depending on the specific Channel implementation in use, be enhanced with extension attributes prior to forwarding to the deadLetterSink . These extension attributes are as follows: knativeerrordest Type: String Description: The original destination URL to which the failed event was sent. This could be either a delivery or reply URL based on which operation encountered the failed event. Constraints: Always present because every HTTP Request has a destination URL. Examples: \"http://myservice.mynamespace.svc.cluster.local:3000/mypath\" ...any deadLetterSink URL... knativeerrorcode Type: Int Description: The HTTP Response StatusCode from the final event dispatch attempt. Constraints: Always present because every HTTP Response contains a StatusCode . Examples: \"500\" ...any HTTP StatusCode... knativeerrordata Type: String Description: The HTTP Response Body from the final event dispatch attempt. Constraints: Empty if the HTTP Response Body is empty, and may be truncated if the length is excessive. Examples: 'Internal Server Error: Failed to process event.' '{\"key\": \"value\"}' ...any HTTP Response Body...","title":"Configuring Channel event delivery"},{"location":"eventing/event-delivery/#channel-support","text":"The following table summarizes which delivery parameters are supported for each Channel implementation. Channel Type Supported Delivery Parameters GCP PubSub none In-Memory deadLetterSink , retry , backoffPolicy , backoffDelay Kafka deadLetterSink , retry , backoffPolicy , backoffDelay Natss none","title":"Channel support"},{"location":"eventing/event-registry/","text":"Using the event registry \u00b6 The event registry maintains a catalog of event types that each Broker can consume. It is designed for use with the Broker and Trigger model, and provides information to help you create Triggers. This topic introduces the EventType custom resource and provides information about how to populate the event registry, how to discover events using the registry, and how to leverage that information to subscribe to events of interest. Before you begin \u00b6 It's recommended that you have a basic understanding of the following: Brokers Triggers The CloudEvents spec , particularly the Context Attributes section Event sources About EventType objects \u00b6 EventType objects represent a type of event that can be consumed from a Broker, such as Kafka messages or GitHub pull requests. EventType objects are used to populate the event registry and persist event type information in the cluster datastore. The following is an example EventType YAML that omits irrelevant fields: apiVersion : eventing.knative.dev/v1beta1 kind : EventType metadata : name : dev.knative.source.github.push-34cnb namespace : default labels : eventing.knative.dev/sourceName : github-sample spec : type : dev.knative.source.github.push source : https://github.com/knative/eventing schema : description : broker : default status : conditions : - status : \"True\" type : BrokerExists - status : \"True\" type : BrokerReady - status : \"True\" type : Ready For the full specification for an EventType object, see the EventType API reference . The metadata.name field is advisory, that is, non-authoritative. It is typically generated using generateName to avoid naming collisions. metadata.name is not needed when you create Triggers. For consumers, the fields that matter the most are spec and status . This is because these fields provide the information you need to create Triggers, which is the source and type of event and whether the Broker is ready to receive events. The following table has more information about the spec and status fields of EventType objects: Field Description Required or optional spec.type Refers to the CloudEvent type as it enters into the event mesh. Event consumers can create Triggers filtering on this attribute. This field is authoritative. Required spec.source Refers to the CloudEvent source as it enters into the event mesh. Event consumers can create Triggers filtering on this attribute. Required spec.schema A valid URI with the EventType schema such as a JSON schema or a protobuf schema. Optional spec.description A string describing what the EventType is about. Optional spec.broker Refers to the Broker that can provide the EventType. Required status Tells consumers, or cluster operators, whether the EventType is ready to be consumed or not. The readiness is based on the Broker being ready. Optional Populate the registry with events \u00b6 You can populate the registry with EventType objects manually or automatically. Automatic registration can be the easier method, but it only supports a subset of event sources. Manual registration \u00b6 For manual registration, the cluster configurator applies EventTypes YAML files the same as with any other Kubernetes resource. To apply EventTypes YAML files manually: Create an EventType YAML file. For information about the required fields, see About EventType objects . Apply the YAML by running the command: kubectl apply -f <event-type.yaml> Automatic registration \u00b6 Because manual registration might be tedious and error-prone, Knative also supports registering EventTypes automatically. EventTypes are created automatically when an event source is instantiated. Support for automatic registration \u00b6 Knative supports automatic registration of EventTypes for the following event sources: CronJobSource ApiServerSource GithubSource GcpPubSubSource KafkaSource AwsSqsSource Knative only supports automatic creation of EventTypes for sources that have a Broker as their sink. Procedure for automatic registration \u00b6 To register EventTypes automatically, apply your event source YAML file by running the command: kubectl apply -f <event-source.yaml> After your event source is instantiated, EventTypes are added to the registry. Example: Automatic registration using KafkaSource \u00b6 Given the following KafkaSource sample to populate the registry: apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-sample namespace : default spec : bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 topics : - knative-demo - news sink : apiVersion : eventing.knative.dev/v1 kind : Broker name : default The topics field in the above example is used to generate the EventType source field. After running kubectl apply using the above YAML, the KafkaSource kafka-source-sample is instantiated, and two EventTypes are added to the registry because there are two topics. Discover events using the registry \u00b6 Using the registry, you can discover the different types of events that Broker event meshes can consume. View all event types you can subscribe to \u00b6 To see a list of event types in the registry that are available to subscribe to, run the command: kubectl get eventtypes -n <namespace> Example output using the default namespace in a testing cluster: NAME TYPE SOURCE SCHEMA BROKER DESCRIPTION READY REASON dev.knative.source.github.push-34cnb dev.knative.source.github.push https://github.com/knative/eventing default True dev.knative.source.github.push-44svn dev.knative.source.github.push https://github.com/knative/serving default True dev.knative.source.github.pullrequest-86jhv dev.knative.source.github.pull_request https://github.com/knative/eventing default True dev.knative.source.github.pullrequest-97shf dev.knative.source.github.pull_request https://github.com/knative/serving default True dev.knative.kafka.event-cjvcr dev.knative.kafka.event /apis/v1/namespaces/default/kafkasources/kafka-sample#news default True dev.knative.kafka.event-tdt48 dev.knative.kafka.event /apis/v1/namespaces/default/kafkasources/kafka-sample#knative-demo default True google.pubsub.topic.publish-hrxhh google.pubsub.topic.publish //pubsub.googleapis.com/knative/topics/testing dev False BrokerIsNotReady This example output shows seven different EventType objects in the registry of the default namespace. It assumes that the event sources emitting the events reference a Broker as their sink. View the YAML for an EventType object \u00b6 To see the YAML for an EventType object, run the command: kubectl get eventtype <name> -o yaml Where <name> is the name of an EventType object and can be found in the NAME column of the registry output. For example, dev.knative.source.github.push-34cnb . For an example EventType YAML, see About EventType objects earlier on this page. About subscribing to events \u00b6 After you know what events can be consumed from the Brokers' event meshes, you can create Triggers to subscribe to particular events. Here are a some example Triggers that subscribe to events using exact matching on type or source , based on the registry output mentioned earlier: Subscribes to GitHub pushes from any source: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : push-trigger namespace : default spec : broker : default filter : attributes : type : dev.knative.source.github.push subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : push-service Note As the example registry output mentioned, only two sources, the knative/eventing and knative/serving GitHub repositories, exist for that particular type of event. If later on new sources are registered for GitHub pushes, this Trigger is able to consume them. Subscribes to GitHub pull requests from the knative/eventing GitHub repository: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : gh-knative-eventing-pull-trigger namespace : default spec : broker : default filter : attributes : type : dev.knative.source.github.pull_request source : https://github.com/knative/eventing subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : gh-knative-eventing-pull-service Subscribes to Kafka messages sent to the knative-demo topic: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : kafka-knative-demo-trigger namespace : default spec : broker : default filter : attributes : type : dev.knative.kafka.event source : /apis/v1/namespaces/default/kafkasources/kafka-sample#knative-demo subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : kafka-knative-demo-service Subscribes to PubSub messages from GCP's knative project sent to the testing topic: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : gcp-pubsub-knative-testing-trigger namespace : default spec : broker : dev filter : attributes : source : //pubsub.googleapis.com/knative/topics/testing subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : gcp-pubsub-knative-testing-service Note The example registry output mentioned earlier lists this Broker's readiness as false . This Trigger's subscriber cannot consume events until the Broker becomes ready. Next steps \u00b6 Knative code samples is a useful resource to better understand some of the event sources. Remember, you must point the sources to a Broker if you want automatic registration of EventTypes in the registry.","title":"Using the Event registry"},{"location":"eventing/event-registry/#using-the-event-registry","text":"The event registry maintains a catalog of event types that each Broker can consume. It is designed for use with the Broker and Trigger model, and provides information to help you create Triggers. This topic introduces the EventType custom resource and provides information about how to populate the event registry, how to discover events using the registry, and how to leverage that information to subscribe to events of interest.","title":"Using the event registry"},{"location":"eventing/event-registry/#before-you-begin","text":"It's recommended that you have a basic understanding of the following: Brokers Triggers The CloudEvents spec , particularly the Context Attributes section Event sources","title":"Before you begin"},{"location":"eventing/event-registry/#about-eventtype-objects","text":"EventType objects represent a type of event that can be consumed from a Broker, such as Kafka messages or GitHub pull requests. EventType objects are used to populate the event registry and persist event type information in the cluster datastore. The following is an example EventType YAML that omits irrelevant fields: apiVersion : eventing.knative.dev/v1beta1 kind : EventType metadata : name : dev.knative.source.github.push-34cnb namespace : default labels : eventing.knative.dev/sourceName : github-sample spec : type : dev.knative.source.github.push source : https://github.com/knative/eventing schema : description : broker : default status : conditions : - status : \"True\" type : BrokerExists - status : \"True\" type : BrokerReady - status : \"True\" type : Ready For the full specification for an EventType object, see the EventType API reference . The metadata.name field is advisory, that is, non-authoritative. It is typically generated using generateName to avoid naming collisions. metadata.name is not needed when you create Triggers. For consumers, the fields that matter the most are spec and status . This is because these fields provide the information you need to create Triggers, which is the source and type of event and whether the Broker is ready to receive events. The following table has more information about the spec and status fields of EventType objects: Field Description Required or optional spec.type Refers to the CloudEvent type as it enters into the event mesh. Event consumers can create Triggers filtering on this attribute. This field is authoritative. Required spec.source Refers to the CloudEvent source as it enters into the event mesh. Event consumers can create Triggers filtering on this attribute. Required spec.schema A valid URI with the EventType schema such as a JSON schema or a protobuf schema. Optional spec.description A string describing what the EventType is about. Optional spec.broker Refers to the Broker that can provide the EventType. Required status Tells consumers, or cluster operators, whether the EventType is ready to be consumed or not. The readiness is based on the Broker being ready. Optional","title":"About EventType objects"},{"location":"eventing/event-registry/#populate-the-registry-with-events","text":"You can populate the registry with EventType objects manually or automatically. Automatic registration can be the easier method, but it only supports a subset of event sources.","title":"Populate the registry with events"},{"location":"eventing/event-registry/#manual-registration","text":"For manual registration, the cluster configurator applies EventTypes YAML files the same as with any other Kubernetes resource. To apply EventTypes YAML files manually: Create an EventType YAML file. For information about the required fields, see About EventType objects . Apply the YAML by running the command: kubectl apply -f <event-type.yaml>","title":"Manual registration"},{"location":"eventing/event-registry/#automatic-registration","text":"Because manual registration might be tedious and error-prone, Knative also supports registering EventTypes automatically. EventTypes are created automatically when an event source is instantiated.","title":"Automatic registration"},{"location":"eventing/event-registry/#support-for-automatic-registration","text":"Knative supports automatic registration of EventTypes for the following event sources: CronJobSource ApiServerSource GithubSource GcpPubSubSource KafkaSource AwsSqsSource Knative only supports automatic creation of EventTypes for sources that have a Broker as their sink.","title":"Support for automatic registration"},{"location":"eventing/event-registry/#procedure-for-automatic-registration","text":"To register EventTypes automatically, apply your event source YAML file by running the command: kubectl apply -f <event-source.yaml> After your event source is instantiated, EventTypes are added to the registry.","title":"Procedure for automatic registration"},{"location":"eventing/event-registry/#example-automatic-registration-using-kafkasource","text":"Given the following KafkaSource sample to populate the registry: apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-sample namespace : default spec : bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 topics : - knative-demo - news sink : apiVersion : eventing.knative.dev/v1 kind : Broker name : default The topics field in the above example is used to generate the EventType source field. After running kubectl apply using the above YAML, the KafkaSource kafka-source-sample is instantiated, and two EventTypes are added to the registry because there are two topics.","title":"Example: Automatic registration using KafkaSource"},{"location":"eventing/event-registry/#discover-events-using-the-registry","text":"Using the registry, you can discover the different types of events that Broker event meshes can consume.","title":"Discover events using the registry"},{"location":"eventing/event-registry/#view-all-event-types-you-can-subscribe-to","text":"To see a list of event types in the registry that are available to subscribe to, run the command: kubectl get eventtypes -n <namespace> Example output using the default namespace in a testing cluster: NAME TYPE SOURCE SCHEMA BROKER DESCRIPTION READY REASON dev.knative.source.github.push-34cnb dev.knative.source.github.push https://github.com/knative/eventing default True dev.knative.source.github.push-44svn dev.knative.source.github.push https://github.com/knative/serving default True dev.knative.source.github.pullrequest-86jhv dev.knative.source.github.pull_request https://github.com/knative/eventing default True dev.knative.source.github.pullrequest-97shf dev.knative.source.github.pull_request https://github.com/knative/serving default True dev.knative.kafka.event-cjvcr dev.knative.kafka.event /apis/v1/namespaces/default/kafkasources/kafka-sample#news default True dev.knative.kafka.event-tdt48 dev.knative.kafka.event /apis/v1/namespaces/default/kafkasources/kafka-sample#knative-demo default True google.pubsub.topic.publish-hrxhh google.pubsub.topic.publish //pubsub.googleapis.com/knative/topics/testing dev False BrokerIsNotReady This example output shows seven different EventType objects in the registry of the default namespace. It assumes that the event sources emitting the events reference a Broker as their sink.","title":"View all event types you can subscribe to"},{"location":"eventing/event-registry/#view-the-yaml-for-an-eventtype-object","text":"To see the YAML for an EventType object, run the command: kubectl get eventtype <name> -o yaml Where <name> is the name of an EventType object and can be found in the NAME column of the registry output. For example, dev.knative.source.github.push-34cnb . For an example EventType YAML, see About EventType objects earlier on this page.","title":"View the YAML for an EventType object"},{"location":"eventing/event-registry/#about-subscribing-to-events","text":"After you know what events can be consumed from the Brokers' event meshes, you can create Triggers to subscribe to particular events. Here are a some example Triggers that subscribe to events using exact matching on type or source , based on the registry output mentioned earlier: Subscribes to GitHub pushes from any source: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : push-trigger namespace : default spec : broker : default filter : attributes : type : dev.knative.source.github.push subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : push-service Note As the example registry output mentioned, only two sources, the knative/eventing and knative/serving GitHub repositories, exist for that particular type of event. If later on new sources are registered for GitHub pushes, this Trigger is able to consume them. Subscribes to GitHub pull requests from the knative/eventing GitHub repository: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : gh-knative-eventing-pull-trigger namespace : default spec : broker : default filter : attributes : type : dev.knative.source.github.pull_request source : https://github.com/knative/eventing subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : gh-knative-eventing-pull-service Subscribes to Kafka messages sent to the knative-demo topic: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : kafka-knative-demo-trigger namespace : default spec : broker : default filter : attributes : type : dev.knative.kafka.event source : /apis/v1/namespaces/default/kafkasources/kafka-sample#knative-demo subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : kafka-knative-demo-service Subscribes to PubSub messages from GCP's knative project sent to the testing topic: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : gcp-pubsub-knative-testing-trigger namespace : default spec : broker : dev filter : attributes : source : //pubsub.googleapis.com/knative/topics/testing subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : gcp-pubsub-knative-testing-service Note The example registry output mentioned earlier lists this Broker's readiness as false . This Trigger's subscriber cannot consume events until the Broker becomes ready.","title":"About subscribing to events"},{"location":"eventing/event-registry/#next-steps","text":"Knative code samples is a useful resource to better understand some of the event sources. Remember, you must point the sources to a Broker if you want automatic registration of EventTypes in the registry.","title":"Next steps"},{"location":"eventing/getting-started/","text":"Getting Started with Knative Eventing \u00b6 After you install Knative Eventing, you can create, send, and verify events. This guide shows how you can use a basic workflow for managing events. Before you start to manage events, you must create the objects needed to transport the events. Creating a Knative Eventing namespace \u00b6 Namespaces are used to group together and organize your Knative resources. Create a new namespace called event-example by entering the following command: kubectl create namespace event-example Adding a broker to the namespace \u00b6 The broker allows you to route events to different event sinks or consumers. Add a broker named default to your namespace by copying the following YAML into a file: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : default namespace : event-example Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Verify that the broker is working correctly, by entering the following command: kubectl -n event-example get broker default This shows information about your broker. If the broker is working correctly, it shows a READY status of True : NAME READY REASON URL AGE default True http://broker-ingress.knative-eventing.svc.cluster.local/event-example/default 1m If READY is False , wait a few moments and then run the command again. If you continue to receive the False status, see the Debugging Guide to troubleshoot the issue. Creating event consumers \u00b6 In this step, you create two event consumers, hello-display and goodbye-display , to demonstrate how you can configure your event producers to target a specific consumer. To deploy the hello-display consumer to your cluster, copy the following YAML into a file: apiVersion : apps/v1 kind : Deployment metadata : name : hello-display namespace : event-example spec : replicas : 1 selector : matchLabels : &labels app : hello-display template : metadata : labels : *labels spec : containers : - name : event-display image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- kind : Service apiVersion : v1 metadata : name : hello-display namespace : event-example spec : selector : app : hello-display ports : - protocol : TCP port : 80 targetPort : 8080 Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To deploy the goodbye-display consumer to your cluster, copy the following YAML into a file: apiVersion : apps/v1 kind : Deployment metadata : name : goodbye-display namespace : event-example spec : replicas : 1 selector : matchLabels : &labels app : goodbye-display template : metadata : labels : *labels spec : containers : - name : event-display # Source code: https://github.com/knative/eventing/tree/main/cmd/event_display image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- kind : Service apiVersion : v1 metadata : name : goodbye-display namespace : event-example spec : selector : app : goodbye-display ports : - protocol : TCP port : 80 targetPort : 8080 Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Verify that the event consumers are working by entering the following command: kubectl -n event-example get deployments hello-display goodbye-display This lists the hello-display and goodbye-display consumers that you deployed: NAME READY UP-TO-DATE AVAILABLE AGE hello-display 1 /1 1 1 26s goodbye-display 1 /1 1 1 16s The number of replicas in the READY column should match the number of replicas in the AVAILABLE column. If the numbers do not match, see the Debugging Guide to troubleshoot the issue. Creating triggers \u00b6 A trigger defines the events that each event consumer receives. Brokers use triggers to forward events to the correct consumers. Each trigger can specify a filter that enables selection of relevant events based on the Cloud Event context attributes. Create a trigger by copying the following YAML into a file: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : hello-display namespace : event-example spec : broker : default filter : attributes : type : greeting subscriber : ref : apiVersion : v1 kind : Service name : hello-display The command creates a trigger that sends all events of type greeting to your event consumer named hello-display . Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To add a second trigger, copy the following YAML into a file: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : goodbye-display namespace : event-example spec : broker : default filter : attributes : source : sendoff subscriber : ref : apiVersion : v1 kind : Service name : goodbye-display The command creates a trigger that sends all events of source sendoff to your event consumer named goodbye-display . Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Verify that the triggers are working correctly by running the following command: kubectl -n event-example get triggers This returns the hello-display and goodbye-display triggers that you created: NAME READY REASON BROKER SUBSCRIBER_URI AGE goodbye-display True default http://goodbye-display.event-example.svc.cluster.local/ 9s hello-display True default http://hello-display.event-example.svc.cluster.local/ 16s If the triggers are correctly configured, they will be ready and pointing to the correct broker ( default ) and SUBSCRIBER_URI . The SUBSCRIBER_URI has a value similar to triggerName.namespaceName.svc.cluster.local . The exact value depends on the broker implementation. If this value looks incorrect, see the Debugging Guide to troubleshoot the issue. Creating a pod as an event producer \u00b6 This guide uses curl commands to manually send individual events as HTTP requests to the broker, and demonstrate how these events are received by the correct event consumer. The broker can only be accessed from within the cluster where Knative Eventing is installed. You must create a pod within that cluster to act as an event producer that will execute the curl commands. To create a pod, copy the following YAML into a file: apiVersion : v1 kind : Pod metadata : labels : run : curl name : curl namespace : event-example spec : containers : # This could be any image that we can SSH into and has curl. - image : radial/busyboxplus:curl imagePullPolicy : IfNotPresent name : curl resources : {} stdin : true terminationMessagePath : /dev/termination-log terminationMessagePolicy : File tty : true Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Sending events to the broker \u00b6 SSH into the pod by running the following command: kubectl -n event-example attach curl -it You will see a prompt similar to the following: Defaulting container name to curl. Use 'kubectl describe pod/ -n event-example' to see all of the containers in this pod. If you don ' t see a command prompt, try pressing enter. [ root@curl:/ ] $ Make a HTTP request to the broker. To show the various types of events you can send, you will make three requests: To make the first request, which creates an event that has the type greeting , run the following in the SSH terminal: curl -v \"http://broker-ingress.knative-eventing.svc.cluster.local/event-example/default\" \\ -X POST \\ -H \"Ce-Id: say-hello\" \\ -H \"Ce-Specversion: 1.0\" \\ -H \"Ce-Type: greeting\" \\ -H \"Ce-Source: not-sendoff\" \\ -H \"Content-Type: application/json\" \\ -d '{\"msg\":\"Hello Knative!\"}' When the broker receives your event, hello-display will activate and send it to the event consumer of the same name. If the event has been received, you will receive a 202 Accepted response similar to the following example: < HTTP/1.1 202 Accepted < Content-Length: 0 < Date: Mon, 12 Aug 2019 19 :48:18 GMT To make the second request, which creates an event that has the source sendoff , run the following in the SSH terminal: curl -v \"http://broker-ingress.knative-eventing.svc.cluster.local/event-example/default\" \\ -X POST \\ -H \"Ce-Id: say-goodbye\" \\ -H \"Ce-Specversion: 1.0\" \\ -H \"Ce-Type: not-greeting\" \\ -H \"Ce-Source: sendoff\" \\ -H \"Content-Type: application/json\" \\ -d '{\"msg\":\"Goodbye Knative!\"}' When the broker receives your event, goodbye-display will activate and send the event to the event consumer of the same name. If the event has been received, you will receive a 202 Accepted response similar to the following example: < HTTP/1.1 202 Accepted < Content-Length: 0 < Date: Mon, 12 Aug 2019 19:48:18 GMT To make the third request, which creates an event that has the type greeting and the source sendoff , run the following in the SSH terminal: curl -v \"http://broker-ingress.knative-eventing.svc.cluster.local/event-example/default\" \\ -X POST \\ -H \"Ce-Id: say-hello-goodbye\" \\ -H \"Ce-Specversion: 1.0\" \\ -H \"Ce-Type: greeting\" \\ -H \"Ce-Source: sendoff\" \\ -H \"Content-Type: application/json\" \\ -d '{\"msg\":\"Hello Knative! Goodbye Knative!\"}' When the broker receives your event, hello-display and goodbye-display will activate and send the event to the event consumers of the same name. If the event has been received, you will receive a 202 Accepted response similar to the following example: < HTTP/1.1 202 Accepted < Content-Length: 0 < Date: Mon, 12 Aug 2019 19 :48:18 GMT Exit SSH by typing exit into the command prompt. You have sent two events to the hello-display event consumer and two events to the goodbye-display event consumer (note that say-hello-goodbye activates the trigger conditions for both hello-display and goodbye-display ). You will verify that these events were received correctly in the next section. Verifying that events were received \u00b6 After you send the events, verify that the events were received by the correct subscribers. Look at the logs for the hello-display event consumer by entering the following command: kubectl -n event-example logs -l app = hello-display --tail = 100 This returns the Attributes and Data of the events you sent to hello-display : \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: greeting source: not-sendoff id: say-hello time: 2019 -05-20T17:59:43.81718488Z contenttype: application/json Extensions, knativehistory: default-broker-srk54-channel-24gls.event-example.svc.cluster.local Data, { \"msg\" : \"Hello Knative!\" } \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: greeting source: sendoff id: say-hello-goodbye time: 2019 -05-20T17:59:54.211866425Z contenttype: application/json Extensions, knativehistory: default-broker-srk54-channel-24gls.event-example.svc.cluster.local Data, { \"msg\" : \"Hello Knative! Goodbye Knative!\" } Look at the logs for the goodbye-display event consumer by entering the following command: kubectl -n event-example logs -l app = goodbye-display --tail = 100 This returns the Attributes and Data of the events you sent to goodbye-display : \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: not-greeting source: sendoff id: say-goodbye time: 2019 -05-20T17:59:49.044926148Z contenttype: application/json Extensions, knativehistory: default-broker-srk54-channel-24gls.event-example.svc.cluster.local Data, { \"msg\" : \"Goodbye Knative!\" } \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: greeting source: sendoff id: say-hello-goodbye time: 2019 -05-20T17:59:54.211866425Z contenttype: application/json Extensions, knativehistory: default-broker-srk54-channel-24gls.event-example.svc.cluster.local Data, { \"msg\" : \"Hello Knative! Goodbye Knative!\" } Cleaning up example resources \u00b6 You can delete the event-example namespace and its associated resources from your cluster if you do not plan to use it again in the future. Delete the event-example namespace and all of its resources from your cluster by entering the following command: kubectl delete namespace event-example","title":"Eventing\u5165\u95e8"},{"location":"eventing/getting-started/#getting-started-with-knative-eventing","text":"After you install Knative Eventing, you can create, send, and verify events. This guide shows how you can use a basic workflow for managing events. Before you start to manage events, you must create the objects needed to transport the events.","title":"Getting Started with Knative Eventing"},{"location":"eventing/getting-started/#creating-a-knative-eventing-namespace","text":"Namespaces are used to group together and organize your Knative resources. Create a new namespace called event-example by entering the following command: kubectl create namespace event-example","title":"Creating a Knative Eventing namespace"},{"location":"eventing/getting-started/#adding-a-broker-to-the-namespace","text":"The broker allows you to route events to different event sinks or consumers. Add a broker named default to your namespace by copying the following YAML into a file: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : default namespace : event-example Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Verify that the broker is working correctly, by entering the following command: kubectl -n event-example get broker default This shows information about your broker. If the broker is working correctly, it shows a READY status of True : NAME READY REASON URL AGE default True http://broker-ingress.knative-eventing.svc.cluster.local/event-example/default 1m If READY is False , wait a few moments and then run the command again. If you continue to receive the False status, see the Debugging Guide to troubleshoot the issue.","title":"Adding a broker to the namespace"},{"location":"eventing/getting-started/#creating-event-consumers","text":"In this step, you create two event consumers, hello-display and goodbye-display , to demonstrate how you can configure your event producers to target a specific consumer. To deploy the hello-display consumer to your cluster, copy the following YAML into a file: apiVersion : apps/v1 kind : Deployment metadata : name : hello-display namespace : event-example spec : replicas : 1 selector : matchLabels : &labels app : hello-display template : metadata : labels : *labels spec : containers : - name : event-display image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- kind : Service apiVersion : v1 metadata : name : hello-display namespace : event-example spec : selector : app : hello-display ports : - protocol : TCP port : 80 targetPort : 8080 Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To deploy the goodbye-display consumer to your cluster, copy the following YAML into a file: apiVersion : apps/v1 kind : Deployment metadata : name : goodbye-display namespace : event-example spec : replicas : 1 selector : matchLabels : &labels app : goodbye-display template : metadata : labels : *labels spec : containers : - name : event-display # Source code: https://github.com/knative/eventing/tree/main/cmd/event_display image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- kind : Service apiVersion : v1 metadata : name : goodbye-display namespace : event-example spec : selector : app : goodbye-display ports : - protocol : TCP port : 80 targetPort : 8080 Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Verify that the event consumers are working by entering the following command: kubectl -n event-example get deployments hello-display goodbye-display This lists the hello-display and goodbye-display consumers that you deployed: NAME READY UP-TO-DATE AVAILABLE AGE hello-display 1 /1 1 1 26s goodbye-display 1 /1 1 1 16s The number of replicas in the READY column should match the number of replicas in the AVAILABLE column. If the numbers do not match, see the Debugging Guide to troubleshoot the issue.","title":"Creating event consumers"},{"location":"eventing/getting-started/#creating-triggers","text":"A trigger defines the events that each event consumer receives. Brokers use triggers to forward events to the correct consumers. Each trigger can specify a filter that enables selection of relevant events based on the Cloud Event context attributes. Create a trigger by copying the following YAML into a file: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : hello-display namespace : event-example spec : broker : default filter : attributes : type : greeting subscriber : ref : apiVersion : v1 kind : Service name : hello-display The command creates a trigger that sends all events of type greeting to your event consumer named hello-display . Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To add a second trigger, copy the following YAML into a file: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : goodbye-display namespace : event-example spec : broker : default filter : attributes : source : sendoff subscriber : ref : apiVersion : v1 kind : Service name : goodbye-display The command creates a trigger that sends all events of source sendoff to your event consumer named goodbye-display . Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Verify that the triggers are working correctly by running the following command: kubectl -n event-example get triggers This returns the hello-display and goodbye-display triggers that you created: NAME READY REASON BROKER SUBSCRIBER_URI AGE goodbye-display True default http://goodbye-display.event-example.svc.cluster.local/ 9s hello-display True default http://hello-display.event-example.svc.cluster.local/ 16s If the triggers are correctly configured, they will be ready and pointing to the correct broker ( default ) and SUBSCRIBER_URI . The SUBSCRIBER_URI has a value similar to triggerName.namespaceName.svc.cluster.local . The exact value depends on the broker implementation. If this value looks incorrect, see the Debugging Guide to troubleshoot the issue.","title":"Creating triggers"},{"location":"eventing/getting-started/#creating-a-pod-as-an-event-producer","text":"This guide uses curl commands to manually send individual events as HTTP requests to the broker, and demonstrate how these events are received by the correct event consumer. The broker can only be accessed from within the cluster where Knative Eventing is installed. You must create a pod within that cluster to act as an event producer that will execute the curl commands. To create a pod, copy the following YAML into a file: apiVersion : v1 kind : Pod metadata : labels : run : curl name : curl namespace : event-example spec : containers : # This could be any image that we can SSH into and has curl. - image : radial/busyboxplus:curl imagePullPolicy : IfNotPresent name : curl resources : {} stdin : true terminationMessagePath : /dev/termination-log terminationMessagePolicy : File tty : true Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"Creating a pod as an event producer"},{"location":"eventing/getting-started/#sending-events-to-the-broker","text":"SSH into the pod by running the following command: kubectl -n event-example attach curl -it You will see a prompt similar to the following: Defaulting container name to curl. Use 'kubectl describe pod/ -n event-example' to see all of the containers in this pod. If you don ' t see a command prompt, try pressing enter. [ root@curl:/ ] $ Make a HTTP request to the broker. To show the various types of events you can send, you will make three requests: To make the first request, which creates an event that has the type greeting , run the following in the SSH terminal: curl -v \"http://broker-ingress.knative-eventing.svc.cluster.local/event-example/default\" \\ -X POST \\ -H \"Ce-Id: say-hello\" \\ -H \"Ce-Specversion: 1.0\" \\ -H \"Ce-Type: greeting\" \\ -H \"Ce-Source: not-sendoff\" \\ -H \"Content-Type: application/json\" \\ -d '{\"msg\":\"Hello Knative!\"}' When the broker receives your event, hello-display will activate and send it to the event consumer of the same name. If the event has been received, you will receive a 202 Accepted response similar to the following example: < HTTP/1.1 202 Accepted < Content-Length: 0 < Date: Mon, 12 Aug 2019 19 :48:18 GMT To make the second request, which creates an event that has the source sendoff , run the following in the SSH terminal: curl -v \"http://broker-ingress.knative-eventing.svc.cluster.local/event-example/default\" \\ -X POST \\ -H \"Ce-Id: say-goodbye\" \\ -H \"Ce-Specversion: 1.0\" \\ -H \"Ce-Type: not-greeting\" \\ -H \"Ce-Source: sendoff\" \\ -H \"Content-Type: application/json\" \\ -d '{\"msg\":\"Goodbye Knative!\"}' When the broker receives your event, goodbye-display will activate and send the event to the event consumer of the same name. If the event has been received, you will receive a 202 Accepted response similar to the following example: < HTTP/1.1 202 Accepted < Content-Length: 0 < Date: Mon, 12 Aug 2019 19:48:18 GMT To make the third request, which creates an event that has the type greeting and the source sendoff , run the following in the SSH terminal: curl -v \"http://broker-ingress.knative-eventing.svc.cluster.local/event-example/default\" \\ -X POST \\ -H \"Ce-Id: say-hello-goodbye\" \\ -H \"Ce-Specversion: 1.0\" \\ -H \"Ce-Type: greeting\" \\ -H \"Ce-Source: sendoff\" \\ -H \"Content-Type: application/json\" \\ -d '{\"msg\":\"Hello Knative! Goodbye Knative!\"}' When the broker receives your event, hello-display and goodbye-display will activate and send the event to the event consumers of the same name. If the event has been received, you will receive a 202 Accepted response similar to the following example: < HTTP/1.1 202 Accepted < Content-Length: 0 < Date: Mon, 12 Aug 2019 19 :48:18 GMT Exit SSH by typing exit into the command prompt. You have sent two events to the hello-display event consumer and two events to the goodbye-display event consumer (note that say-hello-goodbye activates the trigger conditions for both hello-display and goodbye-display ). You will verify that these events were received correctly in the next section.","title":"Sending events to the broker"},{"location":"eventing/getting-started/#verifying-that-events-were-received","text":"After you send the events, verify that the events were received by the correct subscribers. Look at the logs for the hello-display event consumer by entering the following command: kubectl -n event-example logs -l app = hello-display --tail = 100 This returns the Attributes and Data of the events you sent to hello-display : \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: greeting source: not-sendoff id: say-hello time: 2019 -05-20T17:59:43.81718488Z contenttype: application/json Extensions, knativehistory: default-broker-srk54-channel-24gls.event-example.svc.cluster.local Data, { \"msg\" : \"Hello Knative!\" } \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: greeting source: sendoff id: say-hello-goodbye time: 2019 -05-20T17:59:54.211866425Z contenttype: application/json Extensions, knativehistory: default-broker-srk54-channel-24gls.event-example.svc.cluster.local Data, { \"msg\" : \"Hello Knative! Goodbye Knative!\" } Look at the logs for the goodbye-display event consumer by entering the following command: kubectl -n event-example logs -l app = goodbye-display --tail = 100 This returns the Attributes and Data of the events you sent to goodbye-display : \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: not-greeting source: sendoff id: say-goodbye time: 2019 -05-20T17:59:49.044926148Z contenttype: application/json Extensions, knativehistory: default-broker-srk54-channel-24gls.event-example.svc.cluster.local Data, { \"msg\" : \"Goodbye Knative!\" } \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: greeting source: sendoff id: say-hello-goodbye time: 2019 -05-20T17:59:54.211866425Z contenttype: application/json Extensions, knativehistory: default-broker-srk54-channel-24gls.event-example.svc.cluster.local Data, { \"msg\" : \"Hello Knative! Goodbye Knative!\" }","title":"Verifying that events were received"},{"location":"eventing/getting-started/#cleaning-up-example-resources","text":"You can delete the event-example namespace and its associated resources from your cluster if you do not plan to use it again in the future. Delete the event-example namespace and all of its resources from your cluster by entering the following command: kubectl delete namespace event-example","title":"Cleaning up example resources"},{"location":"eventing/broker/","text":"Brokers \u00b6 Brokers are Kubernetes custom resources that define an event mesh for collecting a pool of CloudEvents . Brokers provide a discoverable endpoint, status.address , for event ingress, and triggers for event delivery. Event producers can send events to a broker by POSTing the event to the status.address.url of the broker. Event delivery mechanics are an implementation detail that depend on the configured broker class . Using brokers and triggers abstracts the details of event routing from the event producer and event consumer. After an event has entered a broker, it can be forwarded to subscribers by using triggers. Triggers allow events to be filtered by attributes, so that events with particular attributes can be sent to subscribers that have registered interest in events with those attributes. A subscriber can be any URL or Addressable resource. Subscribers can also reply to an active request from the broker, and can respond with a new CloudEvent that will be sent back into the broker. For most use cases, a single broker per namespace is sufficient, but there are several use cases where multiple brokers can simplify architecture. For example, separate brokers for events containing Personally Identifiable Information (PII) and non-PII events can simplify audit and access control rules. Broker types \u00b6 The following broker types are available for use with Knative Eventing. Multi-tenant channel-based broker \u00b6 Knative Eventing provides a multi-tenant (MT) channel-based broker implementation that uses channels for event routing. Before you can use the MT channel-based broker, you must install a channel implementation . Alternative broker implementations \u00b6 In the Knative Eventing ecosystem, alternative broker implementations are welcome as long as they respect the broker specifications . The following is a list of brokers provided by the community or vendors: GCP broker \u00b6 The GCP broker is optimized for running in GCP. For more details, refer to the documentation . Apache Kafka broker \u00b6 For more information, see Apache Kafka Broker . RabbitMQ broker \u00b6 The RabbitMQ Broker uses RabbitMQ for its underlying implementation. For more information, see RabbitMQ Broker or the docs available on GitHub . Next steps \u00b6 Create an MT channel-based broker . Configure default broker ConfigMap settings . View the broker specifications .","title":"About Brokers"},{"location":"eventing/broker/#brokers","text":"Brokers are Kubernetes custom resources that define an event mesh for collecting a pool of CloudEvents . Brokers provide a discoverable endpoint, status.address , for event ingress, and triggers for event delivery. Event producers can send events to a broker by POSTing the event to the status.address.url of the broker. Event delivery mechanics are an implementation detail that depend on the configured broker class . Using brokers and triggers abstracts the details of event routing from the event producer and event consumer. After an event has entered a broker, it can be forwarded to subscribers by using triggers. Triggers allow events to be filtered by attributes, so that events with particular attributes can be sent to subscribers that have registered interest in events with those attributes. A subscriber can be any URL or Addressable resource. Subscribers can also reply to an active request from the broker, and can respond with a new CloudEvent that will be sent back into the broker. For most use cases, a single broker per namespace is sufficient, but there are several use cases where multiple brokers can simplify architecture. For example, separate brokers for events containing Personally Identifiable Information (PII) and non-PII events can simplify audit and access control rules.","title":"Brokers"},{"location":"eventing/broker/#broker-types","text":"The following broker types are available for use with Knative Eventing.","title":"Broker types"},{"location":"eventing/broker/#multi-tenant-channel-based-broker","text":"Knative Eventing provides a multi-tenant (MT) channel-based broker implementation that uses channels for event routing. Before you can use the MT channel-based broker, you must install a channel implementation .","title":"Multi-tenant channel-based broker"},{"location":"eventing/broker/#alternative-broker-implementations","text":"In the Knative Eventing ecosystem, alternative broker implementations are welcome as long as they respect the broker specifications . The following is a list of brokers provided by the community or vendors:","title":"Alternative broker implementations"},{"location":"eventing/broker/#gcp-broker","text":"The GCP broker is optimized for running in GCP. For more details, refer to the documentation .","title":"GCP broker"},{"location":"eventing/broker/#apache-kafka-broker","text":"For more information, see Apache Kafka Broker .","title":"Apache Kafka broker"},{"location":"eventing/broker/#rabbitmq-broker","text":"The RabbitMQ Broker uses RabbitMQ for its underlying implementation. For more information, see RabbitMQ Broker or the docs available on GitHub .","title":"RabbitMQ broker"},{"location":"eventing/broker/#next-steps","text":"Create an MT channel-based broker . Configure default broker ConfigMap settings . View the broker specifications .","title":"Next steps"},{"location":"eventing/broker/create-mtbroker/","text":"Creating a broker \u00b6 Once you have installed Knative Eventing, you can create an instance of the multi-tenant (MT) channel-based broker that is provided by default. The default backing channel type for an MT channel-based broker is InMemoryChannel. You can create a broker by using the kn CLI or by applying YAML files using kubectl . kn kubectl You can create a broker in current namespace by entering the following command: kn broker create <broker-name> -n <namespace> Note If you choose not to specify a namespace, the broker will be created in the current namespace. Optional: Verify that the broker was created by listing existing brokers. Enter the following command: kn broker list Optional: You can also verify the broker exists by describing the broker you have created. Enter the following command: kn broker describe <broker-name> The YAML in the following example creates a broker named default in the current namespace. For more information about configuring broker options using YAML, see the full broker configuration example . Create a broker in the current namespace by creating a YAML file using the following template: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : <broker-name> Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Optional: Verify that the broker is working correctly, by entering the following command: kubectl -n <namespace> get broker <broker-name> This shows information about your broker. If the broker is working correctly, it shows a READY status of True : NAME READY REASON URL AGE default True http://broker-ingress.knative-eventing.svc.cluster.local/event-example/default 1m If the READY status is False , wait a few moments and then run the command again.","title":"Creating a Broker"},{"location":"eventing/broker/create-mtbroker/#creating-a-broker","text":"Once you have installed Knative Eventing, you can create an instance of the multi-tenant (MT) channel-based broker that is provided by default. The default backing channel type for an MT channel-based broker is InMemoryChannel. You can create a broker by using the kn CLI or by applying YAML files using kubectl . kn kubectl You can create a broker in current namespace by entering the following command: kn broker create <broker-name> -n <namespace> Note If you choose not to specify a namespace, the broker will be created in the current namespace. Optional: Verify that the broker was created by listing existing brokers. Enter the following command: kn broker list Optional: You can also verify the broker exists by describing the broker you have created. Enter the following command: kn broker describe <broker-name> The YAML in the following example creates a broker named default in the current namespace. For more information about configuring broker options using YAML, see the full broker configuration example . Create a broker in the current namespace by creating a YAML file using the following template: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : <broker-name> Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Optional: Verify that the broker is working correctly, by entering the following command: kubectl -n <namespace> get broker <broker-name> This shows information about your broker. If the broker is working correctly, it shows a READY status of True : NAME READY REASON URL AGE default True http://broker-ingress.knative-eventing.svc.cluster.local/event-example/default 1m If the READY status is False , wait a few moments and then run the command again.","title":"Creating a broker"},{"location":"eventing/broker/example-mtbroker/","text":"Broker configuration example \u00b6 The following is a full example of a multi-tenant (MT) channel-based Broker object which shows the possible configuration options that you can modify: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : default namespace : default annotations : eventing.knative.dev/broker.class : MTChannelBasedBroker spec : config : apiVersion : v1 kind : ConfigMap name : config-br-default-channel namespace : knative-eventing delivery : deadLetterSink : ref : kind : Service namespace : example-namespace name : example-service apiVersion : v1 uri : example-uri retry : 5 backoffPolicy : exponential backoffDelay : \"2007-03-01T13:00:00Z/P1Y2M10DT2H30M\" You can specify any valid name for your broker. Using default will create a broker named default . The namespace must be an existing namespace in your cluster. Using default will create the broker in the current namespace. You can set the eventing.knative.dev/broker.class annotation to change the class of the broker. The default broker class is MTChannelBasedBroker , but Knative also supports use of the Kafka broker class. For more information about Kafka brokers, see the Apache Kafka Broker documentation. spec.config is used to specify the default backing channel configuration for MT channel-based broker implementations. For more information on configuring the default channel type, see the documentation on Configure Broker defaults . spec.delivery is used to configure event delivery options. Event delivery options specify what happens to an event that fails to be delivered to an event sink. For more information, see the documentation on Event delivery .","title":"Broker configuration example"},{"location":"eventing/broker/example-mtbroker/#broker-configuration-example","text":"The following is a full example of a multi-tenant (MT) channel-based Broker object which shows the possible configuration options that you can modify: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : default namespace : default annotations : eventing.knative.dev/broker.class : MTChannelBasedBroker spec : config : apiVersion : v1 kind : ConfigMap name : config-br-default-channel namespace : knative-eventing delivery : deadLetterSink : ref : kind : Service namespace : example-namespace name : example-service apiVersion : v1 uri : example-uri retry : 5 backoffPolicy : exponential backoffDelay : \"2007-03-01T13:00:00Z/P1Y2M10DT2H30M\" You can specify any valid name for your broker. Using default will create a broker named default . The namespace must be an existing namespace in your cluster. Using default will create the broker in the current namespace. You can set the eventing.knative.dev/broker.class annotation to change the class of the broker. The default broker class is MTChannelBasedBroker , but Knative also supports use of the Kafka broker class. For more information about Kafka brokers, see the Apache Kafka Broker documentation. spec.config is used to specify the default backing channel configuration for MT channel-based broker implementations. For more information on configuring the default channel type, see the documentation on Configure Broker defaults . spec.delivery is used to configure event delivery options. Event delivery options specify what happens to an event that fails to be delivered to an event sink. For more information, see the documentation on Event delivery .","title":"Broker configuration example"},{"location":"eventing/broker/kafka-broker/","text":"Knative Kafka Broker \u00b6 The Knative Kafka Broker is an Apache Kafka native implementation of the Knative Broker API that reduces network hops, supports any Kafka version, and has a better integration with Kafka for the Broker and Trigger model. Notable features are: Control plane High Availability Horizontally scalable data plane Extensively configurable Ordered delivery of events based on CloudEvents partitioning extension Support any Kafka version, see compatibility matrix Prerequisites \u00b6 Installing Eventing using YAML files . An Apache Kafka cluster (if you're just getting started you can follow Strimzi Quickstart page ). Installation \u00b6 Install the Kafka controller by entering the following command: kubectl apply --filename https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Install the Kafka Broker data plane by entering the following command: kubectl apply --filename https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-broker.yaml Verify that kafka-controller , kafka-broker-receiver and kafka-broker-dispatcher are running, by entering the following command: kubectl get deployments.apps -n knative-eventing Example output: NAME READY UP-TO-DATE AVAILABLE AGE eventing-controller 1 /1 1 1 10s eventing-webhook 1 /1 1 1 9s kafka-controller 1 /1 1 1 3s kafka-broker-dispatcher 1 /1 1 1 4s kafka-broker-receiver 1 /1 1 1 5s Create a Kafka Broker \u00b6 A Kafka Broker object looks like this: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : # case-sensitive eventing.knative.dev/broker.class : Kafka name : default namespace : default spec : # Configuration specific to this broker. config : apiVersion : v1 kind : ConfigMap name : kafka-broker-config namespace : knative-eventing # Optional dead letter sink, you can specify either: # - deadLetterSink.ref, which is a reference to a Callable # - deadLetterSink.uri, which is an absolute URI to a Callable (It can potentially be out of the Kubernetes cluster) delivery : deadLetterSink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : dlq-service spec.config should reference any ConfigMap that looks like the following: apiVersion : v1 kind : ConfigMap metadata : name : kafka-broker-config namespace : knative-eventing data : # Number of topic partitions default.topic.partitions : \"10\" # Replication factor of topic messages. default.topic.replication.factor : \"1\" # A comma separated list of bootstrap servers. (It can be in or out the k8s cluster) bootstrap.servers : \"my-cluster-kafka-bootstrap.kafka:9092\" This ConfigMap is installed in the cluster. You can edit the configuration or create a new one with the same values depending on your needs. Note The default.topic.replication.factor value must be less than or equal to the number of Kafka broker instances in your cluster. For example, if you only have one Kafka broker, the default.topic.replication.factor value should not be more than 1 . Set as default broker implementation \u00b6 To set the Kafka broker as the default implementation for all brokers in the Knative deployment, you can apply global settings by modifying the config-br-defaults ConfigMap in the knative-eventing namespace. This allows you to avoid configuring individual or per-namespace settings for each broker, such as metadata.annotations.eventing.knative.dev/broker.class or spec.config . The following YAML is an example of a config-br-defaults ConfigMap using Kafka broker as the default implementation. apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing data : default-br-config : | clusterDefault: brokerClass: Kafka apiVersion: v1 kind: ConfigMap name: kafka-broker-config namespace: knative-eventing namespaceDefaults: namespace1: brokerClass: Kafka apiVersion: v1 kind: ConfigMap name: kafka-broker-config namespace: knative-eventing namespace2: brokerClass: Kafka apiVersion: v1 kind: ConfigMap name: kafka-broker-config namespace: knative-eventing Security \u00b6 Apache Kafka supports different security features, Knative supports the followings: Authentication using SASL without encryption Authentication using SASL and encryption using SSL Authentication and encryption using SSL Encryption using SSL without client authentication To enable security features, in the ConfigMap referenced by broker.spec.config , we can reference a Secret : apiVersion : v1 kind : ConfigMap metadata : name : kafka-broker-config namespace : knative-eventing data : # Other configurations # ... # Reference a Secret called my_secret auth.secret.ref.name : my_secret The Secret my_secret must exist in the same namespace of the ConfigMap referenced by broker.spec.config , in this case: knative-eventing . Note Certificates and keys must be in PEM format . Authentication using SASL \u00b6 Knative supports the following SASL mechanisms: PLAIN SCRAM-SHA-256 SCRAM-SHA-512 To use a specific SASL mechanism replace <sasl_mechanism> with the mechanism of your choice. Authentication using SASL without encryption \u00b6 kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SASL_PLAINTEXT \\ --from-literal = sasl.mechanism = <sasl_mechanism> \\ --from-literal = user = <my_user> \\ --from-literal = password = <my_password> Authentication using SASL and encryption using SSL \u00b6 kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SASL_SSL \\ --from-literal = sasl.mechanism = <sasl_mechanism> \\ --from-file = ca.crt = caroot.pem \\ --from-literal = user = <my_user> \\ --from-literal = password = <my_password> Encryption using SSL without client authentication \u00b6 kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SSL \\ --from-file = ca.crt = <my_caroot.pem_file_path> \\ --from-literal = user.skip = true Authentication and encryption using SSL \u00b6 kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SSL \\ --from-file = ca.crt = <my_caroot.pem_file_path> \\ --from-file = user.crt = <my_cert.pem_file_path> \\ --from-file = user.key = <my_key.pem_file_path> Note ca.crt can be omitted to fallback to use system's root CA set. Consumer Offsets Commit Interval \u00b6 Kafka consumers keep track of the last successfully sent events by committing offsets. Knative Kafka Broker commits the offset every auto.commit.interval.ms milliseconds. Note To prevent negative impacts to performance, it is not recommended committing offsets every time an event is successfully sent to a subscriber. The interval can be changed by changing the config-kafka-broker-data-plane ConfigMap in the knative-eventing namespace by modifying the parameter auto.commit.interval.ms as follows: apiVersion : v1 kind : ConfigMap metadata : name : config-kafka-broker-data-plane namespace : knative-eventing data : # Some configurations omitted ... config-kafka-broker-consumer.properties : | # Some configurations omitted ... # Commit the offset every 5000 millisecods (5 seconds) auto.commit.interval.ms=5000 Note Knative Kafka Broker guarantees at least once delivery, which means that your applications may receive duplicate events. A higher commit interval means that there is a higher probability of receiving duplicate events, because when a Consumer restarts, it restarts from the last committed offset. Kafka Producer and Consumer configurations \u00b6 Knative exposes all available Kafka producer and consumer configurations that can be modified to suit your workloads. You can change these configurations by modifying the config-kafka-broker-data-plane ConfigMap in the knative-eventing namespace. Documentation for the settings available in this ConfigMap is available on the Apache Kafka website , in particular, Producer configurations and Consumer configurations . Enable debug logging for data plane components \u00b6 The following YAML shows the default logging configuration for data plane components, that is created during the installation step: apiVersion : v1 kind : ConfigMap metadata : name : kafka-config-logging namespace : knative-eventing data : config.xml : | <configuration> <appender name=\"jsonConsoleAppender\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder class=\"net.logstash.logback.encoder.LogstashEncoder\"/> </appender> <root level=\"INFO\"> <appender-ref ref=\"jsonConsoleAppender\"/> </root> </configuration> To change the logging level to DEBUG , you must: Apply the following kafka-config-logging ConfigMap or replace level=\"INFO\" with level=\"DEBUG\" to the ConfigMap kafka-config-logging : apiVersion : v1 kind : ConfigMap metadata : name : kafka-config-logging namespace : knative-eventing data : config.xml : | <configuration> <appender name=\"jsonConsoleAppender\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder class=\"net.logstash.logback.encoder.LogstashEncoder\"/> </appender> <root level=\"DEBUG\"> <appender-ref ref=\"jsonConsoleAppender\"/> </root> </configuration> Restart the kafka-broker-receiver and the kafka-broker-dispatcher , by entering the following commands: kubectl rollout restart deployment -n knative-eventing kafka-broker-receiver kubectl rollout restart deployment -n knative-eventing kafka-broker-dispatcher Configuring the order of delivered events \u00b6 When dispatching events, the Kafka broker can be configured to support different delivery ordering guarantees. You can configure the delivery order of events using the kafka.eventing.knative.dev/delivery.order annotation on the Trigger object: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger annotations : kafka.eventing.knative.dev/delivery.order : ordered spec : broker : my-kafka-broker subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : my-service The supported consumer delivery guarantees are: unordered : An unordered consumer is a non-blocking consumer that delivers messages unordered, while preserving proper offset management. ordered : An ordered consumer is a per-partition blocking consumer that waits for a successful response from the CloudEvent subscriber before it delivers the next message of the partition. unordered is the default ordering guarantee, while ordered is considered unstable, use with caution . Additional information \u00b6 To report a bug or request a feature, open an issue in the eventing-kafka-broker repository .","title":"Knative Kafka Broker"},{"location":"eventing/broker/kafka-broker/#knative-kafka-broker","text":"The Knative Kafka Broker is an Apache Kafka native implementation of the Knative Broker API that reduces network hops, supports any Kafka version, and has a better integration with Kafka for the Broker and Trigger model. Notable features are: Control plane High Availability Horizontally scalable data plane Extensively configurable Ordered delivery of events based on CloudEvents partitioning extension Support any Kafka version, see compatibility matrix","title":"Knative Kafka Broker"},{"location":"eventing/broker/kafka-broker/#prerequisites","text":"Installing Eventing using YAML files . An Apache Kafka cluster (if you're just getting started you can follow Strimzi Quickstart page ).","title":"Prerequisites"},{"location":"eventing/broker/kafka-broker/#installation","text":"Install the Kafka controller by entering the following command: kubectl apply --filename https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Install the Kafka Broker data plane by entering the following command: kubectl apply --filename https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-broker.yaml Verify that kafka-controller , kafka-broker-receiver and kafka-broker-dispatcher are running, by entering the following command: kubectl get deployments.apps -n knative-eventing Example output: NAME READY UP-TO-DATE AVAILABLE AGE eventing-controller 1 /1 1 1 10s eventing-webhook 1 /1 1 1 9s kafka-controller 1 /1 1 1 3s kafka-broker-dispatcher 1 /1 1 1 4s kafka-broker-receiver 1 /1 1 1 5s","title":"Installation"},{"location":"eventing/broker/kafka-broker/#create-a-kafka-broker","text":"A Kafka Broker object looks like this: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : # case-sensitive eventing.knative.dev/broker.class : Kafka name : default namespace : default spec : # Configuration specific to this broker. config : apiVersion : v1 kind : ConfigMap name : kafka-broker-config namespace : knative-eventing # Optional dead letter sink, you can specify either: # - deadLetterSink.ref, which is a reference to a Callable # - deadLetterSink.uri, which is an absolute URI to a Callable (It can potentially be out of the Kubernetes cluster) delivery : deadLetterSink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : dlq-service spec.config should reference any ConfigMap that looks like the following: apiVersion : v1 kind : ConfigMap metadata : name : kafka-broker-config namespace : knative-eventing data : # Number of topic partitions default.topic.partitions : \"10\" # Replication factor of topic messages. default.topic.replication.factor : \"1\" # A comma separated list of bootstrap servers. (It can be in or out the k8s cluster) bootstrap.servers : \"my-cluster-kafka-bootstrap.kafka:9092\" This ConfigMap is installed in the cluster. You can edit the configuration or create a new one with the same values depending on your needs. Note The default.topic.replication.factor value must be less than or equal to the number of Kafka broker instances in your cluster. For example, if you only have one Kafka broker, the default.topic.replication.factor value should not be more than 1 .","title":"Create a Kafka Broker"},{"location":"eventing/broker/kafka-broker/#set-as-default-broker-implementation","text":"To set the Kafka broker as the default implementation for all brokers in the Knative deployment, you can apply global settings by modifying the config-br-defaults ConfigMap in the knative-eventing namespace. This allows you to avoid configuring individual or per-namespace settings for each broker, such as metadata.annotations.eventing.knative.dev/broker.class or spec.config . The following YAML is an example of a config-br-defaults ConfigMap using Kafka broker as the default implementation. apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing data : default-br-config : | clusterDefault: brokerClass: Kafka apiVersion: v1 kind: ConfigMap name: kafka-broker-config namespace: knative-eventing namespaceDefaults: namespace1: brokerClass: Kafka apiVersion: v1 kind: ConfigMap name: kafka-broker-config namespace: knative-eventing namespace2: brokerClass: Kafka apiVersion: v1 kind: ConfigMap name: kafka-broker-config namespace: knative-eventing","title":"Set as default broker implementation"},{"location":"eventing/broker/kafka-broker/#security","text":"Apache Kafka supports different security features, Knative supports the followings: Authentication using SASL without encryption Authentication using SASL and encryption using SSL Authentication and encryption using SSL Encryption using SSL without client authentication To enable security features, in the ConfigMap referenced by broker.spec.config , we can reference a Secret : apiVersion : v1 kind : ConfigMap metadata : name : kafka-broker-config namespace : knative-eventing data : # Other configurations # ... # Reference a Secret called my_secret auth.secret.ref.name : my_secret The Secret my_secret must exist in the same namespace of the ConfigMap referenced by broker.spec.config , in this case: knative-eventing . Note Certificates and keys must be in PEM format .","title":"Security"},{"location":"eventing/broker/kafka-broker/#authentication-using-sasl","text":"Knative supports the following SASL mechanisms: PLAIN SCRAM-SHA-256 SCRAM-SHA-512 To use a specific SASL mechanism replace <sasl_mechanism> with the mechanism of your choice.","title":"Authentication using SASL"},{"location":"eventing/broker/kafka-broker/#authentication-using-sasl-without-encryption","text":"kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SASL_PLAINTEXT \\ --from-literal = sasl.mechanism = <sasl_mechanism> \\ --from-literal = user = <my_user> \\ --from-literal = password = <my_password>","title":"Authentication using SASL without encryption"},{"location":"eventing/broker/kafka-broker/#authentication-using-sasl-and-encryption-using-ssl","text":"kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SASL_SSL \\ --from-literal = sasl.mechanism = <sasl_mechanism> \\ --from-file = ca.crt = caroot.pem \\ --from-literal = user = <my_user> \\ --from-literal = password = <my_password>","title":"Authentication using SASL and encryption using SSL"},{"location":"eventing/broker/kafka-broker/#encryption-using-ssl-without-client-authentication","text":"kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SSL \\ --from-file = ca.crt = <my_caroot.pem_file_path> \\ --from-literal = user.skip = true","title":"Encryption using SSL without client authentication"},{"location":"eventing/broker/kafka-broker/#authentication-and-encryption-using-ssl","text":"kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SSL \\ --from-file = ca.crt = <my_caroot.pem_file_path> \\ --from-file = user.crt = <my_cert.pem_file_path> \\ --from-file = user.key = <my_key.pem_file_path> Note ca.crt can be omitted to fallback to use system's root CA set.","title":"Authentication and encryption using SSL"},{"location":"eventing/broker/kafka-broker/#consumer-offsets-commit-interval","text":"Kafka consumers keep track of the last successfully sent events by committing offsets. Knative Kafka Broker commits the offset every auto.commit.interval.ms milliseconds. Note To prevent negative impacts to performance, it is not recommended committing offsets every time an event is successfully sent to a subscriber. The interval can be changed by changing the config-kafka-broker-data-plane ConfigMap in the knative-eventing namespace by modifying the parameter auto.commit.interval.ms as follows: apiVersion : v1 kind : ConfigMap metadata : name : config-kafka-broker-data-plane namespace : knative-eventing data : # Some configurations omitted ... config-kafka-broker-consumer.properties : | # Some configurations omitted ... # Commit the offset every 5000 millisecods (5 seconds) auto.commit.interval.ms=5000 Note Knative Kafka Broker guarantees at least once delivery, which means that your applications may receive duplicate events. A higher commit interval means that there is a higher probability of receiving duplicate events, because when a Consumer restarts, it restarts from the last committed offset.","title":"Consumer Offsets Commit Interval"},{"location":"eventing/broker/kafka-broker/#kafka-producer-and-consumer-configurations","text":"Knative exposes all available Kafka producer and consumer configurations that can be modified to suit your workloads. You can change these configurations by modifying the config-kafka-broker-data-plane ConfigMap in the knative-eventing namespace. Documentation for the settings available in this ConfigMap is available on the Apache Kafka website , in particular, Producer configurations and Consumer configurations .","title":"Kafka Producer and Consumer configurations"},{"location":"eventing/broker/kafka-broker/#enable-debug-logging-for-data-plane-components","text":"The following YAML shows the default logging configuration for data plane components, that is created during the installation step: apiVersion : v1 kind : ConfigMap metadata : name : kafka-config-logging namespace : knative-eventing data : config.xml : | <configuration> <appender name=\"jsonConsoleAppender\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder class=\"net.logstash.logback.encoder.LogstashEncoder\"/> </appender> <root level=\"INFO\"> <appender-ref ref=\"jsonConsoleAppender\"/> </root> </configuration> To change the logging level to DEBUG , you must: Apply the following kafka-config-logging ConfigMap or replace level=\"INFO\" with level=\"DEBUG\" to the ConfigMap kafka-config-logging : apiVersion : v1 kind : ConfigMap metadata : name : kafka-config-logging namespace : knative-eventing data : config.xml : | <configuration> <appender name=\"jsonConsoleAppender\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder class=\"net.logstash.logback.encoder.LogstashEncoder\"/> </appender> <root level=\"DEBUG\"> <appender-ref ref=\"jsonConsoleAppender\"/> </root> </configuration> Restart the kafka-broker-receiver and the kafka-broker-dispatcher , by entering the following commands: kubectl rollout restart deployment -n knative-eventing kafka-broker-receiver kubectl rollout restart deployment -n knative-eventing kafka-broker-dispatcher","title":"Enable debug logging for data plane components"},{"location":"eventing/broker/kafka-broker/#configuring-the-order-of-delivered-events","text":"When dispatching events, the Kafka broker can be configured to support different delivery ordering guarantees. You can configure the delivery order of events using the kafka.eventing.knative.dev/delivery.order annotation on the Trigger object: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger annotations : kafka.eventing.knative.dev/delivery.order : ordered spec : broker : my-kafka-broker subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : my-service The supported consumer delivery guarantees are: unordered : An unordered consumer is a non-blocking consumer that delivers messages unordered, while preserving proper offset management. ordered : An ordered consumer is a per-partition blocking consumer that waits for a successful response from the CloudEvent subscriber before it delivers the next message of the partition. unordered is the default ordering guarantee, while ordered is considered unstable, use with caution .","title":"Configuring the order of delivered events"},{"location":"eventing/broker/kafka-broker/#additional-information","text":"To report a bug or request a feature, open an issue in the eventing-kafka-broker repository .","title":"Additional information"},{"location":"eventing/broker/rabbitmq-broker/","text":"Creating a RabbitMQ Broker \u00b6 This topic describes how to create a RabbitMQ Broker. Prerequisites \u00b6 To use the RabbitMQ Broker, you must have the following installed: Knative Eventing RabbitMQ Cluster Operator - our recommendation is latest release CertManager v1.5.4 - easiest integration with RabbitMQ Messaging Topology Operator RabbitMQ Messaging Topology Operator - our recommendation is latest release with CertManager Install the RabbitMQ controller \u00b6 Install the RabbitMQ controller by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-rabbitmq/latest/rabbitmq-broker.yaml Verify that rabbitmq-broker-controller and rabbitmq-broker-webhook are running: kubectl get deployments.apps -n knative-eventing Example output: NAME READY UP-TO-DATE AVAILABLE AGE eventing-controller 1 /1 1 1 10s eventing-webhook 1 /1 1 1 9s rabbitmq-broker-controller 1 /1 1 1 3s rabbitmq-broker-webhook 1 /1 1 1 4s Create a RabbitMQ cluster \u00b6 Deploy a RabbitMQ cluster: Create a YAML file using the following template: apiVersion : rabbitmq.com/v1beta1 kind : RabbitmqCluster metadata : name : <cluster-name> annotations : # A single RabbitMQ cluster per Knative Eventing installation rabbitmq.com/topology-allowed-namespaces : \"*\" Where <cluster-name> is the name you want for your RabbitMQ cluster, for example, rabbitmq . Apply the YAML file by running the command: kubectl create -f <filename> Where <filename> is the name of the file you created in the previous step. Wait for the cluster to become ready. When the cluster is ready, ALLREPLICASREADY will be true in the output of the following command: kubectl get rmq <cluster-name> Where <cluster-name> is the name you gave your cluster in the step above. Example output: NAME ALLREPLICASREADY RECONCILESUCCESS AGE rabbitmq True True 38s For more information about configuring the RabbitmqCluster CRD, see the RabbitMQ website . Create a RabbitMQ Broker object \u00b6 Create a YAML file using the following template: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : eventing.knative.dev/broker.class : RabbitMQBroker name : <cluster-name> spec : config : apiVersion : rabbitmq.com/v1beta1 kind : RabbitmqCluster name : <cluster-name> Where <cluster-name> is the name you gave your RabbitMQ cluster in the step above. Apply the YAML file by running the command: kubectl apply -f <filename> Where <filename> is the name of the file you created in the previous step. Additional information \u00b6 To report a bug or request a feature, open an issue in the eventing-rabbitmq repository .","title":"RabbitMQ Broker"},{"location":"eventing/broker/rabbitmq-broker/#creating-a-rabbitmq-broker","text":"This topic describes how to create a RabbitMQ Broker.","title":"Creating a RabbitMQ Broker"},{"location":"eventing/broker/rabbitmq-broker/#prerequisites","text":"To use the RabbitMQ Broker, you must have the following installed: Knative Eventing RabbitMQ Cluster Operator - our recommendation is latest release CertManager v1.5.4 - easiest integration with RabbitMQ Messaging Topology Operator RabbitMQ Messaging Topology Operator - our recommendation is latest release with CertManager","title":"Prerequisites"},{"location":"eventing/broker/rabbitmq-broker/#install-the-rabbitmq-controller","text":"Install the RabbitMQ controller by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-rabbitmq/latest/rabbitmq-broker.yaml Verify that rabbitmq-broker-controller and rabbitmq-broker-webhook are running: kubectl get deployments.apps -n knative-eventing Example output: NAME READY UP-TO-DATE AVAILABLE AGE eventing-controller 1 /1 1 1 10s eventing-webhook 1 /1 1 1 9s rabbitmq-broker-controller 1 /1 1 1 3s rabbitmq-broker-webhook 1 /1 1 1 4s","title":"Install the RabbitMQ controller"},{"location":"eventing/broker/rabbitmq-broker/#create-a-rabbitmq-cluster","text":"Deploy a RabbitMQ cluster: Create a YAML file using the following template: apiVersion : rabbitmq.com/v1beta1 kind : RabbitmqCluster metadata : name : <cluster-name> annotations : # A single RabbitMQ cluster per Knative Eventing installation rabbitmq.com/topology-allowed-namespaces : \"*\" Where <cluster-name> is the name you want for your RabbitMQ cluster, for example, rabbitmq . Apply the YAML file by running the command: kubectl create -f <filename> Where <filename> is the name of the file you created in the previous step. Wait for the cluster to become ready. When the cluster is ready, ALLREPLICASREADY will be true in the output of the following command: kubectl get rmq <cluster-name> Where <cluster-name> is the name you gave your cluster in the step above. Example output: NAME ALLREPLICASREADY RECONCILESUCCESS AGE rabbitmq True True 38s For more information about configuring the RabbitmqCluster CRD, see the RabbitMQ website .","title":"Create a RabbitMQ cluster"},{"location":"eventing/broker/rabbitmq-broker/#create-a-rabbitmq-broker-object","text":"Create a YAML file using the following template: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : eventing.knative.dev/broker.class : RabbitMQBroker name : <cluster-name> spec : config : apiVersion : rabbitmq.com/v1beta1 kind : RabbitmqCluster name : <cluster-name> Where <cluster-name> is the name you gave your RabbitMQ cluster in the step above. Apply the YAML file by running the command: kubectl apply -f <filename> Where <filename> is the name of the file you created in the previous step.","title":"Create a RabbitMQ Broker object"},{"location":"eventing/broker/rabbitmq-broker/#additional-information","text":"To report a bug or request a feature, open an issue in the eventing-rabbitmq repository .","title":"Additional information"},{"location":"eventing/broker/triggers/","text":"Triggers \u00b6 A trigger represents a desire to subscribe to events from a specific broker. The subscriber value must be a Destination . Example Triggers \u00b6 The following trigger receives all the events from the default broker and delivers them to the Knative Serving service my-service : Create a YAML file using the following example: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger spec : broker : default subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : my-service Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. The following trigger receives all the events from the default broker and delivers them to the custom path /my-custom-path for the Kubernetes service my-service : Create a YAML file using the following example: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger spec : broker : default subscriber : ref : apiVersion : v1 kind : Service name : my-service uri : /my-custom-path Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Trigger filtering \u00b6 Exact match filtering on any number of CloudEvents attributes as well as extensions are supported. If your filter sets multiple attributes, an event must have all of the attributes for the trigger to filter it. Note that we only support exact matching on string values. Example \u00b6 This example filters events from the default broker that are of type dev.knative.foo.bar and have the extension myextension with the value my-extension-value . Create a YAML file using the following example: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger spec : broker : default filter : attributes : type : dev.knative.foo.bar myextension : my-extension-value subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : my-service Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Trigger annotations \u00b6 You can modify a Trigger's behavior by setting the following two annotations: eventing.knative.dev/injection : if set to enabled , Eventing automatically creates a Broker for a Trigger if it doesn't exist. The Broker is created in the namespace where the Trigger is created. This annotation only works if you have the Sugar Controller enabled, which is optional and not enabled by default. knative.dev/dependency : this annotation is used to mark the sources that the Trigger depends on. If one of the dependencies is not ready, the Trigger will not be ready. The following YAML is an example of a Trigger with a dependency: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger annotations : knative.dev/dependency : '{\"kind\":\"PingSource\",\"name\":\"test-ping-source\",\"apiVersion\":\"sources.knative.dev/v1\"}' spec : broker : default filter : attributes : type : dev.knative.foo.bar myextension : my-extension-value subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : my-service","title":"Triggers"},{"location":"eventing/broker/triggers/#triggers","text":"A trigger represents a desire to subscribe to events from a specific broker. The subscriber value must be a Destination .","title":"Triggers"},{"location":"eventing/broker/triggers/#example-triggers","text":"The following trigger receives all the events from the default broker and delivers them to the Knative Serving service my-service : Create a YAML file using the following example: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger spec : broker : default subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : my-service Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. The following trigger receives all the events from the default broker and delivers them to the custom path /my-custom-path for the Kubernetes service my-service : Create a YAML file using the following example: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger spec : broker : default subscriber : ref : apiVersion : v1 kind : Service name : my-service uri : /my-custom-path Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"Example Triggers"},{"location":"eventing/broker/triggers/#trigger-filtering","text":"Exact match filtering on any number of CloudEvents attributes as well as extensions are supported. If your filter sets multiple attributes, an event must have all of the attributes for the trigger to filter it. Note that we only support exact matching on string values.","title":"Trigger filtering"},{"location":"eventing/broker/triggers/#example","text":"This example filters events from the default broker that are of type dev.knative.foo.bar and have the extension myextension with the value my-extension-value . Create a YAML file using the following example: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger spec : broker : default filter : attributes : type : dev.knative.foo.bar myextension : my-extension-value subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : my-service Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"Example"},{"location":"eventing/broker/triggers/#trigger-annotations","text":"You can modify a Trigger's behavior by setting the following two annotations: eventing.knative.dev/injection : if set to enabled , Eventing automatically creates a Broker for a Trigger if it doesn't exist. The Broker is created in the namespace where the Trigger is created. This annotation only works if you have the Sugar Controller enabled, which is optional and not enabled by default. knative.dev/dependency : this annotation is used to mark the sources that the Trigger depends on. If one of the dependencies is not ready, the Trigger will not be ready. The following YAML is an example of a Trigger with a dependency: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger annotations : knative.dev/dependency : '{\"kind\":\"PingSource\",\"name\":\"test-ping-source\",\"apiVersion\":\"sources.knative.dev/v1\"}' spec : broker : default filter : attributes : type : dev.knative.foo.bar myextension : my-extension-value subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : my-service","title":"Trigger annotations"},{"location":"eventing/channels/","text":"Channels \u00b6 Channels are Kubernetes custom resources that define a single event forwarding and persistence layer. A channel provides an event delivery mechanism that can fan-out received events, through subscriptions, to multiple destinations, or sinks. Examples of sinks include brokers and Knative services. Next steps \u00b6 Learn about default available channel types Create a channel Create a subscription","title":"About Channels"},{"location":"eventing/channels/#channels","text":"Channels are Kubernetes custom resources that define a single event forwarding and persistence layer. A channel provides an event delivery mechanism that can fan-out received events, through subscriptions, to multiple destinations, or sinks. Examples of sinks include brokers and Knative services.","title":"Channels"},{"location":"eventing/channels/#next-steps","text":"Learn about default available channel types Create a channel Create a subscription","title":"Next steps"},{"location":"eventing/channels/channel-types-defaults/","text":"Channel types and defaults \u00b6 Knative uses two types of Channels: A generic Channel object. Channel implementations that each have their own custom resource definitions (CRDs), such as InMemoryChannel and KafkaChannel. The KafkaChannel supports an ordered consumer delivery guarantee, which is a per-partition blocking consumer that waits for a successful response from the CloudEvent subscriber before it delivers the next message of the partition. Custom Channel implementations each have their own event delivery mechanisms, such as in-memory or Broker-based. Examples of Brokers include KafkaBroker and the GCP Pub/Sub Broker. Knative provides the InMemoryChannel Channel implementation by default. This default implementation is useful for developers who do not want to configure a specific implementation type, such as Apache Kafka or NATSS Channels. You can use the generic Channel object if you want to create a Channel without specifying which Channel implementation CRD is used. This is useful if you do not care about the properties a particular Channel implementation provides, such as ordering and persistence, and you want to use the implementation selected by the cluster administrator. Cluster administrators can modify the default Channel implementation settings by editing the default-ch-webhook ConfigMap in the knative-eventing namespace. For more information about modifying ConfigMaps, see Configuring the Eventing Operator custom resource . Default Channels can be configured for the cluster, a namespace on the cluster, or both. Note If a default Channel implementation is configured for a namespace, this will overwrite the configuration for the cluster. In the following example, the cluster default Channel implementation is InMemoryChannel, while the namespace default Channel implementation for the example-namespace is KafkaChannel. apiVersion : v1 kind : ConfigMap metadata : name : default-ch-webhook namespace : knative-eventing data : default-ch-config : | clusterDefault: apiVersion: messaging.knative.dev/v1 kind: InMemoryChannel namespaceDefaults: example-namespace: apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 2 replicationFactor: 1 Note InMemoryChannel Channels must not be used in production environments. Next steps \u00b6 Create an InMemoryChannel","title":"Channel types and defaults"},{"location":"eventing/channels/channel-types-defaults/#channel-types-and-defaults","text":"Knative uses two types of Channels: A generic Channel object. Channel implementations that each have their own custom resource definitions (CRDs), such as InMemoryChannel and KafkaChannel. The KafkaChannel supports an ordered consumer delivery guarantee, which is a per-partition blocking consumer that waits for a successful response from the CloudEvent subscriber before it delivers the next message of the partition. Custom Channel implementations each have their own event delivery mechanisms, such as in-memory or Broker-based. Examples of Brokers include KafkaBroker and the GCP Pub/Sub Broker. Knative provides the InMemoryChannel Channel implementation by default. This default implementation is useful for developers who do not want to configure a specific implementation type, such as Apache Kafka or NATSS Channels. You can use the generic Channel object if you want to create a Channel without specifying which Channel implementation CRD is used. This is useful if you do not care about the properties a particular Channel implementation provides, such as ordering and persistence, and you want to use the implementation selected by the cluster administrator. Cluster administrators can modify the default Channel implementation settings by editing the default-ch-webhook ConfigMap in the knative-eventing namespace. For more information about modifying ConfigMaps, see Configuring the Eventing Operator custom resource . Default Channels can be configured for the cluster, a namespace on the cluster, or both. Note If a default Channel implementation is configured for a namespace, this will overwrite the configuration for the cluster. In the following example, the cluster default Channel implementation is InMemoryChannel, while the namespace default Channel implementation for the example-namespace is KafkaChannel. apiVersion : v1 kind : ConfigMap metadata : name : default-ch-webhook namespace : knative-eventing data : default-ch-config : | clusterDefault: apiVersion: messaging.knative.dev/v1 kind: InMemoryChannel namespaceDefaults: example-namespace: apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 2 replicationFactor: 1 Note InMemoryChannel Channels must not be used in production environments.","title":"Channel types and defaults"},{"location":"eventing/channels/channel-types-defaults/#next-steps","text":"Create an InMemoryChannel","title":"Next steps"},{"location":"eventing/channels/channels-crds/","text":"Available Channels \u00b6 This is a non-exhaustive list of the available Channels for Knative Eventing. Note Inclusion in this list is not an endorsement, nor does it imply any level of support. Name Status Support Description GCP PubSub Proof of Concept None Channels are backed by GCP PubSub . InMemoryChannel Proof of Concept None In-memory channels are a best effort Channel. They should NOT be used in Production. They are useful for development. KafkaChannel - Consolidated Proof of Concept None Channels are backed by Apache Kafka topics. The original Knative KafkaChannel implementation which utilizes a single combined Kafka Producer / Consumer deployment. KafkaChannel - Distributed Proof of Concept None Channels are backed by Apache Kafka topics. An alternate KafkaChannel implementation, contributed by SAP's Kyma project, which provides a more granular deployment of Producers / Consumers. NatssChannel Proof of Concept None Channels are backed by NATS Streaming .","title":"Available Channels"},{"location":"eventing/channels/channels-crds/#available-channels","text":"This is a non-exhaustive list of the available Channels for Knative Eventing. Note Inclusion in this list is not an endorsement, nor does it imply any level of support. Name Status Support Description GCP PubSub Proof of Concept None Channels are backed by GCP PubSub . InMemoryChannel Proof of Concept None In-memory channels are a best effort Channel. They should NOT be used in Production. They are useful for development. KafkaChannel - Consolidated Proof of Concept None Channels are backed by Apache Kafka topics. The original Knative KafkaChannel implementation which utilizes a single combined Kafka Producer / Consumer deployment. KafkaChannel - Distributed Proof of Concept None Channels are backed by Apache Kafka topics. An alternate KafkaChannel implementation, contributed by SAP's Kyma project, which provides a more granular deployment of Producers / Consumers. NatssChannel Proof of Concept None Channels are backed by NATS Streaming .","title":"Available Channels"},{"location":"eventing/channels/create-default-channel/","text":"Creating a Channel using cluster or namespace defaults \u00b6 Developers can create Channels of any supported implementation type by creating an instance of a Channel object. To create a Channel: Create a YAML file for the Channel object using the following template: apiVersion : messaging.knative.dev/v1 kind : Channel metadata : name : <example-channel> namespace : <namespace> Where: <example-channel> is the name of the Channel you want to create. <namespace> is the name of your target namespace. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. If you create this object in the default namespace, according to the default ConfigMap example in Channel types and defaults , it is an InMemoryChannel Channel implementation. After the Channel object is created, a mutating admission webhook sets the spec.channelTemplate based on the default Channel implementation: apiVersion : messaging.knative.dev/v1 kind : Channel metadata : name : <example-channel> namespace : <namespace> spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : <channel-template-kind> Where: <example-channel> is the name of the Channel you want to create. <namespace> is the name of your target namespace. <channel-template-kind> is the kind of Channel, such as InMemoryChannel or KafkaChannel, based on the default ConfigMap. See an example in Channel types and defaults . Note The spec.channelTemplate property cannot be changed after creation, because it is set by the default Channel mechanism, not the user. The Channel controller creates a backing Channel instance based on the spec.channelTemplate . When this mechanism is used two objects are created; a generic Channel object and an InMemoryChannel object. The generic object acts as a proxy for the InMemoryChannel object by copying its Subscriptions to, and setting its status to, that of the InMemoryChannel object. Note Defaults are only applied by the webhook when a Channel or Sequence is initially created. If the default settings are changed, the new defaults will only be applied to newly created Channels, Brokers, or Sequences. Existing resources are not updated automatically to use the new configuration.","title":"Creating a Channel using cluster or namespace defaults"},{"location":"eventing/channels/create-default-channel/#creating-a-channel-using-cluster-or-namespace-defaults","text":"Developers can create Channels of any supported implementation type by creating an instance of a Channel object. To create a Channel: Create a YAML file for the Channel object using the following template: apiVersion : messaging.knative.dev/v1 kind : Channel metadata : name : <example-channel> namespace : <namespace> Where: <example-channel> is the name of the Channel you want to create. <namespace> is the name of your target namespace. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. If you create this object in the default namespace, according to the default ConfigMap example in Channel types and defaults , it is an InMemoryChannel Channel implementation. After the Channel object is created, a mutating admission webhook sets the spec.channelTemplate based on the default Channel implementation: apiVersion : messaging.knative.dev/v1 kind : Channel metadata : name : <example-channel> namespace : <namespace> spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : <channel-template-kind> Where: <example-channel> is the name of the Channel you want to create. <namespace> is the name of your target namespace. <channel-template-kind> is the kind of Channel, such as InMemoryChannel or KafkaChannel, based on the default ConfigMap. See an example in Channel types and defaults . Note The spec.channelTemplate property cannot be changed after creation, because it is set by the default Channel mechanism, not the user. The Channel controller creates a backing Channel instance based on the spec.channelTemplate . When this mechanism is used two objects are created; a generic Channel object and an InMemoryChannel object. The generic object acts as a proxy for the InMemoryChannel object by copying its Subscriptions to, and setting its status to, that of the InMemoryChannel object. Note Defaults are only applied by the webhook when a Channel or Sequence is initially created. If the default settings are changed, the new defaults will only be applied to newly created Channels, Brokers, or Sequences. Existing resources are not updated automatically to use the new configuration.","title":"Creating a Channel using cluster or namespace defaults"},{"location":"eventing/channels/subscriptions/","text":"Subscriptions \u00b6 After you have created a Channel and a Sink, you can create a Subscription to enable event delivery. The Subscription consists of a Subscription object, which specifies the Channel and the Sink (also known as the Subscriber) to deliver events to. You can also specify some Sink-specific options, such as how to handle failures. For more information about Subscription objects, see Subscription . Creating a Subscription \u00b6 kn YAML Create a Subscription between a Channel and a Sink by running: kn subscription create <subscription-name> \\ --channel <Group:Version:Kind>:<channel-name> \\ --sink <sink-prefix>:<sink-name> \\ --sink-reply <sink-prefix>:<sink-name> \\ --sink-dead-letter <sink-prefix>:<sink-name> --channel specifies the source for cloud events that should be processed. You must provide the Channel name. If you are not using the default Channel that is backed by the Channel resource, you must prefix the Channel name with the <Group:Version:Kind> for the specified Channel type. For example, this is messaging.knative.dev:v1beta1:KafkaChannel for a Kafka-backed Channel. --sink specifies the target destination to which the event should be delivered. By default, the <sink-name> is interpreted as a Knative service of this name, in the same namespace as the Subscription. You can specify the type of the Sink by using one of the following prefixes: ksvc : A Knative service. svc : A Kubernetes Service. channel : A Channel that should be used as the destination. You can only reference default Channel types here. broker : An Eventing Broker. --sink-reply is an optional argument you can use to specify where the Sink reply is sent. It uses the same naming conventions for specifying the Sink as the --sink flag. --sink-dead-letter is an optional argument you can use to specify where to send the CloudEvent in case of a failure. It uses the same naming conventions for specifying the Sink as the --sink flag. ksvc : A Knative service. svc : A Kubernetes Service. channel : A Channel that should be used as destination. Only default Channel types can be referenced here. broker : An Eventing Broker. --sink-reply and --sink-dead-letter are optional arguments. They can be used to specify where the Sink reply is sent, and where to send the CloudEvent in case of a failure, respectively. Both use the same naming conventions for specifying the Sink as the --sink flag. This example command creates a Channel named mysubscription that routes events from a Channel named mychannel to a Knative service named myservice . Note The Sink prefix is optional. You can also specify the service for --sink as just --sink <service-name> and omit the ksvc prefix. Create a YAML file for the Subscription object using the following example: apiVersion : messaging.knative.dev/v1 kind : Subscription metadata : name : <subscription-name> # Name of the Subscription. namespace : default spec : channel : apiVersion : messaging.knative.dev/v1 kind : Channel name : <channel-name> # Configuration settings for the Channel that the Subscription connects to. delivery : deadLetterSink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : <service-name> # Configuration settings for event delivery. # This tells the Subscription what happens to events that cannot be delivered to the Subscriber. # When this is configured, events that failed to be consumed are sent to the deadLetterSink. # The event is dropped, no re-delivery of the event is attempted, and an error is logged in the system. # The deadLetterSink value must be a Destination. subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : <service-name> # Configuration settings for the Subscriber. This is the event Sink that events are delivered to from the Channel. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Listing Subscriptions \u00b6 You can list all existing Subscriptions by using the kn CLI tool. List all Subscriptions: kn subscription list List Subscriptions in YAML format: kn subscription list -o yaml Describing a Subscription \u00b6 You can print details about a Subscription by using the kn CLI tool: kn subscription describe <subscription-name> Deleting Subscriptions \u00b6 You can delete a Subscription by using the kn or kubectl CLI tools. kn kubectl kn subscription delete <subscription-name> kubectl subscription delete <subscription-name> Next steps \u00b6 Creating a Channel using cluster or namespace defaults","title":"Subscriptions"},{"location":"eventing/channels/subscriptions/#subscriptions","text":"After you have created a Channel and a Sink, you can create a Subscription to enable event delivery. The Subscription consists of a Subscription object, which specifies the Channel and the Sink (also known as the Subscriber) to deliver events to. You can also specify some Sink-specific options, such as how to handle failures. For more information about Subscription objects, see Subscription .","title":"Subscriptions"},{"location":"eventing/channels/subscriptions/#creating-a-subscription","text":"kn YAML Create a Subscription between a Channel and a Sink by running: kn subscription create <subscription-name> \\ --channel <Group:Version:Kind>:<channel-name> \\ --sink <sink-prefix>:<sink-name> \\ --sink-reply <sink-prefix>:<sink-name> \\ --sink-dead-letter <sink-prefix>:<sink-name> --channel specifies the source for cloud events that should be processed. You must provide the Channel name. If you are not using the default Channel that is backed by the Channel resource, you must prefix the Channel name with the <Group:Version:Kind> for the specified Channel type. For example, this is messaging.knative.dev:v1beta1:KafkaChannel for a Kafka-backed Channel. --sink specifies the target destination to which the event should be delivered. By default, the <sink-name> is interpreted as a Knative service of this name, in the same namespace as the Subscription. You can specify the type of the Sink by using one of the following prefixes: ksvc : A Knative service. svc : A Kubernetes Service. channel : A Channel that should be used as the destination. You can only reference default Channel types here. broker : An Eventing Broker. --sink-reply is an optional argument you can use to specify where the Sink reply is sent. It uses the same naming conventions for specifying the Sink as the --sink flag. --sink-dead-letter is an optional argument you can use to specify where to send the CloudEvent in case of a failure. It uses the same naming conventions for specifying the Sink as the --sink flag. ksvc : A Knative service. svc : A Kubernetes Service. channel : A Channel that should be used as destination. Only default Channel types can be referenced here. broker : An Eventing Broker. --sink-reply and --sink-dead-letter are optional arguments. They can be used to specify where the Sink reply is sent, and where to send the CloudEvent in case of a failure, respectively. Both use the same naming conventions for specifying the Sink as the --sink flag. This example command creates a Channel named mysubscription that routes events from a Channel named mychannel to a Knative service named myservice . Note The Sink prefix is optional. You can also specify the service for --sink as just --sink <service-name> and omit the ksvc prefix. Create a YAML file for the Subscription object using the following example: apiVersion : messaging.knative.dev/v1 kind : Subscription metadata : name : <subscription-name> # Name of the Subscription. namespace : default spec : channel : apiVersion : messaging.knative.dev/v1 kind : Channel name : <channel-name> # Configuration settings for the Channel that the Subscription connects to. delivery : deadLetterSink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : <service-name> # Configuration settings for event delivery. # This tells the Subscription what happens to events that cannot be delivered to the Subscriber. # When this is configured, events that failed to be consumed are sent to the deadLetterSink. # The event is dropped, no re-delivery of the event is attempted, and an error is logged in the system. # The deadLetterSink value must be a Destination. subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : <service-name> # Configuration settings for the Subscriber. This is the event Sink that events are delivered to from the Channel. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"Creating a Subscription"},{"location":"eventing/channels/subscriptions/#listing-subscriptions","text":"You can list all existing Subscriptions by using the kn CLI tool. List all Subscriptions: kn subscription list List Subscriptions in YAML format: kn subscription list -o yaml","title":"Listing Subscriptions"},{"location":"eventing/channels/subscriptions/#describing-a-subscription","text":"You can print details about a Subscription by using the kn CLI tool: kn subscription describe <subscription-name>","title":"Describing a Subscription"},{"location":"eventing/channels/subscriptions/#deleting-subscriptions","text":"You can delete a Subscription by using the kn or kubectl CLI tools. kn kubectl kn subscription delete <subscription-name> kubectl subscription delete <subscription-name>","title":"Deleting Subscriptions"},{"location":"eventing/channels/subscriptions/#next-steps","text":"Creating a Channel using cluster or namespace defaults","title":"Next steps"},{"location":"eventing/configuration/broker-configuration/","text":"Configure Broker defaults \u00b6 Knative Eventing provides a config-br-defaults ConfigMap that contains the configuration settings that govern default Broker creation. The default config-br-defaults ConfigMap is as follows: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configures the default for any Broker that does not specify a spec.config or Broker class. default-br-config : | clusterDefault: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: config-br-default-channel namespace: knative-eventing Channel implementation options \u00b6 The following example shows a Broker object where the spec.config configuration is specified in a config-br-default-channel ConfigMap: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : eventing.knative.dev/broker.class : MTChannelBasedBroker name : default spec : # Configuration specific to this broker. config : apiVersion : v1 kind : ConfigMap name : config-br-default-channel namespace : knative-eventing A Broker object that does not have a spec.config specified uses the config-br-default-channel ConfigMap dy default because this is specified in the config-br-defaults ConfigMap. However, if you have installed a different Channel implementation, for example, Kafka, and would like this to be used as the default Channel implementation for any Broker that is created, you can change the config-br-defaults ConfigMap to look as follows: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configures the default for any Broker that does not specify a spec.config or Broker class. default-br-config : | clusterDefault: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: kafka-channel namespace: knative-eventing Now every Broker created in the cluster that does not have a spec.config will be configured to use the kafka-channel ConfigMap. For more information about creating a kafka-channel ConfigMap to use with your Broker, see the Kafka Channel ConfigMap documentation. Changing the default Channel implementation for a namespace \u00b6 You can modify the default Broker creation behavior for one or more namespaces. For example, if you wanted to use the kafka-channel ConfigMap for all other Brokers created, but wanted to use config-br-default-channel ConfigMap for namespace-1 and namespace-2 , you would use the following ConfigMap settings: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configures the default for any Broker that does not specify a spec.config or Broker class. default-br-config : | clusterDefault: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: config-kafka-channel namespace: knative-eventing namespaceDefaults: namespace-1: apiVersion: v1 kind: ConfigMap name: config-br-default-channel namespace: knative-eventing namespace-2: apiVersion: v1 kind: ConfigMap name: config-br-default-channel namespace: knative-eventing Broker class options \u00b6 When a Broker is created without a specified BrokerClass annotation, the default MTChannelBasedBroker Broker class is used, as specified in the config-br-defaults ConfigMap. The following example creates a Broker called default in the default namespace, and uses MTChannelBasedBroker as the implementation: Create a YAML file for your Broker using the following example: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : default namespace : default Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Configuring the Broker class \u00b6 To configure a Broker class, you can modify the eventing.knative.dev/broker.class annotation and spec.config for the Broker object. MTChannelBasedBroker is the Broker class default. Modify the eventing.knative.dev/broker.class annotation. Replace MTChannelBasedBroker with the class type you want to use: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : eventing.knative.dev/broker.class : MTChannelBasedBroker name : default namespace : default Configure the spec.config with the details of the ConfigMap that defines the backing Channel for the Broker class: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : eventing.knative.dev/broker.class : MTChannelBasedBroker name : default namespace : default spec : config : apiVersion : v1 kind : ConfigMap name : config-br-default-channel namespace : knative-eventing Configuring the default BrokerClass for the cluster \u00b6 You can configure the clusterDefault Broker class so that any Broker created in the cluster that does not have a BrokerClass annotation uses this default class. Example \u00b6 apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configures the default for any Broker that does not specify a spec.config or Broker class. default-br-config : | clusterDefault: brokerClass: MTChannelBasedBroker Configuring the default BrokerClass for namespaces \u00b6 You can modify the default Broker class for one or more namespaces. For example, if you want to use a KafkaBroker class for all other Brokers created on the cluster, but you want to use the MTChannelBasedBroker class for Brokers created in namespace-1 and namespace-2 , you would use the following ConfigMap settings: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configures the default for any Broker that does not specify a spec.config or Broker class. default-br-config : | clusterDefault: brokerClass: KafkaBroker namespaceDefaults: namespace1: brokerClass: MTChannelBasedBroker namespace2: brokerClass: MTChannelBasedBroker","title":"Configure Broker defaults"},{"location":"eventing/configuration/broker-configuration/#configure-broker-defaults","text":"Knative Eventing provides a config-br-defaults ConfigMap that contains the configuration settings that govern default Broker creation. The default config-br-defaults ConfigMap is as follows: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configures the default for any Broker that does not specify a spec.config or Broker class. default-br-config : | clusterDefault: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: config-br-default-channel namespace: knative-eventing","title":"Configure Broker defaults"},{"location":"eventing/configuration/broker-configuration/#channel-implementation-options","text":"The following example shows a Broker object where the spec.config configuration is specified in a config-br-default-channel ConfigMap: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : eventing.knative.dev/broker.class : MTChannelBasedBroker name : default spec : # Configuration specific to this broker. config : apiVersion : v1 kind : ConfigMap name : config-br-default-channel namespace : knative-eventing A Broker object that does not have a spec.config specified uses the config-br-default-channel ConfigMap dy default because this is specified in the config-br-defaults ConfigMap. However, if you have installed a different Channel implementation, for example, Kafka, and would like this to be used as the default Channel implementation for any Broker that is created, you can change the config-br-defaults ConfigMap to look as follows: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configures the default for any Broker that does not specify a spec.config or Broker class. default-br-config : | clusterDefault: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: kafka-channel namespace: knative-eventing Now every Broker created in the cluster that does not have a spec.config will be configured to use the kafka-channel ConfigMap. For more information about creating a kafka-channel ConfigMap to use with your Broker, see the Kafka Channel ConfigMap documentation.","title":"Channel implementation options"},{"location":"eventing/configuration/broker-configuration/#changing-the-default-channel-implementation-for-a-namespace","text":"You can modify the default Broker creation behavior for one or more namespaces. For example, if you wanted to use the kafka-channel ConfigMap for all other Brokers created, but wanted to use config-br-default-channel ConfigMap for namespace-1 and namespace-2 , you would use the following ConfigMap settings: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configures the default for any Broker that does not specify a spec.config or Broker class. default-br-config : | clusterDefault: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: config-kafka-channel namespace: knative-eventing namespaceDefaults: namespace-1: apiVersion: v1 kind: ConfigMap name: config-br-default-channel namespace: knative-eventing namespace-2: apiVersion: v1 kind: ConfigMap name: config-br-default-channel namespace: knative-eventing","title":"Changing the default Channel implementation for a namespace"},{"location":"eventing/configuration/broker-configuration/#broker-class-options","text":"When a Broker is created without a specified BrokerClass annotation, the default MTChannelBasedBroker Broker class is used, as specified in the config-br-defaults ConfigMap. The following example creates a Broker called default in the default namespace, and uses MTChannelBasedBroker as the implementation: Create a YAML file for your Broker using the following example: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : default namespace : default Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"Broker class options"},{"location":"eventing/configuration/broker-configuration/#configuring-the-broker-class","text":"To configure a Broker class, you can modify the eventing.knative.dev/broker.class annotation and spec.config for the Broker object. MTChannelBasedBroker is the Broker class default. Modify the eventing.knative.dev/broker.class annotation. Replace MTChannelBasedBroker with the class type you want to use: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : eventing.knative.dev/broker.class : MTChannelBasedBroker name : default namespace : default Configure the spec.config with the details of the ConfigMap that defines the backing Channel for the Broker class: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : eventing.knative.dev/broker.class : MTChannelBasedBroker name : default namespace : default spec : config : apiVersion : v1 kind : ConfigMap name : config-br-default-channel namespace : knative-eventing","title":"Configuring the Broker class"},{"location":"eventing/configuration/broker-configuration/#configuring-the-default-brokerclass-for-the-cluster","text":"You can configure the clusterDefault Broker class so that any Broker created in the cluster that does not have a BrokerClass annotation uses this default class.","title":"Configuring the default BrokerClass for the cluster"},{"location":"eventing/configuration/broker-configuration/#example","text":"apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configures the default for any Broker that does not specify a spec.config or Broker class. default-br-config : | clusterDefault: brokerClass: MTChannelBasedBroker","title":"Example"},{"location":"eventing/configuration/broker-configuration/#configuring-the-default-brokerclass-for-namespaces","text":"You can modify the default Broker class for one or more namespaces. For example, if you want to use a KafkaBroker class for all other Brokers created on the cluster, but you want to use the MTChannelBasedBroker class for Brokers created in namespace-1 and namespace-2 , you would use the following ConfigMap settings: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing labels : eventing.knative.dev/release : devel data : # Configures the default for any Broker that does not specify a spec.config or Broker class. default-br-config : | clusterDefault: brokerClass: KafkaBroker namespaceDefaults: namespace1: brokerClass: MTChannelBasedBroker namespace2: brokerClass: MTChannelBasedBroker","title":"Configuring the default BrokerClass for namespaces"},{"location":"eventing/configuration/channel-configuration/","text":"Configure Channel defaults \u00b6 Knative Eventing provides a default-ch-webhook ConfigMap that contains the configuration settings that govern default Channel creation. The default default-ch-webhook ConfigMap is as follows: apiVersion : v1 kind : ConfigMap metadata : name : default-ch-webhook namespace : knative-eventing labels : eventing.knative.dev/release : devel app.kubernetes.io/version : devel app.kubernetes.io/part-of : knative-eventing data : default-ch-config : | clusterDefault: apiVersion: messaging.knative.dev/v1 kind: InMemoryChannel namespaceDefaults: some-namespace: apiVersion: messaging.knative.dev/v1 kind: InMemoryChannel By changing the data.default-ch-config property we can define the clusterDefaults and per Namespace defaults. This configuration is used by the Channel custom resource definition (CRD) to create platform specific implementations. Note The clusterDefault setting determines the global, cluster-wide default Channel type. You can configure Channel defaults for individual namespaces by using the namespaceDefaults setting.","title":"Configure Channel defaults"},{"location":"eventing/configuration/channel-configuration/#configure-channel-defaults","text":"Knative Eventing provides a default-ch-webhook ConfigMap that contains the configuration settings that govern default Channel creation. The default default-ch-webhook ConfigMap is as follows: apiVersion : v1 kind : ConfigMap metadata : name : default-ch-webhook namespace : knative-eventing labels : eventing.knative.dev/release : devel app.kubernetes.io/version : devel app.kubernetes.io/part-of : knative-eventing data : default-ch-config : | clusterDefault: apiVersion: messaging.knative.dev/v1 kind: InMemoryChannel namespaceDefaults: some-namespace: apiVersion: messaging.knative.dev/v1 kind: InMemoryChannel By changing the data.default-ch-config property we can define the clusterDefaults and per Namespace defaults. This configuration is used by the Channel custom resource definition (CRD) to create platform specific implementations. Note The clusterDefault setting determines the global, cluster-wide default Channel type. You can configure Channel defaults for individual namespaces by using the namespaceDefaults setting.","title":"Configure Channel defaults"},{"location":"eventing/configuration/kafka-channel-configuration/","text":"Configure Kafka Channels \u00b6 Note This guide assumes Knative Eventing is installed in the knative-eventing namespace. If you have installed Knative Eventing in a different namespace, replace knative-eventing with the name of that namespace. To use Kafka Channels, you must: Install the KafkaChannel custom resource definition (CRD). Create a ConfigMap that specifies default configurations for how KafkaChannel instances are created. Create a kafka-channel ConfigMap \u00b6 Create a YAML file for the kafka-channel ConfigMap using the following template: apiVersion : v1 kind : ConfigMap metadata : name : kafka-channel namespace : knative-eventing data : channel-template-spec : | apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 3 replicationFactor: 1 Note This example specifies two extra parameters that are specific to Kafka Channels; numPartitions and replicationFactor . Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Optional. To create a Broker that uses Kafka Channels, specify the kafka-channel ConfigMap in the Broker spec. You can do this by creating a YAML file using the following template: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : eventing.knative.dev/broker.class : MTChannelBasedBroker name : kafka-backed-broker namespace : default spec : config : apiVersion : v1 kind : ConfigMap name : kafka-channel namespace : knative-eventing Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"Configure Kafka Channel defaults"},{"location":"eventing/configuration/kafka-channel-configuration/#configure-kafka-channels","text":"Note This guide assumes Knative Eventing is installed in the knative-eventing namespace. If you have installed Knative Eventing in a different namespace, replace knative-eventing with the name of that namespace. To use Kafka Channels, you must: Install the KafkaChannel custom resource definition (CRD). Create a ConfigMap that specifies default configurations for how KafkaChannel instances are created.","title":"Configure Kafka Channels"},{"location":"eventing/configuration/kafka-channel-configuration/#create-a-kafka-channel-configmap","text":"Create a YAML file for the kafka-channel ConfigMap using the following template: apiVersion : v1 kind : ConfigMap metadata : name : kafka-channel namespace : knative-eventing data : channel-template-spec : | apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 3 replicationFactor: 1 Note This example specifies two extra parameters that are specific to Kafka Channels; numPartitions and replicationFactor . Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Optional. To create a Broker that uses Kafka Channels, specify the kafka-channel ConfigMap in the Broker spec. You can do this by creating a YAML file using the following template: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : annotations : eventing.knative.dev/broker.class : MTChannelBasedBroker name : kafka-backed-broker namespace : default spec : config : apiVersion : v1 kind : ConfigMap name : kafka-channel namespace : knative-eventing Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"Create a kafka-channel ConfigMap"},{"location":"eventing/configuration/sources-configuration/","text":"Configure event source defaults \u00b6 This topic describes how to configure defaults for Knative event sources. You can configure event sources depending on how they generate events. Configure defaults for PingSource \u00b6 PingSource is an event source that produces events with a fixed payload on a specified cron schedule. For how to create a new PingSource, see Creating a PingSource object . For the available parameters, see PingSource reference . In addition to the parameters that you can configure in the PingSource resource, there is a global ConfigMap called config-ping-defaults . This ConfigMap allows you to change the maximum amount of data that the PingSource adds to the CloudEvents it produces. The data-max-size parameter allows you to set the maximum number of bytes allowed to be sent for a message excluding any base64 decoding. The default value, -1 , sets no limit for data. apiVersion: v1 kind: ConfigMap metadata: name: config-ping-defaults namespace: knative-eventing data: data-max-size: -1 You can edit this ConfigMap by running the command: kubectl edit cm config-ping-defaults -n knative-eventing","title":"Configure event source defaults"},{"location":"eventing/configuration/sources-configuration/#configure-event-source-defaults","text":"This topic describes how to configure defaults for Knative event sources. You can configure event sources depending on how they generate events.","title":"Configure event source defaults"},{"location":"eventing/configuration/sources-configuration/#configure-defaults-for-pingsource","text":"PingSource is an event source that produces events with a fixed payload on a specified cron schedule. For how to create a new PingSource, see Creating a PingSource object . For the available parameters, see PingSource reference . In addition to the parameters that you can configure in the PingSource resource, there is a global ConfigMap called config-ping-defaults . This ConfigMap allows you to change the maximum amount of data that the PingSource adds to the CloudEvents it produces. The data-max-size parameter allows you to set the maximum number of bytes allowed to be sent for a message excluding any base64 decoding. The default value, -1 , sets no limit for data. apiVersion: v1 kind: ConfigMap metadata: name: config-ping-defaults namespace: knative-eventing data: data-max-size: -1 You can edit this ConfigMap by running the command: kubectl edit cm config-ping-defaults -n knative-eventing","title":"Configure defaults for PingSource"},{"location":"eventing/custom-event-source/","text":"Custom event sources \u00b6 If you need to ingress events from an event producer that is not included in Knative, or from a producer that emits events which are not in the CloudEvent format that is used by Knative, you can do this by using one of the following methods: Create a custom Knative event source . Use a PodSpecable object as an event source, by creating a SinkBinding . Use a container as an event source, by creating a ContainerSource .","title":"Custom event sources overview"},{"location":"eventing/custom-event-source/#custom-event-sources","text":"If you need to ingress events from an event producer that is not included in Knative, or from a producer that emits events which are not in the CloudEvent format that is used by Knative, you can do this by using one of the following methods: Create a custom Knative event source . Use a PodSpecable object as an event source, by creating a SinkBinding . Use a container as an event source, by creating a ContainerSource .","title":"Custom event sources"},{"location":"eventing/custom-event-source/containersource/","text":"Create a ContainerSource \u00b6 The ContainerSource object starts a container image that generates events and sends messages to a sink URI. You can also use ContainerSource to support your own event sources in Knative. To create a custom event source using ContainerSource, you must create a container image, and a ContainerSource that uses your image URI. Before you begin \u00b6 Before you can create a ContainerSource object, you must have Knative Eventing installed on your cluster. Develop, build and publish a container image \u00b6 You can develop a container image by using any language, and can build and publish your image by using any tools you like. The following are some basic guidelines: Two environments variables are injected by the ContainerSource controller; K_SINK and K_CE_OVERRIDES , resolved from spec.sink and spec.ceOverrides respectively. The event messages are sent to the sink URI specified in K_SINK . The message must be sent as a POST in CloudEvents HTTP format . Create a ContainerSource object \u00b6 Build an image of your event source and publish it to your image repository. Your image must read the environment variable K_SINK and post messages to the URL specified in K_SINK . You can use the following YAML to deploy a demo heartbeats event source: apiVersion : sources.knative.dev/v1 kind : ContainerSource metadata : name : heartbeat-source spec : template : spec : containers : - image : gcr.io/knative-nightly/knative.dev/eventing/cmd/heartbeats:latest name : heartbeats sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Create a namespace for your ContainerSource by running the command: kubectl create namespace <namespace> Where <namespace> is the namespace that you want your ContainerSource to use. For example, heartbeat-source . Create a sink. If you do not already have a sink, you can use the following Knative Service, which dumps incoming messages into its log: Note To create a Knative service you must have Knative Serving installed on your cluster. kn YAML To create a sink, run the command: kn service create event-display --port 8080 --image gcr.io/knative-releases/knative.dev/eventing/cmd/event_display Create a YAML file using the following example: apiVersion : apps/v1 kind : Deployment metadata : name : event-display spec : replicas : 1 selector : matchLabels : &labels app : event-display template : metadata : labels : *labels spec : containers : - name : event-display image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- kind : Service apiVersion : v1 metadata : name : event-display spec : selector : app : event-display ports : - protocol : TCP port : 80 targetPort : 8080 Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Create a concrete ContainerSource with specific arguments and environment settings: kn YAML To create the ContainerSource, run the command: kn source container create <name> --image <image-uri> --sink <sink> -e POD_NAME = <pod-name> -e POD_NAMESPACE = <pod-namespace> Where: <name> is the name you want for your ContainerSource object, for example, test-heartbeats . <image-uri> corresponds to the image URI you built and published in step 1, for example, gcr.io/knative-nightly/knative.dev/eventing/cmd/heartbeats . <pod-name> is the name of the Pod that the container runs in, for example, mypod . <pod-namespace> is the namespace that the Pod runs in, for example, event-test . <sink> is the name of your sink, for example, event-display . For a list of available options, see the Knative client documentation . Create a YAML file using the following template: apiVersion : sources.knative.dev/v1 kind : ContainerSource metadata : name : <containersource-name> spec : template : spec : containers : - image : <event-source-image-uri> name : <container-name> env : - name : POD_NAME value : \"<pod-name>\" - name : POD_NAMESPACE value : \"<pod-namespace>\" sink : ref : apiVersion : v1 kind : Service name : <sink> Where: <namespace> is the namespace you created for your ContainerSource, for example, containersource-example . <containersource-name> is the name you want for your ContainerSource, for example, test-heartbeats . <event-source-image-uri> corresponds to the image URI you built and published in step 1, for example, gcr.io/knative-nightly/knative.dev/eventing/cmd/heartbeats . <container-name> is the name of your event source, for example, heartbeats . <pod-name> is the name of the Pod that the container runs in, for example, mypod . <pod-namespace> is the namespace that the Pod runs in, for example, event-test . <sink> is the name of your sink, for example, event-display . For more information about the fields you can configure for the ContainerSource object, see ContainerSource Reference . Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Note Arguments and environment variables are set and are passed to the container. Verify the ContainerSource object \u00b6 View the logs for your event consumer by running the command: kubectl -n <namespace> logs -l <pod-name> --tail = 200 Where: <namespace> is the namespace that contains the ContainerSource object. <pod-name> is the name of the Pod that the container runs in. For example: $ kubectl -n containersource-example logs -l app = event-display --tail = 200 Verify that the output returns the properties of the events that your ContainerSource sent to your sink. In the following example, the command has returned the Attributes and Data properties of the events that the ContainerSource sent to the event-display Service: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.eventing.samples.heartbeat source: https://knative.dev/eventing/cmd/heartbeats/#event-test/mypod id: 2b72d7bf-c38f-4a98-a433-608fbcdd2596 time: 2019-10-18T15:23:20.809775386Z contenttype: application/json Extensions, beats: true heart: yes the: 42 Data, { \"id\": 2, \"label\": \"\" } Delete the ContainerSource object \u00b6 To delete the ContainerSource object and all of the related resources in the namespace: Delete the namespace by running the command: kubectl delete namespace <namespace> Where <namespace> is the namespace that contains the ContainerSource object. Reference Documentation \u00b6 See the ContainerSource reference .","title":"Create a ContainerSource"},{"location":"eventing/custom-event-source/containersource/#create-a-containersource","text":"The ContainerSource object starts a container image that generates events and sends messages to a sink URI. You can also use ContainerSource to support your own event sources in Knative. To create a custom event source using ContainerSource, you must create a container image, and a ContainerSource that uses your image URI.","title":"Create a ContainerSource"},{"location":"eventing/custom-event-source/containersource/#before-you-begin","text":"Before you can create a ContainerSource object, you must have Knative Eventing installed on your cluster.","title":"Before you begin"},{"location":"eventing/custom-event-source/containersource/#develop-build-and-publish-a-container-image","text":"You can develop a container image by using any language, and can build and publish your image by using any tools you like. The following are some basic guidelines: Two environments variables are injected by the ContainerSource controller; K_SINK and K_CE_OVERRIDES , resolved from spec.sink and spec.ceOverrides respectively. The event messages are sent to the sink URI specified in K_SINK . The message must be sent as a POST in CloudEvents HTTP format .","title":"Develop, build and publish a container image"},{"location":"eventing/custom-event-source/containersource/#create-a-containersource-object","text":"Build an image of your event source and publish it to your image repository. Your image must read the environment variable K_SINK and post messages to the URL specified in K_SINK . You can use the following YAML to deploy a demo heartbeats event source: apiVersion : sources.knative.dev/v1 kind : ContainerSource metadata : name : heartbeat-source spec : template : spec : containers : - image : gcr.io/knative-nightly/knative.dev/eventing/cmd/heartbeats:latest name : heartbeats sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Create a namespace for your ContainerSource by running the command: kubectl create namespace <namespace> Where <namespace> is the namespace that you want your ContainerSource to use. For example, heartbeat-source . Create a sink. If you do not already have a sink, you can use the following Knative Service, which dumps incoming messages into its log: Note To create a Knative service you must have Knative Serving installed on your cluster. kn YAML To create a sink, run the command: kn service create event-display --port 8080 --image gcr.io/knative-releases/knative.dev/eventing/cmd/event_display Create a YAML file using the following example: apiVersion : apps/v1 kind : Deployment metadata : name : event-display spec : replicas : 1 selector : matchLabels : &labels app : event-display template : metadata : labels : *labels spec : containers : - name : event-display image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- kind : Service apiVersion : v1 metadata : name : event-display spec : selector : app : event-display ports : - protocol : TCP port : 80 targetPort : 8080 Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Create a concrete ContainerSource with specific arguments and environment settings: kn YAML To create the ContainerSource, run the command: kn source container create <name> --image <image-uri> --sink <sink> -e POD_NAME = <pod-name> -e POD_NAMESPACE = <pod-namespace> Where: <name> is the name you want for your ContainerSource object, for example, test-heartbeats . <image-uri> corresponds to the image URI you built and published in step 1, for example, gcr.io/knative-nightly/knative.dev/eventing/cmd/heartbeats . <pod-name> is the name of the Pod that the container runs in, for example, mypod . <pod-namespace> is the namespace that the Pod runs in, for example, event-test . <sink> is the name of your sink, for example, event-display . For a list of available options, see the Knative client documentation . Create a YAML file using the following template: apiVersion : sources.knative.dev/v1 kind : ContainerSource metadata : name : <containersource-name> spec : template : spec : containers : - image : <event-source-image-uri> name : <container-name> env : - name : POD_NAME value : \"<pod-name>\" - name : POD_NAMESPACE value : \"<pod-namespace>\" sink : ref : apiVersion : v1 kind : Service name : <sink> Where: <namespace> is the namespace you created for your ContainerSource, for example, containersource-example . <containersource-name> is the name you want for your ContainerSource, for example, test-heartbeats . <event-source-image-uri> corresponds to the image URI you built and published in step 1, for example, gcr.io/knative-nightly/knative.dev/eventing/cmd/heartbeats . <container-name> is the name of your event source, for example, heartbeats . <pod-name> is the name of the Pod that the container runs in, for example, mypod . <pod-namespace> is the namespace that the Pod runs in, for example, event-test . <sink> is the name of your sink, for example, event-display . For more information about the fields you can configure for the ContainerSource object, see ContainerSource Reference . Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Note Arguments and environment variables are set and are passed to the container.","title":"Create a ContainerSource object"},{"location":"eventing/custom-event-source/containersource/#verify-the-containersource-object","text":"View the logs for your event consumer by running the command: kubectl -n <namespace> logs -l <pod-name> --tail = 200 Where: <namespace> is the namespace that contains the ContainerSource object. <pod-name> is the name of the Pod that the container runs in. For example: $ kubectl -n containersource-example logs -l app = event-display --tail = 200 Verify that the output returns the properties of the events that your ContainerSource sent to your sink. In the following example, the command has returned the Attributes and Data properties of the events that the ContainerSource sent to the event-display Service: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.eventing.samples.heartbeat source: https://knative.dev/eventing/cmd/heartbeats/#event-test/mypod id: 2b72d7bf-c38f-4a98-a433-608fbcdd2596 time: 2019-10-18T15:23:20.809775386Z contenttype: application/json Extensions, beats: true heart: yes the: 42 Data, { \"id\": 2, \"label\": \"\" }","title":"Verify the ContainerSource object"},{"location":"eventing/custom-event-source/containersource/#delete-the-containersource-object","text":"To delete the ContainerSource object and all of the related resources in the namespace: Delete the namespace by running the command: kubectl delete namespace <namespace> Where <namespace> is the namespace that contains the ContainerSource object.","title":"Delete the ContainerSource object"},{"location":"eventing/custom-event-source/containersource/#reference-documentation","text":"See the ContainerSource reference .","title":"Reference Documentation"},{"location":"eventing/custom-event-source/containersource/reference/","text":"ContainerSource reference \u00b6 This topic provides reference information about the configurable fields for the ContainerSource object. ContainerSource \u00b6 A ContainerSource definition supports the following fields: Field Description Required or optional apiVersion Specifies the API version, for example sources.knative.dev/v1 . Required kind Identifies this resource object as a ContainerSource object. Required metadata Specifies metadata that uniquely identifies the ContainerSource object. For example, a name . Required spec Specifies the configuration information for this ContainerSource object. Required spec.sink A reference to an object that resolves to a URI to use as the sink. Required spec.template A template in the shape of Deployment.spec.template to be used for this ContainerSource. Required spec.ceOverrides Defines overrides to control the output format and modifications to the event sent to the sink. Optional Template parameter \u00b6 This is a template in the shape of Deployment.spec.template to use for the ContainerSource. For more information, see the Kubernetes Documentation . Example: template parameter \u00b6 apiVersion : sources.knative.dev/v1 kind : ContainerSource metadata : name : test-heartbeats spec : template : spec : containers : - image : gcr.io/knative-nightly/knative.dev/eventing/cmd/heartbeats name : heartbeats args : - --period=1 env : - name : POD_NAME value : \"mypod\" - name : POD_NAMESPACE value : \"event-test\" ... CloudEvent Overrides \u00b6 CloudEvent Overrides defines overrides to control the output format and modifications of the event sent to the sink. A ceOverrides definition supports the following fields: Field Description Required or optional extensions Specifies which attributes are added or overridden on the outbound event. Each extensions key-value pair is set independently on the event as an attribute extension. Optional Note Only valid CloudEvent attribute names are allowed as extensions. You cannot set the spec defined attributes from the extensions override configuration. For example, you can not modify the type attribute. Example: CloudEvent Overrides \u00b6 apiVersion : sources.knative.dev/v1 kind : ContainerSource metadata : name : test-heartbeats spec : ... ceOverrides : extensions : extra : this is an extra attribute additional : 42 Contract This results in the K_CE_OVERRIDES environment variable being set on the subject as follows: { \"extensions\" : { \"extra\" : \"this is an extra attribute\" , \"additional\" : \"42\" } }","title":"ContainerSource Reference"},{"location":"eventing/custom-event-source/containersource/reference/#containersource-reference","text":"This topic provides reference information about the configurable fields for the ContainerSource object.","title":"ContainerSource reference"},{"location":"eventing/custom-event-source/containersource/reference/#containersource","text":"A ContainerSource definition supports the following fields: Field Description Required or optional apiVersion Specifies the API version, for example sources.knative.dev/v1 . Required kind Identifies this resource object as a ContainerSource object. Required metadata Specifies metadata that uniquely identifies the ContainerSource object. For example, a name . Required spec Specifies the configuration information for this ContainerSource object. Required spec.sink A reference to an object that resolves to a URI to use as the sink. Required spec.template A template in the shape of Deployment.spec.template to be used for this ContainerSource. Required spec.ceOverrides Defines overrides to control the output format and modifications to the event sent to the sink. Optional","title":"ContainerSource"},{"location":"eventing/custom-event-source/containersource/reference/#template-parameter","text":"This is a template in the shape of Deployment.spec.template to use for the ContainerSource. For more information, see the Kubernetes Documentation .","title":"Template parameter"},{"location":"eventing/custom-event-source/containersource/reference/#example-template-parameter","text":"apiVersion : sources.knative.dev/v1 kind : ContainerSource metadata : name : test-heartbeats spec : template : spec : containers : - image : gcr.io/knative-nightly/knative.dev/eventing/cmd/heartbeats name : heartbeats args : - --period=1 env : - name : POD_NAME value : \"mypod\" - name : POD_NAMESPACE value : \"event-test\" ...","title":"Example: template parameter"},{"location":"eventing/custom-event-source/containersource/reference/#cloudevent-overrides","text":"CloudEvent Overrides defines overrides to control the output format and modifications of the event sent to the sink. A ceOverrides definition supports the following fields: Field Description Required or optional extensions Specifies which attributes are added or overridden on the outbound event. Each extensions key-value pair is set independently on the event as an attribute extension. Optional Note Only valid CloudEvent attribute names are allowed as extensions. You cannot set the spec defined attributes from the extensions override configuration. For example, you can not modify the type attribute.","title":"CloudEvent Overrides"},{"location":"eventing/custom-event-source/containersource/reference/#example-cloudevent-overrides","text":"apiVersion : sources.knative.dev/v1 kind : ContainerSource metadata : name : test-heartbeats spec : ... ceOverrides : extensions : extra : this is an extra attribute additional : 42 Contract This results in the K_CE_OVERRIDES environment variable being set on the subject as follows: { \"extensions\" : { \"extra\" : \"this is an extra attribute\" , \"additional\" : \"42\" } }","title":"Example: CloudEvent Overrides"},{"location":"eventing/custom-event-source/custom-event-source/","text":"Create a custom event source \u00b6 If you want to create a custom event source for a specific event producer type, you must create the components that enable forwarding events from that producer type to a Knative sink. This type of integration requires more effort than using some simpler integration types, such as SinkBinding or ContainerSource ; however, this provides the most polished result and is the easiest integration type for users to consume. By providing a custom resource definition (CRD) for your source rather than a general container definition, it is easier to expose meaningful configuration options and documentation to users and hide implementation details. Note If you have created a new event source type that is not a part of the core Knative project, you can open a pull request to add it to the list of Third-Party Sources , and announce the new source in one of the Knative Slack channels. You can also add your event source to the knative-sandbox organization, by following the instructions to create a sandbox repository . Required components \u00b6 To create a custom event source, you must create the following components: Component Description Receive adapter Contains logic that specifies how to get events from a producer, what the sink URI is, and how to translate events into the CloudEvent format. Kubernetes controller Manages the event source and reconciles underlying receive adapter deployments. Custom resource definition (CRD) Provides the configuration that the controller uses to manage the receive adapter. Using the sample source \u00b6 The Knative project provides a sample repository that contains a template for a basic event source controller and a receive adapter. For more information on using the sample source, see the documentation . Additional resources \u00b6 Implement CloudEvent binding interfaces, cloudevent's go sdk provides libraries for standard access to configure interfaces as needed. Controller runtime (this is what we share via injection) incorporates protocol specific config into \"generic controller\" CRD.","title":"Creating an event source overview"},{"location":"eventing/custom-event-source/custom-event-source/#create-a-custom-event-source","text":"If you want to create a custom event source for a specific event producer type, you must create the components that enable forwarding events from that producer type to a Knative sink. This type of integration requires more effort than using some simpler integration types, such as SinkBinding or ContainerSource ; however, this provides the most polished result and is the easiest integration type for users to consume. By providing a custom resource definition (CRD) for your source rather than a general container definition, it is easier to expose meaningful configuration options and documentation to users and hide implementation details. Note If you have created a new event source type that is not a part of the core Knative project, you can open a pull request to add it to the list of Third-Party Sources , and announce the new source in one of the Knative Slack channels. You can also add your event source to the knative-sandbox organization, by following the instructions to create a sandbox repository .","title":"Create a custom event source"},{"location":"eventing/custom-event-source/custom-event-source/#required-components","text":"To create a custom event source, you must create the following components: Component Description Receive adapter Contains logic that specifies how to get events from a producer, what the sink URI is, and how to translate events into the CloudEvent format. Kubernetes controller Manages the event source and reconciles underlying receive adapter deployments. Custom resource definition (CRD) Provides the configuration that the controller uses to manage the receive adapter.","title":"Required components"},{"location":"eventing/custom-event-source/custom-event-source/#using-the-sample-source","text":"The Knative project provides a sample repository that contains a template for a basic event source controller and a receive adapter. For more information on using the sample source, see the documentation .","title":"Using the sample source"},{"location":"eventing/custom-event-source/custom-event-source/#additional-resources","text":"Implement CloudEvent binding interfaces, cloudevent's go sdk provides libraries for standard access to configure interfaces as needed. Controller runtime (this is what we share via injection) incorporates protocol specific config into \"generic controller\" CRD.","title":"Additional resources"},{"location":"eventing/custom-event-source/custom-event-source/controller/","text":"Create a controller \u00b6 You can use the sample repository update-codegen.sh script to generate and inject the required components (the clientset , cache , informers , and listers ) into your custom controller. Example controller: import ( // ... sampleSourceClient \"knative.dev/sample-source/pkg/client/injection/client\" samplesourceinformer \"knative.dev/sample-source/pkg/client/injection/informers/samples/v1alpha1/samplesource\" ) // ... func NewController ( ctx context . Context , cmw configmap . Watcher ) * controller . Impl { sampleSourceInformer := samplesourceinformer . Get ( ctx ) r := & Reconciler { // ... samplesourceClientSet : sampleSourceClient . Get ( ctx ), samplesourceLister : sampleSourceInformer . Lister (), // ... } Procedure \u00b6 Generate the components by running the command: ${ CODEGEN_PKG } /generate-groups.sh \"deepcopy,client,informer,lister\" \\ knative.dev/sample-source/pkg/client knative.dev/sample-source/pkg/apis \\ \"samples:v1alpha1\" \\ --go-header-file ${ REPO_ROOT } /hack/boilerplate/boilerplate.go.txt Inject the components by running the command: # Injection ${ KNATIVE_CODEGEN_PKG } /hack/generate-knative.sh \"injection\" \\ knative.dev/sample-source/pkg/client knative.dev/sample-source/pkg/apis \\ \"samples:v1alpha1\" \\ --go-header-file ${ REPO_ROOT } /hack/boilerplate/boilerplate.go.txt Pass the new controller implementation to the sharedmain method: import ( // The set of controllers this controller process runs. \"knative.dev/sample-source/pkg/reconciler/sample\" // This defines the shared main for injected controllers. \"knative.dev/pkg/injection/sharedmain\" ) func main () { sharedmain . Main ( \"sample-source-controller\" , sample . NewController ) } Define the NewController implementation: func NewController ( ctx context . Context , cmw configmap . Watcher , ) * controller . Impl { // ... deploymentInformer := deploymentinformer . Get ( ctx ) sinkBindingInformer := sinkbindinginformer . Get ( ctx ) sampleSourceInformer := samplesourceinformer . Get ( ctx ) r := & Reconciler { dr : & reconciler . DeploymentReconciler { KubeClientSet : kubeclient . Get ( ctx )}, sbr : & reconciler . SinkBindingReconciler { EventingClientSet : eventingclient . Get ( ctx )}, // Config accessor takes care of tracing/config/logging config propagation to the receive adapter configAccessor : reconcilersource . WatchConfigurations ( ctx , \"sample-source\" , cmw ), } A configmap.Watcher and a context, which the injected listers use for the reconciler struct arguments, are passed to this implementation. Import the base reconciler from the knative.dev/pkg dependency: import ( // ... reconcilersource \"knative.dev/eventing/pkg/reconciler/source\" // ... ) Ensure that the event handlers are being filtered to the correct informers: sampleSourceInformer . Informer (). AddEventHandler ( controller . HandleAll ( impl . Enqueue )) Ensure that informers are configured correctly for the secondary resources used by the sample source to deploy and bind the event source and the receive adapter: deploymentInformer . Informer (). AddEventHandler ( cache . FilteringResourceEventHandler { FilterFunc : controller . FilterGroupKind ( v1alpha1 . Kind ( \"SampleSource\" )), Handler : controller . HandleAll ( impl . EnqueueControllerOf ), }) sinkBindingInformer . Informer (). AddEventHandler ( cache . FilteringResourceEventHandler { FilterFunc : controller . FilterGroupKind ( v1alpha1 . Kind ( \"SampleSource\" )), Handler : controller . HandleAll ( impl . EnqueueControllerOf ), })","title":"Create a controller"},{"location":"eventing/custom-event-source/custom-event-source/controller/#create-a-controller","text":"You can use the sample repository update-codegen.sh script to generate and inject the required components (the clientset , cache , informers , and listers ) into your custom controller. Example controller: import ( // ... sampleSourceClient \"knative.dev/sample-source/pkg/client/injection/client\" samplesourceinformer \"knative.dev/sample-source/pkg/client/injection/informers/samples/v1alpha1/samplesource\" ) // ... func NewController ( ctx context . Context , cmw configmap . Watcher ) * controller . Impl { sampleSourceInformer := samplesourceinformer . Get ( ctx ) r := & Reconciler { // ... samplesourceClientSet : sampleSourceClient . Get ( ctx ), samplesourceLister : sampleSourceInformer . Lister (), // ... }","title":"Create a controller"},{"location":"eventing/custom-event-source/custom-event-source/controller/#procedure","text":"Generate the components by running the command: ${ CODEGEN_PKG } /generate-groups.sh \"deepcopy,client,informer,lister\" \\ knative.dev/sample-source/pkg/client knative.dev/sample-source/pkg/apis \\ \"samples:v1alpha1\" \\ --go-header-file ${ REPO_ROOT } /hack/boilerplate/boilerplate.go.txt Inject the components by running the command: # Injection ${ KNATIVE_CODEGEN_PKG } /hack/generate-knative.sh \"injection\" \\ knative.dev/sample-source/pkg/client knative.dev/sample-source/pkg/apis \\ \"samples:v1alpha1\" \\ --go-header-file ${ REPO_ROOT } /hack/boilerplate/boilerplate.go.txt Pass the new controller implementation to the sharedmain method: import ( // The set of controllers this controller process runs. \"knative.dev/sample-source/pkg/reconciler/sample\" // This defines the shared main for injected controllers. \"knative.dev/pkg/injection/sharedmain\" ) func main () { sharedmain . Main ( \"sample-source-controller\" , sample . NewController ) } Define the NewController implementation: func NewController ( ctx context . Context , cmw configmap . Watcher , ) * controller . Impl { // ... deploymentInformer := deploymentinformer . Get ( ctx ) sinkBindingInformer := sinkbindinginformer . Get ( ctx ) sampleSourceInformer := samplesourceinformer . Get ( ctx ) r := & Reconciler { dr : & reconciler . DeploymentReconciler { KubeClientSet : kubeclient . Get ( ctx )}, sbr : & reconciler . SinkBindingReconciler { EventingClientSet : eventingclient . Get ( ctx )}, // Config accessor takes care of tracing/config/logging config propagation to the receive adapter configAccessor : reconcilersource . WatchConfigurations ( ctx , \"sample-source\" , cmw ), } A configmap.Watcher and a context, which the injected listers use for the reconciler struct arguments, are passed to this implementation. Import the base reconciler from the knative.dev/pkg dependency: import ( // ... reconcilersource \"knative.dev/eventing/pkg/reconciler/source\" // ... ) Ensure that the event handlers are being filtered to the correct informers: sampleSourceInformer . Informer (). AddEventHandler ( controller . HandleAll ( impl . Enqueue )) Ensure that informers are configured correctly for the secondary resources used by the sample source to deploy and bind the event source and the receive adapter: deploymentInformer . Informer (). AddEventHandler ( cache . FilteringResourceEventHandler { FilterFunc : controller . FilterGroupKind ( v1alpha1 . Kind ( \"SampleSource\" )), Handler : controller . HandleAll ( impl . EnqueueControllerOf ), }) sinkBindingInformer . Informer (). AddEventHandler ( cache . FilteringResourceEventHandler { FilterFunc : controller . FilterGroupKind ( v1alpha1 . Kind ( \"SampleSource\" )), Handler : controller . HandleAll ( impl . EnqueueControllerOf ), })","title":"Procedure"},{"location":"eventing/custom-event-source/custom-event-source/publish-event-source/","text":"Publish an event source to your cluster \u00b6 Start a minikube cluster: minikube start Setup ko to use the minikube docker instance and local registry: eval $( minikube docker-env ) export KO_DOCKER_REPO = ko.local Apply the CRD and configuration YAML: ko apply -f config Once the sample-source-controller-manager is running in the knative-samples namespace, you can apply the example.yaml to connect our sample-source every 10s directly to a ksvc . apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display namespace : knative-samples spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- apiVersion : samples.knative.dev/v1alpha1 kind : SampleSource metadata : name : sample-source namespace : knative-samples spec : interval : \"10s\" sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display ko apply -f example.yaml Once reconciled, you can confirm the ksvc is outputting valid cloudevents every 10s to align with our specified interval. % kubectl -n knative-samples logs -l serving.knative.dev/service = event-display -c user-container -f \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.sample source: http://sample.knative.dev/heartbeat-source id: d4619592-363e-4a41-82d1-b1586c390e24 time: 2019-12-17T01:31:10.795588888Z datacontenttype: application/json Data, { \"Sequence\": 0, \"Heartbeat\": \"10s\" } \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.sample source: http://sample.knative.dev/heartbeat-source id: db2edad0-06bc-4234-b9e1-7ea3955841d6 time: 2019-12-17T01:31:20.825969504Z datacontenttype: application/json Data, { \"Sequence\": 1, \"Heartbeat\": \"10s\" }","title":"Publishing an event source to your cluster"},{"location":"eventing/custom-event-source/custom-event-source/publish-event-source/#publish-an-event-source-to-your-cluster","text":"Start a minikube cluster: minikube start Setup ko to use the minikube docker instance and local registry: eval $( minikube docker-env ) export KO_DOCKER_REPO = ko.local Apply the CRD and configuration YAML: ko apply -f config Once the sample-source-controller-manager is running in the knative-samples namespace, you can apply the example.yaml to connect our sample-source every 10s directly to a ksvc . apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display namespace : knative-samples spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- apiVersion : samples.knative.dev/v1alpha1 kind : SampleSource metadata : name : sample-source namespace : knative-samples spec : interval : \"10s\" sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display ko apply -f example.yaml Once reconciled, you can confirm the ksvc is outputting valid cloudevents every 10s to align with our specified interval. % kubectl -n knative-samples logs -l serving.knative.dev/service = event-display -c user-container -f \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.sample source: http://sample.knative.dev/heartbeat-source id: d4619592-363e-4a41-82d1-b1586c390e24 time: 2019-12-17T01:31:10.795588888Z datacontenttype: application/json Data, { \"Sequence\": 0, \"Heartbeat\": \"10s\" } \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1.0 type: dev.knative.sample source: http://sample.knative.dev/heartbeat-source id: db2edad0-06bc-4234-b9e1-7ea3955841d6 time: 2019-12-17T01:31:20.825969504Z datacontenttype: application/json Data, { \"Sequence\": 1, \"Heartbeat\": \"10s\" }","title":"Publish an event source to your cluster"},{"location":"eventing/custom-event-source/custom-event-source/receive-adapter/","text":"Create a receive adapter \u00b6 As part of the source reconciliation process, you must create and deploy the underlying receive adapter. The receive adapter requires an injection-based main method that is located in cmd/receiver_adapter/main.go : // This Adapter generates events at a regular interval. package main import ( \"knative.dev/eventing/pkg/adapter\" myadapter \"knative.dev/sample-source/pkg/adapter\" ) func main () { adapter . Main ( \"sample-source\" , myadapter . NewEnv , myadapter . NewAdapter ) } The receive adapter's pkg implementation consists of two main functions: A NewAdapter(ctx context.Context, aEnv adapter.EnvConfigAccessor, ceClient cloudevents.Client) adapter.Adapter {} call, which creates the new adapter with passed variables via the EnvConfigAccessor . The created adapter is passed the CloudEvents client (which is where the events are forwarded to). This is sometimes referred to as a sink, or ceClient in the Knative ecosystem. The return value is a reference to the adapter as defined by the adapter's local struct. In the case of the sample source: // Adapter generates events at a regular interval. type Adapter struct { logger * zap . Logger interval time . Duration nextID int client cloudevents . Client } A Start function, implemented as an interface to the adapter struct : func ( a * Adapter ) Start ( stopCh <- chan struct {}) error { stopCh is the signal to stop the adapter. Otherwise, the role of the function is to process the next event. In the case of the sample-source , this function creates a CloudEvent to forward to the specified sink every X interval, as specified by the EnvConfigAccessor parameter, which is loaded by the resource YAML: func ( a * Adapter ) Start ( stopCh <- chan struct {}) error { a . logger . Infow ( \"Starting heartbeat\" , zap . String ( \"interval\" , a . interval . String ())) for { select { case <- time . After ( a . interval ): event := a . newEvent () a . logger . Infow ( \"Sending new event\" , zap . String ( \"event\" , event . String ())) if result := a . client . Send ( context . Background (), event ); ! cloudevents . IsACK ( result ) { a . logger . Infow ( \"failed to send event\" , zap . String ( \"event\" , event . String ()), zap . Error ( result )) // We got an error but it could be transient, try again next interval. continue } case <- stopCh : a . logger . Info ( \"Shutting down...\" ) return nil } } } Managing the Receive Adapter in the Controller \u00b6 Update the ObservedGeneration and initialize the Status conditions, as defined in the samplesource_lifecycle.go and samplesource_types.go files: src . Status . InitializeConditions () src . Status . ObservedGeneration = src . Generation Create a receive adapter. Verify that the specified Kubernetes resources are valid, and update the Status accordingly. Assemble the ReceiveAdapterArgs : raArgs := resources . ReceiveAdapterArgs { EventSource : src . Namespace + \"/\" + src . Name , Image : r . ReceiveAdapterImage , Source : src , Labels : resources . Labels ( src . Name ), AdditionalEnvs : r . configAccessor . ToEnvVars (), // Grab config envs for tracing/logging/metrics } Note The exact arguments may change based on functional requirements. Create the underlying deployment from the arguments provided, matching pod templates, labels, owner references, etc as needed to fill out the deployment. Example: pkg/reconciler/sample/resources/receive_adapter.go Fetch the existing receive adapter deployment: namespace := owner . GetObjectMeta (). GetNamespace () ra , err := r . KubeClientSet . AppsV1 (). Deployments ( namespace ). Get ( expected . Name , metav1 . GetOptions {}) If there is no existing receive adapter deployment, create one: ra , err = r . KubeClientSet . AppsV1 (). Deployments ( namespace ). Create ( expected ) Check if the expected spec is different from the existing spec, and update the deployment if required: } else if r . podSpecImageSync ( expected . Spec . Template . Spec , ra . Spec . Template . Spec ) { ra . Spec . Template . Spec = expected . Spec . Template . Spec if ra , err = r . KubeClientSet . AppsV1 (). Deployments ( namespace ). Update ( ra ); err != nil { return ra , err } If updated, record the event: return pkgreconciler . NewEvent ( corev1 . EventTypeNormal , \"DeploymentUpdated\" , \"updated deployment: \\\"%s/%s\\\"\" , namespace , name ) If successful, update the Status and MarkDeployed : src . Status . PropagateDeploymentAvailability ( ra ) Create a SinkBinding to bind the receive adapter with the sink. Create a Reference for the receive adapter deployment. This deployment is the SinkBinding's source: tracker . Reference { APIVersion : appsv1 . SchemeGroupVersion . String (), Kind : \"Deployment\" , Namespace : ra . Namespace , Name : ra . Name , } Fetch the existing SinkBinding: namespace := owner . GetObjectMeta (). GetNamespace () sb , err := r . EventingClientSet . SourcesV1alpha2 (). SinkBindings ( namespace ). Get ( expected . Name , metav1 . GetOptions {}) If there is no existing SinkBinding, create one: sb , err = r . EventingClientSet . SourcesV1alpha2 (). SinkBindings ( namespace ). Create ( expected ) Check if the expected spec is different to the existing spec, and update the SinkBinding if required: else if r . specChanged ( sb . Spec , expected . Spec ) { sb . Spec = expected . Spec if sb , err = r . EventingClientSet . SourcesV1alpha2 (). SinkBindings ( namespace ). Update ( sb ); err != nil { return sb , err } If updated, record the event: return pkgreconciler . NewEvent ( corev1 . EventTypeNormal , \"SinkBindingUpdated\" , \"updated SinkBinding: \\\"%s/%s\\\"\" , namespace , name ) MarkSink with the result: src . Status . MarkSink ( sb . Status . SinkURI ) Return a new reconciler event stating that the process is done: return pkgreconciler . NewEvent ( corev1 . EventTypeNormal , \"SampleSourceReconciled\" , \"SampleSource reconciled: \\\"%s/%s\\\"\" , namespace , name )","title":"Create a receive adapter"},{"location":"eventing/custom-event-source/custom-event-source/receive-adapter/#create-a-receive-adapter","text":"As part of the source reconciliation process, you must create and deploy the underlying receive adapter. The receive adapter requires an injection-based main method that is located in cmd/receiver_adapter/main.go : // This Adapter generates events at a regular interval. package main import ( \"knative.dev/eventing/pkg/adapter\" myadapter \"knative.dev/sample-source/pkg/adapter\" ) func main () { adapter . Main ( \"sample-source\" , myadapter . NewEnv , myadapter . NewAdapter ) } The receive adapter's pkg implementation consists of two main functions: A NewAdapter(ctx context.Context, aEnv adapter.EnvConfigAccessor, ceClient cloudevents.Client) adapter.Adapter {} call, which creates the new adapter with passed variables via the EnvConfigAccessor . The created adapter is passed the CloudEvents client (which is where the events are forwarded to). This is sometimes referred to as a sink, or ceClient in the Knative ecosystem. The return value is a reference to the adapter as defined by the adapter's local struct. In the case of the sample source: // Adapter generates events at a regular interval. type Adapter struct { logger * zap . Logger interval time . Duration nextID int client cloudevents . Client } A Start function, implemented as an interface to the adapter struct : func ( a * Adapter ) Start ( stopCh <- chan struct {}) error { stopCh is the signal to stop the adapter. Otherwise, the role of the function is to process the next event. In the case of the sample-source , this function creates a CloudEvent to forward to the specified sink every X interval, as specified by the EnvConfigAccessor parameter, which is loaded by the resource YAML: func ( a * Adapter ) Start ( stopCh <- chan struct {}) error { a . logger . Infow ( \"Starting heartbeat\" , zap . String ( \"interval\" , a . interval . String ())) for { select { case <- time . After ( a . interval ): event := a . newEvent () a . logger . Infow ( \"Sending new event\" , zap . String ( \"event\" , event . String ())) if result := a . client . Send ( context . Background (), event ); ! cloudevents . IsACK ( result ) { a . logger . Infow ( \"failed to send event\" , zap . String ( \"event\" , event . String ()), zap . Error ( result )) // We got an error but it could be transient, try again next interval. continue } case <- stopCh : a . logger . Info ( \"Shutting down...\" ) return nil } } }","title":"Create a receive adapter"},{"location":"eventing/custom-event-source/custom-event-source/receive-adapter/#managing-the-receive-adapter-in-the-controller","text":"Update the ObservedGeneration and initialize the Status conditions, as defined in the samplesource_lifecycle.go and samplesource_types.go files: src . Status . InitializeConditions () src . Status . ObservedGeneration = src . Generation Create a receive adapter. Verify that the specified Kubernetes resources are valid, and update the Status accordingly. Assemble the ReceiveAdapterArgs : raArgs := resources . ReceiveAdapterArgs { EventSource : src . Namespace + \"/\" + src . Name , Image : r . ReceiveAdapterImage , Source : src , Labels : resources . Labels ( src . Name ), AdditionalEnvs : r . configAccessor . ToEnvVars (), // Grab config envs for tracing/logging/metrics } Note The exact arguments may change based on functional requirements. Create the underlying deployment from the arguments provided, matching pod templates, labels, owner references, etc as needed to fill out the deployment. Example: pkg/reconciler/sample/resources/receive_adapter.go Fetch the existing receive adapter deployment: namespace := owner . GetObjectMeta (). GetNamespace () ra , err := r . KubeClientSet . AppsV1 (). Deployments ( namespace ). Get ( expected . Name , metav1 . GetOptions {}) If there is no existing receive adapter deployment, create one: ra , err = r . KubeClientSet . AppsV1 (). Deployments ( namespace ). Create ( expected ) Check if the expected spec is different from the existing spec, and update the deployment if required: } else if r . podSpecImageSync ( expected . Spec . Template . Spec , ra . Spec . Template . Spec ) { ra . Spec . Template . Spec = expected . Spec . Template . Spec if ra , err = r . KubeClientSet . AppsV1 (). Deployments ( namespace ). Update ( ra ); err != nil { return ra , err } If updated, record the event: return pkgreconciler . NewEvent ( corev1 . EventTypeNormal , \"DeploymentUpdated\" , \"updated deployment: \\\"%s/%s\\\"\" , namespace , name ) If successful, update the Status and MarkDeployed : src . Status . PropagateDeploymentAvailability ( ra ) Create a SinkBinding to bind the receive adapter with the sink. Create a Reference for the receive adapter deployment. This deployment is the SinkBinding's source: tracker . Reference { APIVersion : appsv1 . SchemeGroupVersion . String (), Kind : \"Deployment\" , Namespace : ra . Namespace , Name : ra . Name , } Fetch the existing SinkBinding: namespace := owner . GetObjectMeta (). GetNamespace () sb , err := r . EventingClientSet . SourcesV1alpha2 (). SinkBindings ( namespace ). Get ( expected . Name , metav1 . GetOptions {}) If there is no existing SinkBinding, create one: sb , err = r . EventingClientSet . SourcesV1alpha2 (). SinkBindings ( namespace ). Create ( expected ) Check if the expected spec is different to the existing spec, and update the SinkBinding if required: else if r . specChanged ( sb . Spec , expected . Spec ) { sb . Spec = expected . Spec if sb , err = r . EventingClientSet . SourcesV1alpha2 (). SinkBindings ( namespace ). Update ( sb ); err != nil { return sb , err } If updated, record the event: return pkgreconciler . NewEvent ( corev1 . EventTypeNormal , \"SinkBindingUpdated\" , \"updated SinkBinding: \\\"%s/%s\\\"\" , namespace , name ) MarkSink with the result: src . Status . MarkSink ( sb . Status . SinkURI ) Return a new reconciler event stating that the process is done: return pkgreconciler . NewEvent ( corev1 . EventTypeNormal , \"SampleSourceReconciled\" , \"SampleSource reconciled: \\\"%s/%s\\\"\" , namespace , name )","title":"Managing the Receive Adapter in the Controller"},{"location":"eventing/custom-event-source/custom-event-source/sample-repo/","text":"Using the Knative sample repository \u00b6 The Knative project provides a sample repository that contains a template for a basic event source controller and a receive adapter. Prerequisites \u00b6 You are familiar with Kubernetes and Go development. You have installed Git. You have installed Go. You have cloned the sample-source repository . Optional: Install the ko CLI tool. Install the kubectl CLI tool. Set up minikube or kind . Sample files overview \u00b6 Receiver adapter files \u00b6 cmd/receive_adapter/main.go - Translates resource variables to the underlying adapter structure, so that they can be passed into the Knative system. pkg/adapter/adapter.go - The functions that support receiver adapter translation of events to CloudEvents. Controller files \u00b6 cmd/controller/main.go - Passes the event source's NewController implementation to the shared main method. pkg/reconciler/sample/controller.go - The NewController implementation that is passed to the shared main method. CRD files \u00b6 pkg/apis/samples/VERSION/samplesource_types.go - The schema for the underlying API types, which provide the variables to be defined in the resource YAML file. Reconciler files \u00b6 pkg/reconciler/sample/samplesource.go - The reconciliation functions for the receive adapter. pkg/apis/samples/VERSION/samplesource_lifecycle.go - Contains status information for the event source\u2019s reconciliation details: Source ready Sink provided Deployed EventType provided Kubernetes resources correct Procedure \u00b6 Define the types required in the resource\u2019s schema in pkg/apis/samples/v1alpha1/samplesource_types.go . This includes the fields that are required in the resource YAML, as well as those referenced in the controller using the source\u2019s clientset and API: // +genclient // +genreconciler // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object // +k8s:openapi-gen=true type SampleSource struct { metav1 . TypeMeta `json:\",inline\"` // +optional metav1 . ObjectMeta `json:\"metadata,omitempty\"` // Spec holds the desired state of the SampleSource (from the client). Spec SampleSourceSpec `json:\"spec\"` // Status communicates the observed state of the SampleSource (from the controller). // +optional Status SampleSourceStatus `json:\"status,omitempty\"` } // SampleSourceSpec holds the desired state of the SampleSource (from the client). type SampleSourceSpec struct { // inherits duck/v1 SourceSpec, which currently provides: // * Sink - a reference to an object that will resolve to a domain name or // a URI directly to use as the sink. // * CloudEventOverrides - defines overrides to control the output format // and modifications of the event sent to the sink. duckv1 . SourceSpec `json:\",inline\"` // ServiceAccountName holds the name of the Kubernetes service account // as which the underlying K8s resources should be run. If unspecified // this will default to the \"default\" service account for the namespace // in which the SampleSource exists. // +optional ServiceAccountName string `json:\"serviceAccountName,omitempty\"` // Interval is the time interval between events. // // The string format is a sequence of decimal numbers, each with optional // fraction and a unit suffix, such as \"300ms\", \"-1.5h\" or \"2h45m\". Valid time // units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\". If unspecified // this will default to \"10s\". Interval string `json:\"interval\"` } // SampleSourceStatus communicates the observed state of the SampleSource (from the controller). type SampleSourceStatus struct { duckv1 . Status `json:\",inline\"` // SinkURI is the current active sink URI that has been configured // for the SampleSource. // +optional SinkURI * apis . URL `json:\"sinkUri,omitempty\"` } Define the lifecycle to be reflected in the status and SinkURI fields: const ( // SampleConditionReady has status True when the SampleSource is ready to send events. SampleConditionReady = apis . ConditionReady // ... ) Set the lifecycle conditions by defining the functions to be called from the reconciler functions. This is typically done in pkg/apis/samples/VERSION/samplesource_lifecycle.go : // InitializeConditions sets relevant unset conditions to Unknown state. func ( s * SampleSourceStatus ) InitializeConditions () { SampleCondSet . Manage ( s ). InitializeConditions () } ... // MarkSink sets the condition that the source has a sink configured. func ( s * SampleSourceStatus ) MarkSink ( uri * apis . URL ) { s . SinkURI = uri if len ( uri . String ()) > 0 { SampleCondSet . Manage ( s ). MarkTrue ( SampleConditionSinkProvided ) } else { SampleCondSet . Manage ( s ). MarkUnknown ( SampleConditionSinkProvided , \"SinkEmpty\" , \"Sink has resolved to empty.%s\" , \"\" ) } } // MarkNoSink sets the condition that the source does not have a sink configured. func ( s * SampleSourceStatus ) MarkNoSink ( reason , messageFormat string , messageA ... interface {}) { SampleCondSet . Manage ( s ). MarkFalse ( SampleConditionSinkProvided , reason , messageFormat , messageA ... ) }","title":"Configure the sample"},{"location":"eventing/custom-event-source/custom-event-source/sample-repo/#using-the-knative-sample-repository","text":"The Knative project provides a sample repository that contains a template for a basic event source controller and a receive adapter.","title":"Using the Knative sample repository"},{"location":"eventing/custom-event-source/custom-event-source/sample-repo/#prerequisites","text":"You are familiar with Kubernetes and Go development. You have installed Git. You have installed Go. You have cloned the sample-source repository . Optional: Install the ko CLI tool. Install the kubectl CLI tool. Set up minikube or kind .","title":"Prerequisites"},{"location":"eventing/custom-event-source/custom-event-source/sample-repo/#sample-files-overview","text":"","title":"Sample files overview"},{"location":"eventing/custom-event-source/custom-event-source/sample-repo/#receiver-adapter-files","text":"cmd/receive_adapter/main.go - Translates resource variables to the underlying adapter structure, so that they can be passed into the Knative system. pkg/adapter/adapter.go - The functions that support receiver adapter translation of events to CloudEvents.","title":"Receiver adapter files"},{"location":"eventing/custom-event-source/custom-event-source/sample-repo/#controller-files","text":"cmd/controller/main.go - Passes the event source's NewController implementation to the shared main method. pkg/reconciler/sample/controller.go - The NewController implementation that is passed to the shared main method.","title":"Controller files"},{"location":"eventing/custom-event-source/custom-event-source/sample-repo/#crd-files","text":"pkg/apis/samples/VERSION/samplesource_types.go - The schema for the underlying API types, which provide the variables to be defined in the resource YAML file.","title":"CRD files"},{"location":"eventing/custom-event-source/custom-event-source/sample-repo/#reconciler-files","text":"pkg/reconciler/sample/samplesource.go - The reconciliation functions for the receive adapter. pkg/apis/samples/VERSION/samplesource_lifecycle.go - Contains status information for the event source\u2019s reconciliation details: Source ready Sink provided Deployed EventType provided Kubernetes resources correct","title":"Reconciler files"},{"location":"eventing/custom-event-source/custom-event-source/sample-repo/#procedure","text":"Define the types required in the resource\u2019s schema in pkg/apis/samples/v1alpha1/samplesource_types.go . This includes the fields that are required in the resource YAML, as well as those referenced in the controller using the source\u2019s clientset and API: // +genclient // +genreconciler // +k8s:deepcopy-gen:interfaces=k8s.io/apimachinery/pkg/runtime.Object // +k8s:openapi-gen=true type SampleSource struct { metav1 . TypeMeta `json:\",inline\"` // +optional metav1 . ObjectMeta `json:\"metadata,omitempty\"` // Spec holds the desired state of the SampleSource (from the client). Spec SampleSourceSpec `json:\"spec\"` // Status communicates the observed state of the SampleSource (from the controller). // +optional Status SampleSourceStatus `json:\"status,omitempty\"` } // SampleSourceSpec holds the desired state of the SampleSource (from the client). type SampleSourceSpec struct { // inherits duck/v1 SourceSpec, which currently provides: // * Sink - a reference to an object that will resolve to a domain name or // a URI directly to use as the sink. // * CloudEventOverrides - defines overrides to control the output format // and modifications of the event sent to the sink. duckv1 . SourceSpec `json:\",inline\"` // ServiceAccountName holds the name of the Kubernetes service account // as which the underlying K8s resources should be run. If unspecified // this will default to the \"default\" service account for the namespace // in which the SampleSource exists. // +optional ServiceAccountName string `json:\"serviceAccountName,omitempty\"` // Interval is the time interval between events. // // The string format is a sequence of decimal numbers, each with optional // fraction and a unit suffix, such as \"300ms\", \"-1.5h\" or \"2h45m\". Valid time // units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\". If unspecified // this will default to \"10s\". Interval string `json:\"interval\"` } // SampleSourceStatus communicates the observed state of the SampleSource (from the controller). type SampleSourceStatus struct { duckv1 . Status `json:\",inline\"` // SinkURI is the current active sink URI that has been configured // for the SampleSource. // +optional SinkURI * apis . URL `json:\"sinkUri,omitempty\"` } Define the lifecycle to be reflected in the status and SinkURI fields: const ( // SampleConditionReady has status True when the SampleSource is ready to send events. SampleConditionReady = apis . ConditionReady // ... ) Set the lifecycle conditions by defining the functions to be called from the reconciler functions. This is typically done in pkg/apis/samples/VERSION/samplesource_lifecycle.go : // InitializeConditions sets relevant unset conditions to Unknown state. func ( s * SampleSourceStatus ) InitializeConditions () { SampleCondSet . Manage ( s ). InitializeConditions () } ... // MarkSink sets the condition that the source has a sink configured. func ( s * SampleSourceStatus ) MarkSink ( uri * apis . URL ) { s . SinkURI = uri if len ( uri . String ()) > 0 { SampleCondSet . Manage ( s ). MarkTrue ( SampleConditionSinkProvided ) } else { SampleCondSet . Manage ( s ). MarkUnknown ( SampleConditionSinkProvided , \"SinkEmpty\" , \"Sink has resolved to empty.%s\" , \"\" ) } } // MarkNoSink sets the condition that the source does not have a sink configured. func ( s * SampleSourceStatus ) MarkNoSink ( reason , messageFormat string , messageA ... interface {}) { SampleCondSet . Manage ( s ). MarkFalse ( SampleConditionSinkProvided , reason , messageFormat , messageA ... ) }","title":"Procedure"},{"location":"eventing/custom-event-source/sinkbinding/","text":"SinkBinding \u00b6 The SinkBinding object supports decoupling event production from delivery addressing. You can use sink binding to direct a subject to a sink. A subject is a Kubernetes resource that embeds a PodSpec template and produces events. A sink is an addressable Kubernetes object that can receive events. The SinkBinding object injects environment variables into the PodTemplateSpec of the sink. Because of this, the application code does not need to interact directly with the Kubernetes API to locate the event destination. These environment variables are as follows: K_SINK - The URL of the resolved sink. K_CE_OVERRIDES - A JSON object that specifies overrides to the outbound event.","title":"About SinkBinding"},{"location":"eventing/custom-event-source/sinkbinding/#sinkbinding","text":"The SinkBinding object supports decoupling event production from delivery addressing. You can use sink binding to direct a subject to a sink. A subject is a Kubernetes resource that embeds a PodSpec template and produces events. A sink is an addressable Kubernetes object that can receive events. The SinkBinding object injects environment variables into the PodTemplateSpec of the sink. Because of this, the application code does not need to interact directly with the Kubernetes API to locate the event destination. These environment variables are as follows: K_SINK - The URL of the resolved sink. K_CE_OVERRIDES - A JSON object that specifies overrides to the outbound event.","title":"SinkBinding"},{"location":"eventing/custom-event-source/sinkbinding/create-a-sinkbinding/","text":"Create a SinkBinding \u00b6 This topic describes how to create a SinkBinding object. SinkBinding resolves a sink as a URI, sets the URI in the environment variable K_SINK , and adds the URI to a subject using K_SINK . If the URI changes, SinkBinding updates the value of K_SINK . In the following examples, the sink is a Knative Service and the subject is a CronJob. If you have an existing subject and sink, you can replace the examples with your own values. Before you begin \u00b6 Before you can create a SinkBinding object: You must have Knative Eventing installed on your cluster. Optional: If you want to use kn commands with SinkBinding, install the kn CLI. Optional: Choose SinkBinding namespace selection behavior \u00b6 The SinkBinding object operates in one of two modes: exclusion or inclusion . The default mode is exclusion . In exclusion mode, SinkBinding behavior is enabled for the namespace by default. To disallow a namespace from being evaluated for mutation you must exclude it using the label bindings.knative.dev/exclude: true . In inclusion mode, SinkBinding behavior is not enabled for the namespace. Before a namespace can be evaluated for mutation, you must explicitly include it using the label bindings.knative.dev/include: true . To set the SinkBinding object to inclusion mode: Change the value of SINK_BINDING_SELECTION_MODE from exclusion to inclusion by running: kubectl -n knative-eventing set env deployments eventing-webhook --containers = \"eventing-webhook\" SINK_BINDING_SELECTION_MODE = inclusion To verify that SINK_BINDING_SELECTION_MODE is set as desired, run: kubectl -n knative-eventing set env deployments eventing-webhook --containers = \"eventing-webhook\" --list | grep SINK_BINDING Create a namespace \u00b6 If you do not have an existing namespace, create a namespace for the SinkBinding object: kubectl create namespace <namespace> Where <namespace> is the namespace that you want your SinkBinding to use. For example, sinkbinding-example . Note If you have selected inclusion mode, you must add the bindings.knative.dev/include: true label to the namespace to enable SinkBinding behavior. Create a sink \u00b6 The sink can be any addressable Kubernetes object that can receive events. If you do not have an existing sink that you want to connect to the SinkBinding object, create a Knative service. Note To create a Knative service you must have Knative Serving installed on your cluster. kn YAML Create a Knative service by running: kn service create <app-name> --image <image-url> Where: <app-name> is the name of the application. <image-url> is the URL of the image container. For example: $ kn service create event-display --image gcr.io/knative-releases/knative.dev/eventing/cmd/event_display Create a YAML file for the Knative service using the following template: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : <app-name> spec : template : spec : containers : - image : <image-url> Where: <app-name> is the name of the application. For example, event-display . <image-url> is the URL of the image container. For example, gcr.io/knative-releases/knative.dev/eventing/cmd/event_display . Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Create a subject \u00b6 The subject must be a PodSpecable resource. You can use any PodSpecable resource in your cluster, for example: Deployment Job DaemonSet StatefulSet Service.serving.knative.dev If you do not have an existing PodSpecable subject that you want to use, you can use the following sample to create a CronJob object as the subject. The following CronJob makes a single cloud event that targets K_SINK and adds any extra overrides given by CE_OVERRIDES . Create a YAML file for the CronJob using the following example: apiVersion : batch/v1beta1 kind : CronJob metadata : name : heartbeat-cron spec : # Run every minute schedule : \"*/1 * * * *\" jobTemplate : metadata : labels : app : heartbeat-cron spec : template : spec : restartPolicy : Never containers : - name : single-heartbeat image : gcr.io/knative-nightly/knative.dev/eventing/cmd/heartbeats args : - --period=1 env : - name : ONE_SHOT value : \"true\" - name : POD_NAME valueFrom : fieldRef : fieldPath : metadata.name - name : POD_NAMESPACE valueFrom : fieldRef : fieldPath : metadata.namespace Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Create a SinkBinding object \u00b6 Create a SinkBinding object that directs events from your subject to the sink. kn YAML Create a SinkBinding object by running: kn source binding create <name> \\ --namespace <namespace> \\ --subject \"<subject>\" \\ --sink <sink> \\ --ce-override \"<cloudevent-overrides>\" Where: <name> is the name of the SinkBinding object you want to create. <namespace> is the namespace you created for your SinkBinding to use. <subject> is the subject to connect. Examples: Job:batch/v1:app=heartbeat-cron matches all jobs in namespace with label app=heartbeat-cron . Deployment:apps/v1:myapp matches a deployment called myapp in the namespace. Service:serving.knative.dev/v1:hello matches the service called hello . <sink> is the sink to connect. For example http://event-display.svc.cluster.local . Optional: <cloudevent-overrides> in the form key=value . Cloud Event overrides control the output format and modifications of the event sent to the sink and are applied before sending the event. You can provide this flag multiple times. For a list of available options, see the Knative client documentation . For example: $ kn source binding create bind-heartbeat \\ --namespace sinkbinding-example \\ --subject \"Job:batch/v1:app=heartbeat-cron\" \\ --sink http://event-display.svc.cluster.local \\ --ce-override \"sink=bound\" Create a YAML file for the SinkBinding object using the following template: apiVersion : sources.knative.dev/v1 kind : SinkBinding metadata : name : <name> spec : subject : apiVersion : <api-version> kind : <kind> selector : matchLabels : <label-key> : <label-value> sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : <sink> Where: <name> is the name of the SinkBinding object you want to create. For example, bind-heartbeat . <api-version> is the API version of the subject. For example batch/v1 . <kind> is the Kind of your subject. For example Job . <label-key>: <label-value> is a map of key-value pairs to select subjects that have a matching label. For example, app: heartbeat-cron selects any subject with the label app=heartbeat-cron . <sink> is the sink to connect. For example event-display . For more information about the fields you can configure for the SinkBinding object, see Sink Binding Reference . Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Verify the SinkBinding object \u00b6 Verify that a message was sent to the Knative eventing system by looking at the service logs for your sink: kubectl logs -l <sink> -c <container> --since = 10m Where: <sink> is the name of your sink. <container> is the name of the container your sink is running in. For example: $ kubectl logs -l serving.knative.dev/service = event-display -c user-container --since = 10m From the output, observe the lines showing the request headers and body of the event message, sent by the source to the display function. For example: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.eventing.samples.heartbeat source: https://knative.dev/eventing-contrib/cmd/heartbeats/#default/heartbeat-cron-1582120020-75qrz id: 5f4122be-ac6f-4349-a94f-4bfc6eb3f687 time: 2020 -02-19T13:47:10.41428688Z datacontenttype: application/json Extensions, beats: true heart: yes the: 42 Data, { \"id\" : 1 , \"label\" : \"\" } Delete a SinkBinding \u00b6 To delete the SinkBinding object and all of the related resources in the namespace, delete the namespace by running: kubectl delete namespace <namespace> Where <namespace> is the name of the namespace that contains the SinkBinding object.","title":"Create a SinkBinding"},{"location":"eventing/custom-event-source/sinkbinding/create-a-sinkbinding/#create-a-sinkbinding","text":"This topic describes how to create a SinkBinding object. SinkBinding resolves a sink as a URI, sets the URI in the environment variable K_SINK , and adds the URI to a subject using K_SINK . If the URI changes, SinkBinding updates the value of K_SINK . In the following examples, the sink is a Knative Service and the subject is a CronJob. If you have an existing subject and sink, you can replace the examples with your own values.","title":"Create a SinkBinding"},{"location":"eventing/custom-event-source/sinkbinding/create-a-sinkbinding/#before-you-begin","text":"Before you can create a SinkBinding object: You must have Knative Eventing installed on your cluster. Optional: If you want to use kn commands with SinkBinding, install the kn CLI.","title":"Before you begin"},{"location":"eventing/custom-event-source/sinkbinding/create-a-sinkbinding/#optional-choose-sinkbinding-namespace-selection-behavior","text":"The SinkBinding object operates in one of two modes: exclusion or inclusion . The default mode is exclusion . In exclusion mode, SinkBinding behavior is enabled for the namespace by default. To disallow a namespace from being evaluated for mutation you must exclude it using the label bindings.knative.dev/exclude: true . In inclusion mode, SinkBinding behavior is not enabled for the namespace. Before a namespace can be evaluated for mutation, you must explicitly include it using the label bindings.knative.dev/include: true . To set the SinkBinding object to inclusion mode: Change the value of SINK_BINDING_SELECTION_MODE from exclusion to inclusion by running: kubectl -n knative-eventing set env deployments eventing-webhook --containers = \"eventing-webhook\" SINK_BINDING_SELECTION_MODE = inclusion To verify that SINK_BINDING_SELECTION_MODE is set as desired, run: kubectl -n knative-eventing set env deployments eventing-webhook --containers = \"eventing-webhook\" --list | grep SINK_BINDING","title":"Optional: Choose SinkBinding namespace selection behavior"},{"location":"eventing/custom-event-source/sinkbinding/create-a-sinkbinding/#create-a-namespace","text":"If you do not have an existing namespace, create a namespace for the SinkBinding object: kubectl create namespace <namespace> Where <namespace> is the namespace that you want your SinkBinding to use. For example, sinkbinding-example . Note If you have selected inclusion mode, you must add the bindings.knative.dev/include: true label to the namespace to enable SinkBinding behavior.","title":"Create a namespace"},{"location":"eventing/custom-event-source/sinkbinding/create-a-sinkbinding/#create-a-sink","text":"The sink can be any addressable Kubernetes object that can receive events. If you do not have an existing sink that you want to connect to the SinkBinding object, create a Knative service. Note To create a Knative service you must have Knative Serving installed on your cluster. kn YAML Create a Knative service by running: kn service create <app-name> --image <image-url> Where: <app-name> is the name of the application. <image-url> is the URL of the image container. For example: $ kn service create event-display --image gcr.io/knative-releases/knative.dev/eventing/cmd/event_display Create a YAML file for the Knative service using the following template: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : <app-name> spec : template : spec : containers : - image : <image-url> Where: <app-name> is the name of the application. For example, event-display . <image-url> is the URL of the image container. For example, gcr.io/knative-releases/knative.dev/eventing/cmd/event_display . Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"Create a sink"},{"location":"eventing/custom-event-source/sinkbinding/create-a-sinkbinding/#create-a-subject","text":"The subject must be a PodSpecable resource. You can use any PodSpecable resource in your cluster, for example: Deployment Job DaemonSet StatefulSet Service.serving.knative.dev If you do not have an existing PodSpecable subject that you want to use, you can use the following sample to create a CronJob object as the subject. The following CronJob makes a single cloud event that targets K_SINK and adds any extra overrides given by CE_OVERRIDES . Create a YAML file for the CronJob using the following example: apiVersion : batch/v1beta1 kind : CronJob metadata : name : heartbeat-cron spec : # Run every minute schedule : \"*/1 * * * *\" jobTemplate : metadata : labels : app : heartbeat-cron spec : template : spec : restartPolicy : Never containers : - name : single-heartbeat image : gcr.io/knative-nightly/knative.dev/eventing/cmd/heartbeats args : - --period=1 env : - name : ONE_SHOT value : \"true\" - name : POD_NAME valueFrom : fieldRef : fieldPath : metadata.name - name : POD_NAMESPACE valueFrom : fieldRef : fieldPath : metadata.namespace Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"Create a subject"},{"location":"eventing/custom-event-source/sinkbinding/create-a-sinkbinding/#create-a-sinkbinding-object","text":"Create a SinkBinding object that directs events from your subject to the sink. kn YAML Create a SinkBinding object by running: kn source binding create <name> \\ --namespace <namespace> \\ --subject \"<subject>\" \\ --sink <sink> \\ --ce-override \"<cloudevent-overrides>\" Where: <name> is the name of the SinkBinding object you want to create. <namespace> is the namespace you created for your SinkBinding to use. <subject> is the subject to connect. Examples: Job:batch/v1:app=heartbeat-cron matches all jobs in namespace with label app=heartbeat-cron . Deployment:apps/v1:myapp matches a deployment called myapp in the namespace. Service:serving.knative.dev/v1:hello matches the service called hello . <sink> is the sink to connect. For example http://event-display.svc.cluster.local . Optional: <cloudevent-overrides> in the form key=value . Cloud Event overrides control the output format and modifications of the event sent to the sink and are applied before sending the event. You can provide this flag multiple times. For a list of available options, see the Knative client documentation . For example: $ kn source binding create bind-heartbeat \\ --namespace sinkbinding-example \\ --subject \"Job:batch/v1:app=heartbeat-cron\" \\ --sink http://event-display.svc.cluster.local \\ --ce-override \"sink=bound\" Create a YAML file for the SinkBinding object using the following template: apiVersion : sources.knative.dev/v1 kind : SinkBinding metadata : name : <name> spec : subject : apiVersion : <api-version> kind : <kind> selector : matchLabels : <label-key> : <label-value> sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : <sink> Where: <name> is the name of the SinkBinding object you want to create. For example, bind-heartbeat . <api-version> is the API version of the subject. For example batch/v1 . <kind> is the Kind of your subject. For example Job . <label-key>: <label-value> is a map of key-value pairs to select subjects that have a matching label. For example, app: heartbeat-cron selects any subject with the label app=heartbeat-cron . <sink> is the sink to connect. For example event-display . For more information about the fields you can configure for the SinkBinding object, see Sink Binding Reference . Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"Create a SinkBinding object"},{"location":"eventing/custom-event-source/sinkbinding/create-a-sinkbinding/#verify-the-sinkbinding-object","text":"Verify that a message was sent to the Knative eventing system by looking at the service logs for your sink: kubectl logs -l <sink> -c <container> --since = 10m Where: <sink> is the name of your sink. <container> is the name of the container your sink is running in. For example: $ kubectl logs -l serving.knative.dev/service = event-display -c user-container --since = 10m From the output, observe the lines showing the request headers and body of the event message, sent by the source to the display function. For example: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.eventing.samples.heartbeat source: https://knative.dev/eventing-contrib/cmd/heartbeats/#default/heartbeat-cron-1582120020-75qrz id: 5f4122be-ac6f-4349-a94f-4bfc6eb3f687 time: 2020 -02-19T13:47:10.41428688Z datacontenttype: application/json Extensions, beats: true heart: yes the: 42 Data, { \"id\" : 1 , \"label\" : \"\" }","title":"Verify the SinkBinding object"},{"location":"eventing/custom-event-source/sinkbinding/create-a-sinkbinding/#delete-a-sinkbinding","text":"To delete the SinkBinding object and all of the related resources in the namespace, delete the namespace by running: kubectl delete namespace <namespace> Where <namespace> is the name of the namespace that contains the SinkBinding object.","title":"Delete a SinkBinding"},{"location":"eventing/custom-event-source/sinkbinding/reference/","text":"SinkBinding reference \u00b6 This topic provides reference information about the configurable parameters for SinkBinding objects. Supported parameters \u00b6 A SinkBinding resource supports the following parameters: Field Description Required or optional apiVersion Specifies the API version, for example sources.knative.dev/v1 . Required kind Identifies this resource object as a SinkBinding object. Required metadata Specifies metadata that uniquely identifies the SinkBinding object. For example, a name . Required spec Specifies the configuration information for this SinkBinding object. Required spec.sink A reference to an object that resolves to a URI to use as the sink. Required spec.subject A reference to the resources for which the \"runtime contract\" is augmented by Binding implementations. Required spec.ceOverrides Defines overrides to control the output format and modifications to the event sent to the sink. Optional Subject parameter \u00b6 The Subject parameter references the resources for which the \"runtime contract\" is augmented by Binding implementations. A subject definition supports the following fields: Field Description Required or optional apiVersion API version of the referent. Required kind Kind of the referent. Required namespace Namespace of the referent. If omitted, this defaults to the object holding it. Optional name Name of the referent. Do not use if you configure selector . selector Selector of the referents. Do not use if you configure name . selector.matchExpressions A list of label selector requirements. The requirements are ANDed. Use one of matchExpressions or matchLabels selector.matchExpressions.key The label key that the selector applies to. Required if using matchExpressions selector.matchExpressions.operator Represents a key's relationship to a set of values. Valid operators are In , NotIn , Exists and DoesNotExist . Required if using matchExpressions selector.matchExpressions.values An array of string values. If operator is In or NotIn , the values array must be non-empty. If operator is Exists or DoesNotExist , the values array must be empty. This array is replaced during a strategic merge patch. Required if using matchExpressions selector.matchLabels A map of key-value pairs. Each key-value pair in the matchLabels map is equivalent to an element of matchExpressions , where the key field is matchLabels.<key> , the operator is In , and the values array contains only \"matchLabels. \". The requirements are ANDed. Use one of matchExpressions or matchLabels Subject parameter examples \u00b6 Given the following YAML, the Deployment named mysubject in the default namespace is selected: apiVersion : sources.knative.dev/v1 kind : SinkBinding metadata : name : bind-heartbeat spec : subject : apiVersion : apps/v1 kind : Deployment namespace : default name : mysubject ... Given the following YAML, any Job with the label working=example in the default namespace is selected: apiVersion : sources.knative.dev/v1 kind : SinkBinding metadata : name : bind-heartbeat spec : subject : apiVersion : batch/v1beta1 kind : Job namespace : default selector : matchLabels : working : example ... Given the following YAML, any Pod with the label working=example OR working=sample in the default namespace is selected: apiVersion : sources.knative.dev/v1 kind : SinkBinding metadata : name : bind-heartbeat spec : subject : apiVersion : v1 kind : Pod namespace : default selector : - matchExpression : key : working operator : In values : - example - sample ... CloudEvent Overrides \u00b6 CloudEvent Overrides defines overrides to control the output format and modifications of the event sent to the sink. A ceOverrides definition supports the following fields: Field Description Required or optional extensions Specifies which attributes are added or overridden on the outbound event. Each extensions key-value pair is set independently on the event as an attribute extension. Optional Note Only valid CloudEvent attribute names are allowed as extensions. You cannot set the spec defined attributes from the extensions override configuration. For example, you can not modify the type attribute. CloudEvent Overrides example \u00b6 apiVersion : sources.knative.dev/v1 kind : SinkBinding metadata : name : bind-heartbeat spec : ... ceOverrides : extensions : extra : this is an extra attribute additional : 42 Contract This results in the K_CE_OVERRIDES environment variable being set on the subject as follows: { \"extensions\" : { \"extra\" : \"this is an extra attribute\" , \"additional\" : \"42\" } }","title":"SinkBinding reference"},{"location":"eventing/custom-event-source/sinkbinding/reference/#sinkbinding-reference","text":"This topic provides reference information about the configurable parameters for SinkBinding objects.","title":"SinkBinding reference"},{"location":"eventing/custom-event-source/sinkbinding/reference/#supported-parameters","text":"A SinkBinding resource supports the following parameters: Field Description Required or optional apiVersion Specifies the API version, for example sources.knative.dev/v1 . Required kind Identifies this resource object as a SinkBinding object. Required metadata Specifies metadata that uniquely identifies the SinkBinding object. For example, a name . Required spec Specifies the configuration information for this SinkBinding object. Required spec.sink A reference to an object that resolves to a URI to use as the sink. Required spec.subject A reference to the resources for which the \"runtime contract\" is augmented by Binding implementations. Required spec.ceOverrides Defines overrides to control the output format and modifications to the event sent to the sink. Optional","title":"Supported parameters"},{"location":"eventing/custom-event-source/sinkbinding/reference/#subject-parameter","text":"The Subject parameter references the resources for which the \"runtime contract\" is augmented by Binding implementations. A subject definition supports the following fields: Field Description Required or optional apiVersion API version of the referent. Required kind Kind of the referent. Required namespace Namespace of the referent. If omitted, this defaults to the object holding it. Optional name Name of the referent. Do not use if you configure selector . selector Selector of the referents. Do not use if you configure name . selector.matchExpressions A list of label selector requirements. The requirements are ANDed. Use one of matchExpressions or matchLabels selector.matchExpressions.key The label key that the selector applies to. Required if using matchExpressions selector.matchExpressions.operator Represents a key's relationship to a set of values. Valid operators are In , NotIn , Exists and DoesNotExist . Required if using matchExpressions selector.matchExpressions.values An array of string values. If operator is In or NotIn , the values array must be non-empty. If operator is Exists or DoesNotExist , the values array must be empty. This array is replaced during a strategic merge patch. Required if using matchExpressions selector.matchLabels A map of key-value pairs. Each key-value pair in the matchLabels map is equivalent to an element of matchExpressions , where the key field is matchLabels.<key> , the operator is In , and the values array contains only \"matchLabels. \". The requirements are ANDed. Use one of matchExpressions or matchLabels","title":"Subject parameter"},{"location":"eventing/custom-event-source/sinkbinding/reference/#subject-parameter-examples","text":"Given the following YAML, the Deployment named mysubject in the default namespace is selected: apiVersion : sources.knative.dev/v1 kind : SinkBinding metadata : name : bind-heartbeat spec : subject : apiVersion : apps/v1 kind : Deployment namespace : default name : mysubject ... Given the following YAML, any Job with the label working=example in the default namespace is selected: apiVersion : sources.knative.dev/v1 kind : SinkBinding metadata : name : bind-heartbeat spec : subject : apiVersion : batch/v1beta1 kind : Job namespace : default selector : matchLabels : working : example ... Given the following YAML, any Pod with the label working=example OR working=sample in the default namespace is selected: apiVersion : sources.knative.dev/v1 kind : SinkBinding metadata : name : bind-heartbeat spec : subject : apiVersion : v1 kind : Pod namespace : default selector : - matchExpression : key : working operator : In values : - example - sample ...","title":"Subject parameter examples"},{"location":"eventing/custom-event-source/sinkbinding/reference/#cloudevent-overrides","text":"CloudEvent Overrides defines overrides to control the output format and modifications of the event sent to the sink. A ceOverrides definition supports the following fields: Field Description Required or optional extensions Specifies which attributes are added or overridden on the outbound event. Each extensions key-value pair is set independently on the event as an attribute extension. Optional Note Only valid CloudEvent attribute names are allowed as extensions. You cannot set the spec defined attributes from the extensions override configuration. For example, you can not modify the type attribute.","title":"CloudEvent Overrides"},{"location":"eventing/custom-event-source/sinkbinding/reference/#cloudevent-overrides-example","text":"apiVersion : sources.knative.dev/v1 kind : SinkBinding metadata : name : bind-heartbeat spec : ... ceOverrides : extensions : extra : this is an extra attribute additional : 42 Contract This results in the K_CE_OVERRIDES environment variable being set on the subject as follows: { \"extensions\" : { \"extra\" : \"this is an extra attribute\" , \"additional\" : \"42\" } }","title":"CloudEvent Overrides example"},{"location":"eventing/experimental-features/","text":"Eventing experimental features \u00b6 To keep Knative innovative, the maintainers of this project have developed an experimental features process that allows new, experimental features to be delivered and tested by users without affecting the stability of the core project. Warning Experimental features are unstable and may cause issues in your Knative setup or even your cluster setup. These features should be used with caution, and should never be tested on a production environment. For more information about quality guarantees for features at different stages of development, see the Feature stage definition documentation. This document explains how to enable experimental features and which ones are available today. Before you begin \u00b6 You must have a Knative cluster running with Knative Eventing installed. Experimental features configuration \u00b6 When you install Knative Eventing, the config-features ConfigMap is added to your cluster in the knative-eventing namespace. To enable a feature, you must add it to the config-features ConfigMap under the data spec, and set the value for the feature to enabled . For example, to enable a feature called new-cool-feature , you would add the following ConfigMap entry: apiVersion : v1 kind : ConfigMap metadata : name : config-features namespace : knative-eventing labels : eventing.knative.dev/release : devel knative.dev/config-propagation : original knative.dev/config-category : eventing data : new-cool-feature : enabled To disable it, you can either remove the flag or set it to disabled : apiVersion : v1 kind : ConfigMap metadata : name : config-features namespace : knative-eventing labels : eventing.knative.dev/release : devel knative.dev/config-propagation : original knative.dev/config-category : eventing data : new-cool-feature : disabled Available experimental features \u00b6 The following table gives an overview of the available experimental features in Knative Eventing: Feature Flag Description Maturity DeliverySpec.RetryAfterMax field delivery-retryafter Specify a maximum retry duration that overrides HTTP Retry-After headers when calculating backoff times for retrying 429 and 503 responses. Alpha, disabled by default DeliverySpec.Timeout field delivery-timeout When using the delivery spec to configure event delivery parameters, you can use the timeout field to specify the timeout for each sent HTTP request. Alpha, disabled by default KReference.Group field kreference-group Specify the API group of KReference resources without the API version. Alpha, disabled by default Knative reference mapping kreference-mapping Provide mappings from a Knative reference to a templated URI. Alpha, disabled by default New trigger filters new-trigger-filters Enables a new Trigger filters field that supports a set of powerful filter expressions. Alpha, disabled by default Strict Subscriber strict-subscriber Invalidates Subscriptions if the field spec.subscriber is not defined. Alpha, disabled by default","title":"About Eventing experimental features"},{"location":"eventing/experimental-features/#eventing-experimental-features","text":"To keep Knative innovative, the maintainers of this project have developed an experimental features process that allows new, experimental features to be delivered and tested by users without affecting the stability of the core project. Warning Experimental features are unstable and may cause issues in your Knative setup or even your cluster setup. These features should be used with caution, and should never be tested on a production environment. For more information about quality guarantees for features at different stages of development, see the Feature stage definition documentation. This document explains how to enable experimental features and which ones are available today.","title":"Eventing experimental features"},{"location":"eventing/experimental-features/#before-you-begin","text":"You must have a Knative cluster running with Knative Eventing installed.","title":"Before you begin"},{"location":"eventing/experimental-features/#experimental-features-configuration","text":"When you install Knative Eventing, the config-features ConfigMap is added to your cluster in the knative-eventing namespace. To enable a feature, you must add it to the config-features ConfigMap under the data spec, and set the value for the feature to enabled . For example, to enable a feature called new-cool-feature , you would add the following ConfigMap entry: apiVersion : v1 kind : ConfigMap metadata : name : config-features namespace : knative-eventing labels : eventing.knative.dev/release : devel knative.dev/config-propagation : original knative.dev/config-category : eventing data : new-cool-feature : enabled To disable it, you can either remove the flag or set it to disabled : apiVersion : v1 kind : ConfigMap metadata : name : config-features namespace : knative-eventing labels : eventing.knative.dev/release : devel knative.dev/config-propagation : original knative.dev/config-category : eventing data : new-cool-feature : disabled","title":"Experimental features configuration"},{"location":"eventing/experimental-features/#available-experimental-features","text":"The following table gives an overview of the available experimental features in Knative Eventing: Feature Flag Description Maturity DeliverySpec.RetryAfterMax field delivery-retryafter Specify a maximum retry duration that overrides HTTP Retry-After headers when calculating backoff times for retrying 429 and 503 responses. Alpha, disabled by default DeliverySpec.Timeout field delivery-timeout When using the delivery spec to configure event delivery parameters, you can use the timeout field to specify the timeout for each sent HTTP request. Alpha, disabled by default KReference.Group field kreference-group Specify the API group of KReference resources without the API version. Alpha, disabled by default Knative reference mapping kreference-mapping Provide mappings from a Knative reference to a templated URI. Alpha, disabled by default New trigger filters new-trigger-filters Enables a new Trigger filters field that supports a set of powerful filter expressions. Alpha, disabled by default Strict Subscriber strict-subscriber Invalidates Subscriptions if the field spec.subscriber is not defined. Alpha, disabled by default","title":"Available experimental features"},{"location":"eventing/experimental-features/delivery-retryafter/","text":"DeliverySpec.RetryAfterMax field \u00b6 Flag name : delivery-retryafter Stage : Alpha, disabled by default Tracking issue : #5811 Persona : Developer When using the delivery spec to configure event delivery parameters, you can use the retryAfterMax field to specify how HTTP Retry-After headers are handled when calculating backoff times for retrying 429 and 503 responses. You can specify a delivery spec for Channels, Subscriptions, Brokers, Triggers, and any other resource spec that accepts the delivery field. The retryAfterMax field only takes effect if you configure the delivery spec to perform retries, and only pertains to retry attempts on 429 and 503 response codes. The field provides an override to prevent large Retry-After durations from impacting throughput, and must be specified using the ISO 8601 format. The largest of the normal backoff duration and the Retry-After header value will be used for the subsequent retry attempt. Specifying a \"zero\" value of PT0S effectively disables Retry-After support. Prior to this experimental feature, Knative Eventing implementations have not supported Retry-After headers, and this is an attempt to provide a path for standardizing that support. To begin, the feature is opt-in , but the final state will be opt-out as follows: Feature Stage Feature Flag retryAfterMax Field Absent retryAfterMax Field Present Alpha / Beta Disabled Accepted by Webhook Validation & Retry-After headers NOT enforced Rejected by WebHook Validation Alpha / Beta Enabled Accepted by Webhook Validation & Retry-After headers NOT enforced Accepted by Webhook Validation & Retry-After headers enforced if max override > 0 Stable / GA n/a Retry-After headers enforced without max override Retry-After headers enforced if max override > 0 The following example shows a Subscription that retries sending an event three times, and respects Retry-After headers while imposing a maximum backoff of 120 seconds: apiVersion : messaging.knative.dev/v1 kind : Subscription metadata : name : example-subscription namespace : example-namespace spec : subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : example-sink delivery : backoffDelay : PT2S backoffPolicy : linear retry : 3 retryAfterMax : PT120S Note While the experimental feature flag enforces all DeliverySpec usage of the retryAfterMax field through Webhook validation, it does not guarantee all implementations, such as Channels or Sources, actually implement support for the field. The shared HTTPMessageSender.SendWithRetries() logic has been enhanced to support this feature, and all implementations using it to perform retries will automatically benefit. Sandbox implementations not based on this shared library, for example RabbitMQ or Google Pub/Sub, would require additional development effort to respect the retryAfterMax field.","title":"DeliverySpec.RetryAfterMax field"},{"location":"eventing/experimental-features/delivery-retryafter/#deliveryspecretryaftermax-field","text":"Flag name : delivery-retryafter Stage : Alpha, disabled by default Tracking issue : #5811 Persona : Developer When using the delivery spec to configure event delivery parameters, you can use the retryAfterMax field to specify how HTTP Retry-After headers are handled when calculating backoff times for retrying 429 and 503 responses. You can specify a delivery spec for Channels, Subscriptions, Brokers, Triggers, and any other resource spec that accepts the delivery field. The retryAfterMax field only takes effect if you configure the delivery spec to perform retries, and only pertains to retry attempts on 429 and 503 response codes. The field provides an override to prevent large Retry-After durations from impacting throughput, and must be specified using the ISO 8601 format. The largest of the normal backoff duration and the Retry-After header value will be used for the subsequent retry attempt. Specifying a \"zero\" value of PT0S effectively disables Retry-After support. Prior to this experimental feature, Knative Eventing implementations have not supported Retry-After headers, and this is an attempt to provide a path for standardizing that support. To begin, the feature is opt-in , but the final state will be opt-out as follows: Feature Stage Feature Flag retryAfterMax Field Absent retryAfterMax Field Present Alpha / Beta Disabled Accepted by Webhook Validation & Retry-After headers NOT enforced Rejected by WebHook Validation Alpha / Beta Enabled Accepted by Webhook Validation & Retry-After headers NOT enforced Accepted by Webhook Validation & Retry-After headers enforced if max override > 0 Stable / GA n/a Retry-After headers enforced without max override Retry-After headers enforced if max override > 0 The following example shows a Subscription that retries sending an event three times, and respects Retry-After headers while imposing a maximum backoff of 120 seconds: apiVersion : messaging.knative.dev/v1 kind : Subscription metadata : name : example-subscription namespace : example-namespace spec : subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : example-sink delivery : backoffDelay : PT2S backoffPolicy : linear retry : 3 retryAfterMax : PT120S Note While the experimental feature flag enforces all DeliverySpec usage of the retryAfterMax field through Webhook validation, it does not guarantee all implementations, such as Channels or Sources, actually implement support for the field. The shared HTTPMessageSender.SendWithRetries() logic has been enhanced to support this feature, and all implementations using it to perform retries will automatically benefit. Sandbox implementations not based on this shared library, for example RabbitMQ or Google Pub/Sub, would require additional development effort to respect the retryAfterMax field.","title":"DeliverySpec.RetryAfterMax field"},{"location":"eventing/experimental-features/delivery-timeout/","text":"DeliverySpec.Timeout field \u00b6 Flag name : delivery-timeout Stage : Alpha, disabled by default Tracking issue : #5148 Persona : Developer When using the delivery spec to configure event delivery parameters, you can use timeout field to specify the timeout for each sent HTTP request. The duration of the timeout parameter is specified using the ISO 8601 format. The following example shows a Subscription that retries sending an event 3 times, and on each retry the request timeout is 5 seconds: apiVersion : messaging.knative.dev/v1 kind : Subscription metadata : name : example-subscription namespace : example-namespace spec : subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : example-sink delivery : backoffDelay : PT2S backoffPolicy : linear retry : 3 timeout : PT5S You can specify a delivery spec for Channels, Subscriptions, Brokers, Triggers, and any other resource spec that accepts the delivery field.","title":"DeliverySpec.Timeout field"},{"location":"eventing/experimental-features/delivery-timeout/#deliveryspectimeout-field","text":"Flag name : delivery-timeout Stage : Alpha, disabled by default Tracking issue : #5148 Persona : Developer When using the delivery spec to configure event delivery parameters, you can use timeout field to specify the timeout for each sent HTTP request. The duration of the timeout parameter is specified using the ISO 8601 format. The following example shows a Subscription that retries sending an event 3 times, and on each retry the request timeout is 5 seconds: apiVersion : messaging.knative.dev/v1 kind : Subscription metadata : name : example-subscription namespace : example-namespace spec : subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : example-sink delivery : backoffDelay : PT2S backoffPolicy : linear retry : 3 timeout : PT5S You can specify a delivery spec for Channels, Subscriptions, Brokers, Triggers, and any other resource spec that accepts the delivery field.","title":"DeliverySpec.Timeout field"},{"location":"eventing/experimental-features/kreference-group/","text":"KReference.Group field \u00b6 Flag name : kreference-group Stage : Alpha, disabled by default Tracking issue : #5086 Persona : Developer When using the KReference type to refer to another Knative resource, you can just specify the API group of the resource, instead of the full APIVersion . For example, in order to refer to an InMemoryChannel , instead of the following spec: apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel name : my-channel You can use the following: group : messaging.knative.dev kind : InMemoryChannel name : my-channel With this feature you can allow Knative to resolve the full APIVersion and further upgrades, deprecations and removals of the referred CRD without affecting existing resources. Note At the moment this feature is implemented only for Subscription.Spec.Subscriber.Ref and Subscription.Spec.Channel .","title":"KReference.Group field"},{"location":"eventing/experimental-features/kreference-group/#kreferencegroup-field","text":"Flag name : kreference-group Stage : Alpha, disabled by default Tracking issue : #5086 Persona : Developer When using the KReference type to refer to another Knative resource, you can just specify the API group of the resource, instead of the full APIVersion . For example, in order to refer to an InMemoryChannel , instead of the following spec: apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel name : my-channel You can use the following: group : messaging.knative.dev kind : InMemoryChannel name : my-channel With this feature you can allow Knative to resolve the full APIVersion and further upgrades, deprecations and removals of the referred CRD without affecting existing resources. Note At the moment this feature is implemented only for Subscription.Spec.Subscriber.Ref and Subscription.Spec.Channel .","title":"KReference.Group field"},{"location":"eventing/experimental-features/kreference-mapping/","text":"Knative reference mapping \u00b6 Flag name : kreference-mapping Stage : Alpha, disabled by default Tracking issue : #5593 Persona : Administrator, Developer When enabled, this feature allows you to provide mappings from a Knative reference to a templated URI. Note Currently only PingSource supports this experimental feature. For example, you can directly reference non-addressable resources anywhere that Knative Eventing accepts a reference, such as for a PingSource sink, or a Trigger subscriber. Mappings are defined by a cluster administrator in the config-reference-mapping ConfigMap. The following example maps JobDefinition to a Job runner service: apiVersion : v1 kind : ConfigMap metadata : name : config-kreference-mapping namespace : knative-eventing data : JobDefinition.v1.mygroup : \"https://jobrunner.{{ .SystemNamespace }}.svc.cluster.local/{{ .Name }}\" The key must be of the form <Kind>.<version>.<group> . The value must resolved to a valid URI. Currently, the following template data are supported: Name: The name of the referenced object Namespace: The namespace of the referenced object UID: The UID of the referenced object SystemNamespace: The namespace of where Knative Eventing is installed Given the above mapping, the following example shows how you can directly reference JobDefinition objects in a PingSource: apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : trigger-job-every-minute spec : schedule : \"*/1 * * * *\" sink : ref : apiVersion : mygroup/v1 kind : JobDefinition name : ajob","title":"Knative reference mapping"},{"location":"eventing/experimental-features/kreference-mapping/#knative-reference-mapping","text":"Flag name : kreference-mapping Stage : Alpha, disabled by default Tracking issue : #5593 Persona : Administrator, Developer When enabled, this feature allows you to provide mappings from a Knative reference to a templated URI. Note Currently only PingSource supports this experimental feature. For example, you can directly reference non-addressable resources anywhere that Knative Eventing accepts a reference, such as for a PingSource sink, or a Trigger subscriber. Mappings are defined by a cluster administrator in the config-reference-mapping ConfigMap. The following example maps JobDefinition to a Job runner service: apiVersion : v1 kind : ConfigMap metadata : name : config-kreference-mapping namespace : knative-eventing data : JobDefinition.v1.mygroup : \"https://jobrunner.{{ .SystemNamespace }}.svc.cluster.local/{{ .Name }}\" The key must be of the form <Kind>.<version>.<group> . The value must resolved to a valid URI. Currently, the following template data are supported: Name: The name of the referenced object Namespace: The namespace of the referenced object UID: The UID of the referenced object SystemNamespace: The namespace of where Knative Eventing is installed Given the above mapping, the following example shows how you can directly reference JobDefinition objects in a PingSource: apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : trigger-job-every-minute spec : schedule : \"*/1 * * * *\" sink : ref : apiVersion : mygroup/v1 kind : JobDefinition name : ajob","title":"Knative reference mapping"},{"location":"eventing/experimental-features/new-trigger-filters/","text":"New trigger filters \u00b6 Flag name : new-trigger-filters Stage : Alpha, disabled by default Tracking issue : #5204 Overview \u00b6 This experimental feature enables a new filters field in Triggers that conforms to the filters API field defined in the CloudEvents Subscriptions API . It allows users to specify a set of powerful filter expressions, where each expression evaluates to either true or false for each event. The following example shows a Trigger using the new filters field: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger spec : broker : default filters : - cesql : \"source LIKE '%commerce%' AND type IN ('order.created', 'order.updated', 'order.canceled')\" subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : my-service About the filters field \u00b6 An array of filter expressions that evaluates to true or false. If any filter expression in the array evaluates to false, the event will not be sent to the subscriber . Each filter expression follows a dialect that defines the type of filter and the set of additional properties that are allowed within the filter expression. Supported filter dialects \u00b6 The filters field supports the following dialects: exact \u00b6 CloudEvent attribute String value must exactly match the specified String value. Matching is case-sensitive. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : ... spec : ... filters : - exact : type : com.github.push prefix \u00b6 CloudEvent attribute String value must start with the specified String value. Matching is case-sensitive. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : ... spec : ... filters : - prefix : type : com.github. suffix \u00b6 CloudEvent attribute String value must end with the specified String value. Matching is case-sensitive. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : ... spec : ... filters : - suffix : type : .created all \u00b6 All nested filter expessions must evaluate to true. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : ... spec : ... filters : - all : - exact : type : com.github.push - exact : subject : https://github.com/cloudevents/spec any \u00b6 At least one nested filter expession must evaluate to true. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : ... spec : ... filters : - any : - exact : type : com.github.push - exact : subject : https://github.com/cloudevents/spec not \u00b6 The nested expression evaluated must evaluate to false. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : ... spec : ... filters : - not : - exact : type : com.github.push cesql \u00b6 The provided CloudEvents SQL Expression must evaluate to true. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : ... spec : ... filters : - cesql : \"source LIKE '%commerce%' AND type IN ('order.created', 'order.updated', 'order.canceled')\" Conflict with the current filter field \u00b6 The current filter field will continue to be supported. However, if you enable this feature and an object includes both filter and filters , the new filters field overrides the filter field. This allows you to try the new filters field without compromising existing filters, and you can introduce it to existing Trigger objects gradually. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger spec : broker : default # Current filter field. Will be ignored. filter : attributes : type : dev.knative.foo.bar myextension : my-extension-value # Enhanced filters field. This will override the old filter field. filters : - cesql : \"type == 'dev.knative.foo.bar' AND myextension == 'my-extension-value'\" subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : my-service FAQ \u00b6 Why add yet another field? Why not make the current filter field more robust? \u00b6 The reason is twofold. First, at the time of developing Trigger APIs, there was no Subscriptions API in CloudEvents Project, so it makes sense to experiment with an API that is closer to the Subscriptions API. Second, we still want to support users workload with the old filter field, and give them the possibility to transition to the new filters field. Why filters and not another name that wouldn't conflict with the filter field? \u00b6 We considered other names, such as cefilters , subscriptionsAPIFilters , or enhancedFilters , but we decided that this would be a step further from aligning with the Subscriptions API. Instead, we decided it is a good opportunity to conform with the Subscriptions API, at least at the field name level, and to leverage the safety of this being an experimental feature.","title":"New Trigger Filters"},{"location":"eventing/experimental-features/new-trigger-filters/#new-trigger-filters","text":"Flag name : new-trigger-filters Stage : Alpha, disabled by default Tracking issue : #5204","title":"New trigger filters"},{"location":"eventing/experimental-features/new-trigger-filters/#overview","text":"This experimental feature enables a new filters field in Triggers that conforms to the filters API field defined in the CloudEvents Subscriptions API . It allows users to specify a set of powerful filter expressions, where each expression evaluates to either true or false for each event. The following example shows a Trigger using the new filters field: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger spec : broker : default filters : - cesql : \"source LIKE '%commerce%' AND type IN ('order.created', 'order.updated', 'order.canceled')\" subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : my-service","title":"Overview"},{"location":"eventing/experimental-features/new-trigger-filters/#about-the-filters-field","text":"An array of filter expressions that evaluates to true or false. If any filter expression in the array evaluates to false, the event will not be sent to the subscriber . Each filter expression follows a dialect that defines the type of filter and the set of additional properties that are allowed within the filter expression.","title":"About the filters field"},{"location":"eventing/experimental-features/new-trigger-filters/#supported-filter-dialects","text":"The filters field supports the following dialects:","title":"Supported filter dialects"},{"location":"eventing/experimental-features/new-trigger-filters/#exact","text":"CloudEvent attribute String value must exactly match the specified String value. Matching is case-sensitive. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : ... spec : ... filters : - exact : type : com.github.push","title":"exact"},{"location":"eventing/experimental-features/new-trigger-filters/#prefix","text":"CloudEvent attribute String value must start with the specified String value. Matching is case-sensitive. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : ... spec : ... filters : - prefix : type : com.github.","title":"prefix"},{"location":"eventing/experimental-features/new-trigger-filters/#suffix","text":"CloudEvent attribute String value must end with the specified String value. Matching is case-sensitive. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : ... spec : ... filters : - suffix : type : .created","title":"suffix"},{"location":"eventing/experimental-features/new-trigger-filters/#all","text":"All nested filter expessions must evaluate to true. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : ... spec : ... filters : - all : - exact : type : com.github.push - exact : subject : https://github.com/cloudevents/spec","title":"all"},{"location":"eventing/experimental-features/new-trigger-filters/#any","text":"At least one nested filter expession must evaluate to true. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : ... spec : ... filters : - any : - exact : type : com.github.push - exact : subject : https://github.com/cloudevents/spec","title":"any"},{"location":"eventing/experimental-features/new-trigger-filters/#not","text":"The nested expression evaluated must evaluate to false. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : ... spec : ... filters : - not : - exact : type : com.github.push","title":"not"},{"location":"eventing/experimental-features/new-trigger-filters/#cesql","text":"The provided CloudEvents SQL Expression must evaluate to true. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : ... spec : ... filters : - cesql : \"source LIKE '%commerce%' AND type IN ('order.created', 'order.updated', 'order.canceled')\"","title":"cesql"},{"location":"eventing/experimental-features/new-trigger-filters/#conflict-with-the-current-filter-field","text":"The current filter field will continue to be supported. However, if you enable this feature and an object includes both filter and filters , the new filters field overrides the filter field. This allows you to try the new filters field without compromising existing filters, and you can introduce it to existing Trigger objects gradually. apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : my-service-trigger spec : broker : default # Current filter field. Will be ignored. filter : attributes : type : dev.knative.foo.bar myextension : my-extension-value # Enhanced filters field. This will override the old filter field. filters : - cesql : \"type == 'dev.knative.foo.bar' AND myextension == 'my-extension-value'\" subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : my-service","title":"Conflict with the current filter field"},{"location":"eventing/experimental-features/new-trigger-filters/#faq","text":"","title":"FAQ"},{"location":"eventing/experimental-features/new-trigger-filters/#why-add-yet-another-field-why-not-make-the-current-filter-field-more-robust","text":"The reason is twofold. First, at the time of developing Trigger APIs, there was no Subscriptions API in CloudEvents Project, so it makes sense to experiment with an API that is closer to the Subscriptions API. Second, we still want to support users workload with the old filter field, and give them the possibility to transition to the new filters field.","title":"Why add yet another field? Why not make the current filter field more robust?"},{"location":"eventing/experimental-features/new-trigger-filters/#why-filters-and-not-another-name-that-wouldnt-conflict-with-the-filter-field","text":"We considered other names, such as cefilters , subscriptionsAPIFilters , or enhancedFilters , but we decided that this would be a step further from aligning with the Subscriptions API. Instead, we decided it is a good opportunity to conform with the Subscriptions API, at least at the field name level, and to leverage the safety of this being an experimental feature.","title":"Why filters and not another name that wouldn't conflict with the filter field?"},{"location":"eventing/experimental-features/strict-subscriber/","text":"Strict Subscriber \u00b6 Flag name : strict-subscriber Stage : Alpha, disabled by default Tracking issue : #5762 When defining a Subscription, if the strict-subscriber flag is enabled, validation fails if the field spec.subscriber is not defined. This flag was implemented to follow the latest version of the Knative Eventing spec . For example, the following Subscription will fail validation if the strict-subscriber flag is enabled: apiVersion : messaging.knative.dev/v1 kind : Subscription metadata : name : example-subscription namespace : example-namespace spec : reply : ref : apiVersion : serving.knative.dev/v1 kind : Service name : example-reply With the flag disabled (default behavior) the Subscription can define either a subscriber or a reply field, and validation will succeed. This is the default behavior in Knative v0.26 and earlier.","title":"Strict Subscriber"},{"location":"eventing/experimental-features/strict-subscriber/#strict-subscriber","text":"Flag name : strict-subscriber Stage : Alpha, disabled by default Tracking issue : #5762 When defining a Subscription, if the strict-subscriber flag is enabled, validation fails if the field spec.subscriber is not defined. This flag was implemented to follow the latest version of the Knative Eventing spec . For example, the following Subscription will fail validation if the strict-subscriber flag is enabled: apiVersion : messaging.knative.dev/v1 kind : Subscription metadata : name : example-subscription namespace : example-namespace spec : reply : ref : apiVersion : serving.knative.dev/v1 kind : Service name : example-reply With the flag disabled (default behavior) the Subscription can define either a subscriber or a reply field, and validation will succeed. This is the default behavior in Knative v0.26 and earlier.","title":"Strict Subscriber"},{"location":"eventing/flows/","text":"Eventing Flows \u00b6 Knative Eventing provides a collection of custom resource definitions (CRDs) that you can use to define event flows: Sequence is for defining an in-order list of functions. Parallel is for defining a list of branches, each receiving the same CloudEvent.","title":"About flows"},{"location":"eventing/flows/#eventing-flows","text":"Knative Eventing provides a collection of custom resource definitions (CRDs) that you can use to define event flows: Sequence is for defining an in-order list of functions. Parallel is for defining a list of branches, each receiving the same CloudEvent.","title":"Eventing Flows"},{"location":"eventing/flows/parallel/","text":"Parallel \u00b6 Parallel CRD provides a way to easily define a list of branches, each receiving the same CloudEvent sent to the Parallel ingress channel. Typically, each branch consists of a filter function guarding the execution of the branch. Parallel creates Channel s and Subscription s under the hood. Usage \u00b6 Parallel Spec \u00b6 Parallel has three parts for the Spec: branches defines the list of filter and subscriber pairs, one per branch, and optionally a reply object. For each branch: (optional) the filter is evaluated and when it returns an event the subscriber is executed. Both filter and subscriber must be Addressable . the event returned by the subscriber is sent to the branch reply object. When the reply is empty, the event is sent to the spec.reply object. (optional) channelTemplate defines the Template which will be used to create Channel s. (optional) reply defines where the result of each branch is sent to when the branch does not have its own reply object. Parallel Status \u00b6 Parallel has three parts for the Status: conditions which details the overall status of the Parallel object ingressChannelStatus and branchesStatuses which convey the status of underlying Channel and Subscription resource that are created as part of this Parallel. address which is exposed so that Parallel can be used where Addressable can be used. Sending to this address will target the Channel which is fronting this Parallel (same as ingressChannelStatus ). Examples \u00b6 Learn how to use Parallel by following the code samples .","title":"Parallel"},{"location":"eventing/flows/parallel/#parallel","text":"Parallel CRD provides a way to easily define a list of branches, each receiving the same CloudEvent sent to the Parallel ingress channel. Typically, each branch consists of a filter function guarding the execution of the branch. Parallel creates Channel s and Subscription s under the hood.","title":"Parallel"},{"location":"eventing/flows/parallel/#usage","text":"","title":"Usage"},{"location":"eventing/flows/parallel/#parallel-spec","text":"Parallel has three parts for the Spec: branches defines the list of filter and subscriber pairs, one per branch, and optionally a reply object. For each branch: (optional) the filter is evaluated and when it returns an event the subscriber is executed. Both filter and subscriber must be Addressable . the event returned by the subscriber is sent to the branch reply object. When the reply is empty, the event is sent to the spec.reply object. (optional) channelTemplate defines the Template which will be used to create Channel s. (optional) reply defines where the result of each branch is sent to when the branch does not have its own reply object.","title":"Parallel Spec"},{"location":"eventing/flows/parallel/#parallel-status","text":"Parallel has three parts for the Status: conditions which details the overall status of the Parallel object ingressChannelStatus and branchesStatuses which convey the status of underlying Channel and Subscription resource that are created as part of this Parallel. address which is exposed so that Parallel can be used where Addressable can be used. Sending to this address will target the Channel which is fronting this Parallel (same as ingressChannelStatus ).","title":"Parallel Status"},{"location":"eventing/flows/parallel/#examples","text":"Learn how to use Parallel by following the code samples .","title":"Examples"},{"location":"eventing/flows/sequence/","text":"Sequence \u00b6 Sequence CRD provides a way to define an in-order list of functions that will be invoked. Each step can modify, filter or create a new kind of an event. Sequence creates Channel s and Subscription s under the hood. Info Sequence needs \"hairpin\" traffic. Please verify that your pod can reach itself via the service IP. If the \"hairpin\" traffic is not available, you can reach out to your cluster administrator since its a cluster level (typically CNI) setting. Usage \u00b6 Sequence Spec \u00b6 Sequence has three parts for the Spec: Steps which defines the in-order list of Subscriber s, aka, which functions are executed in the listed order. These are specified using the messaging.v1.SubscriberSpec just like you would when creating Subscription . Each step should be Addressable . ChannelTemplate defines the Template which will be used to create Channel s between the steps. Reply (Optional) Reference to where the results of the final step in the sequence are sent to. Sequence Status \u00b6 Sequence has four parts for the Status: Conditions which detail the overall Status of the Sequence object ChannelStatuses which convey the Status of underlying Channel resources that are created as part of this Sequence. It is an array and each Status corresponds to the Step number, so the first entry in the array is the Status of the Channel before the first Step. SubscriptionStatuses which convey the Status of underlying Subscription resources that are created as part of this Sequence. It is an array and each Status corresponds to the Step number, so the first entry in the array is the Subscription which is created to wire the first channel to the first step in the Steps array. AddressStatus which is exposed so that Sequence can be used where Addressable can be used. Sending to this address will target the Channel which is fronting the first Step in the Sequence. Examples \u00b6 For each of the following examples, you use a PingSource as the source of events. We also use a very simple transformer which performs very trivial transformation of the incoming events to demonstrate they have passed through each stage. Sequence with no reply \u00b6 For the first example, we'll use a 3 Step Sequence that is wired directly into the PingSource . Each of the steps simply tacks on \"- Handled by \", for example the first Step in the Sequence will take the incoming message and append \"- Handled by 0\" to the incoming message. See Sequence with no reply (terminal last Step) . Sequence with reply \u00b6 For the next example, we'll use the same 3 Step Sequence that is wired directly into the PingSource . Each of the steps simply tacks on \"- Handled by \", for example the first Step in the Sequence will take the incoming message and append \"- Handled by 0\" to the incoming message. The only difference is that we'll use the Subscriber.Spec.Reply field to wire the output of the last Step to an event display pod. See Sequence with reply (last Step produces output) . Chaining Sequences together \u00b6 For the next example, we'll use the same 3 Step Sequence that is wired directly into the PingSource . Each of the steps simply tacks on \"- Handled by \", for example the first Step in the Sequence will take the incoming message and append \"- Handled by 0\" to the incoming message. The only difference is that we'll use the Subscriber.Spec.Reply field to wire the output of the last Step to another Sequence that does the same message modifications as the first pipeline (with different steps however). See Chaining Sequences together . Using Sequence with Broker/Trigger model \u00b6 You can also create a Trigger which targets Sequence . This time we'll wire PingSource to send events to a Broker and then we'll have the Sequence emit the resulting Events back into the Broker so that the results of the Sequence can be observed by other Triggers. See Using Sequence with Broker/Trigger model .","title":"About Sequences"},{"location":"eventing/flows/sequence/#sequence","text":"Sequence CRD provides a way to define an in-order list of functions that will be invoked. Each step can modify, filter or create a new kind of an event. Sequence creates Channel s and Subscription s under the hood. Info Sequence needs \"hairpin\" traffic. Please verify that your pod can reach itself via the service IP. If the \"hairpin\" traffic is not available, you can reach out to your cluster administrator since its a cluster level (typically CNI) setting.","title":"Sequence"},{"location":"eventing/flows/sequence/#usage","text":"","title":"Usage"},{"location":"eventing/flows/sequence/#sequence-spec","text":"Sequence has three parts for the Spec: Steps which defines the in-order list of Subscriber s, aka, which functions are executed in the listed order. These are specified using the messaging.v1.SubscriberSpec just like you would when creating Subscription . Each step should be Addressable . ChannelTemplate defines the Template which will be used to create Channel s between the steps. Reply (Optional) Reference to where the results of the final step in the sequence are sent to.","title":"Sequence Spec"},{"location":"eventing/flows/sequence/#sequence-status","text":"Sequence has four parts for the Status: Conditions which detail the overall Status of the Sequence object ChannelStatuses which convey the Status of underlying Channel resources that are created as part of this Sequence. It is an array and each Status corresponds to the Step number, so the first entry in the array is the Status of the Channel before the first Step. SubscriptionStatuses which convey the Status of underlying Subscription resources that are created as part of this Sequence. It is an array and each Status corresponds to the Step number, so the first entry in the array is the Subscription which is created to wire the first channel to the first step in the Steps array. AddressStatus which is exposed so that Sequence can be used where Addressable can be used. Sending to this address will target the Channel which is fronting the first Step in the Sequence.","title":"Sequence Status"},{"location":"eventing/flows/sequence/#examples","text":"For each of the following examples, you use a PingSource as the source of events. We also use a very simple transformer which performs very trivial transformation of the incoming events to demonstrate they have passed through each stage.","title":"Examples"},{"location":"eventing/flows/sequence/#sequence-with-no-reply","text":"For the first example, we'll use a 3 Step Sequence that is wired directly into the PingSource . Each of the steps simply tacks on \"- Handled by \", for example the first Step in the Sequence will take the incoming message and append \"- Handled by 0\" to the incoming message. See Sequence with no reply (terminal last Step) .","title":"Sequence with no reply"},{"location":"eventing/flows/sequence/#sequence-with-reply","text":"For the next example, we'll use the same 3 Step Sequence that is wired directly into the PingSource . Each of the steps simply tacks on \"- Handled by \", for example the first Step in the Sequence will take the incoming message and append \"- Handled by 0\" to the incoming message. The only difference is that we'll use the Subscriber.Spec.Reply field to wire the output of the last Step to an event display pod. See Sequence with reply (last Step produces output) .","title":"Sequence with reply"},{"location":"eventing/flows/sequence/#chaining-sequences-together","text":"For the next example, we'll use the same 3 Step Sequence that is wired directly into the PingSource . Each of the steps simply tacks on \"- Handled by \", for example the first Step in the Sequence will take the incoming message and append \"- Handled by 0\" to the incoming message. The only difference is that we'll use the Subscriber.Spec.Reply field to wire the output of the last Step to another Sequence that does the same message modifications as the first pipeline (with different steps however). See Chaining Sequences together .","title":"Chaining Sequences together"},{"location":"eventing/flows/sequence/#using-sequence-with-brokertrigger-model","text":"You can also create a Trigger which targets Sequence . This time we'll wire PingSource to send events to a Broker and then we'll have the Sequence emit the resulting Events back into the Broker so that the results of the Sequence can be observed by other Triggers. See Using Sequence with Broker/Trigger model .","title":"Using Sequence with Broker/Trigger model"},{"location":"eventing/flows/sequence/sequence-reply-to-event-display/","text":"Sequence wired to event-display \u00b6 We are going to create the following logical configuration. We create a PingSource, feeding events to a Sequence , then taking the output of that Sequence and displaying the resulting output. The functions used in these examples live in https://github.com/knative/eventing/blob/main/cmd/appender/main.go . Prerequisites \u00b6 For this example, we'll assume you have set up an InMemoryChannel as well as Knative Serving (for our functions). The examples use default namespace, again, if you want to deploy to another Namespace, you will need to modify the examples to reflect this. If you want to use different type of Channel , you will have to modify the Sequence.Spec.ChannelTemplate to create the appropriate Channel resources. Setup \u00b6 Create the Knative Services \u00b6 Change default in the following command to create the steps in the namespace where you want resources created: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : first spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 0\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : second spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 1\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : third spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 2\" --- kubectl -n default create -f ./steps.yaml Create the Sequence \u00b6 The sequence.yaml file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel. apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : first - ref : apiVersion : serving.knative.dev/v1 kind : Service name : second - ref : apiVersion : serving.knative.dev/v1 kind : Service name : third reply : ref : kind : Service apiVersion : serving.knative.dev/v1 name : event-display Change default in the following command to create the Sequence in the namespace where you want the resources to be created: kubectl -n default create -f ./sequence.yaml Create the Service displaying the events created by Sequence \u00b6 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display Change default in the following command to create the Sequence in the namespace where you want your resources to be created: kubectl -n default create -f ./event-display.yaml Create the PingSource targeting the Sequence \u00b6 This will create a PingSource which will send a CloudEvent with {\"message\": \"Hello world!\"} as the data payload every 2 minutes. apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : ping-source spec : schedule : \"*/2 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : flows.knative.dev/v1 kind : Sequence name : sequence kubectl -n default create -f ./ping-source.yaml Inspecting the results \u00b6 You can now see the final output by inspecting the logs of the event-display pods. kubectl -n default get pods Wait a bit and then look at the logs for the event-display pod: kubectl -n default logs -l serving.knative.dev/service = event-display -c user-container --tail = -1 \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: samples.http.mode3 source: /apis/v1/namespaces/default/pingsources/ping-source id: e8fa7906-ab62-4e61-9c13-a9406e2130a9 time: 2020 -03-02T20:52:00.0004957Z datacontenttype: application/json Extensions, knativehistory: sequence-kn-sequence-0-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-1-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-2-kn-channel.default.svc.cluster.local traceparent: 00 -6e2947379387f35ddc933b9190af16ad-de3db0bc4e442394-00 Data, { \"id\" : 0 , \"message\" : \"Hello world! - Handled by 0 - Handled by 1 - Handled by 2\" } And you can see that the initial PingSource message (\"Hello World!\") has been appended to it by each of the steps in the Sequence.","title":"Displaying Sequence output"},{"location":"eventing/flows/sequence/sequence-reply-to-event-display/#sequence-wired-to-event-display","text":"We are going to create the following logical configuration. We create a PingSource, feeding events to a Sequence , then taking the output of that Sequence and displaying the resulting output. The functions used in these examples live in https://github.com/knative/eventing/blob/main/cmd/appender/main.go .","title":"Sequence wired to event-display"},{"location":"eventing/flows/sequence/sequence-reply-to-event-display/#prerequisites","text":"For this example, we'll assume you have set up an InMemoryChannel as well as Knative Serving (for our functions). The examples use default namespace, again, if you want to deploy to another Namespace, you will need to modify the examples to reflect this. If you want to use different type of Channel , you will have to modify the Sequence.Spec.ChannelTemplate to create the appropriate Channel resources.","title":"Prerequisites"},{"location":"eventing/flows/sequence/sequence-reply-to-event-display/#setup","text":"","title":"Setup"},{"location":"eventing/flows/sequence/sequence-reply-to-event-display/#create-the-knative-services","text":"Change default in the following command to create the steps in the namespace where you want resources created: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : first spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 0\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : second spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 1\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : third spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 2\" --- kubectl -n default create -f ./steps.yaml","title":"Create the Knative Services"},{"location":"eventing/flows/sequence/sequence-reply-to-event-display/#create-the-sequence","text":"The sequence.yaml file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel. apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : first - ref : apiVersion : serving.knative.dev/v1 kind : Service name : second - ref : apiVersion : serving.knative.dev/v1 kind : Service name : third reply : ref : kind : Service apiVersion : serving.knative.dev/v1 name : event-display Change default in the following command to create the Sequence in the namespace where you want the resources to be created: kubectl -n default create -f ./sequence.yaml","title":"Create the Sequence"},{"location":"eventing/flows/sequence/sequence-reply-to-event-display/#create-the-service-displaying-the-events-created-by-sequence","text":"apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display Change default in the following command to create the Sequence in the namespace where you want your resources to be created: kubectl -n default create -f ./event-display.yaml","title":"Create the Service displaying the events created by Sequence"},{"location":"eventing/flows/sequence/sequence-reply-to-event-display/#create-the-pingsource-targeting-the-sequence","text":"This will create a PingSource which will send a CloudEvent with {\"message\": \"Hello world!\"} as the data payload every 2 minutes. apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : ping-source spec : schedule : \"*/2 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : flows.knative.dev/v1 kind : Sequence name : sequence kubectl -n default create -f ./ping-source.yaml","title":"Create the PingSource targeting the Sequence"},{"location":"eventing/flows/sequence/sequence-reply-to-event-display/#inspecting-the-results","text":"You can now see the final output by inspecting the logs of the event-display pods. kubectl -n default get pods Wait a bit and then look at the logs for the event-display pod: kubectl -n default logs -l serving.knative.dev/service = event-display -c user-container --tail = -1 \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: samples.http.mode3 source: /apis/v1/namespaces/default/pingsources/ping-source id: e8fa7906-ab62-4e61-9c13-a9406e2130a9 time: 2020 -03-02T20:52:00.0004957Z datacontenttype: application/json Extensions, knativehistory: sequence-kn-sequence-0-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-1-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-2-kn-channel.default.svc.cluster.local traceparent: 00 -6e2947379387f35ddc933b9190af16ad-de3db0bc4e442394-00 Data, { \"id\" : 0 , \"message\" : \"Hello world! - Handled by 0 - Handled by 1 - Handled by 2\" } And you can see that the initial PingSource message (\"Hello World!\") has been appended to it by each of the steps in the Sequence.","title":"Inspecting the results"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/","text":"Sequence wired to another Sequence \u00b6 We are going to create the following logical configuration. We create a PingSource, feeding events to a Sequence , then taking the output of that Sequence and sending it to a second Sequence and finally displaying the resulting output. The functions used in these examples live in https://github.com/knative/eventing/blob/main/cmd/appender/main.go . Prerequisites \u00b6 For this example, we'll assume you have set up an InMemoryChannel as well as Knative Serving (for our functions). The examples use default namespace, again, if you want to deploy to another namespace, you will need to modify the examples to reflect this. If you want to use different type of Channel , you will have to modify the Sequence.Spec.ChannelTemplate to create the appropriate Channel resources. Setup \u00b6 Create the Knative Services \u00b6 Change default in the following command to create the steps in the namespace where you want resources created: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : first spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 0\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : second spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 1\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : third spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 2\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : fourth spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 3\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : fifth spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 4\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sixth spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 5\" --- kubectl -n default create -f ./steps.yaml Create the first Sequence \u00b6 The sequence1.yaml file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel. apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : first-sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : first - ref : apiVersion : serving.knative.dev/v1 kind : Service name : second - ref : apiVersion : serving.knative.dev/v1 kind : Service name : third reply : ref : kind : Sequence apiVersion : flows.knative.dev/v1 name : second-sequence Change default in the following command to create the Sequence in the namespace where you want your resources created: kubectl -n default create -f ./sequence1.yaml Create the second Sequence \u00b6 The sequence2.yaml file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel. apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : second-sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : fourth - ref : apiVersion : serving.knative.dev/v1 kind : Service name : fifth - ref : apiVersion : serving.knative.dev/v1 kind : Service name : sixth reply : ref : kind : Service apiVersion : serving.knative.dev/v1 name : event-display kubectl -n default create -f ./sequence2.yaml Create the Service displaying the events created by Sequence \u00b6 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display Change default in the following command to create the Sequence in the namespace where you want your resources created: kubectl -n default create -f ./event-display.yaml Create the PingSource targeting the first Sequence \u00b6 This will create a PingSource which will send a CloudEvent with {\"message\": \"Hello world!\"} as the data payload every 2 minutes. apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : ping-source spec : schedule : \"*/2 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : flows.knative.dev/v1 kind : Sequence name : first-sequence kubectl -n default create -f ./ping-source.yaml Inspecting the results \u00b6 You can now see the final output by inspecting the logs of the event-display pods. kubectl -n default get pods Then look at the logs for the event-display pod: kubectl -n default logs -l serving.knative.dev/service = event-display -c user-container --tail = -1 \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.ping source: /apis/v1/namespaces/default/pingsources/ping-source id: 29d531df-78d8-4d11-9ffd-ba24045241a9 time: 2020 -03-02T21:18:00.0011708Z datacontenttype: application/json Extensions, knativehistory: first-sequence-kn-sequence-0-kn-channel.default.svc.cluster.local ; first-sequence-kn-sequence-1-kn-channel.default.svc.cluster.local ; first-sequence-kn-sequence-2-kn-channel.default.svc.cluster.local ; second-sequence-kn-sequence-0-kn-channel.default.svc.cluster.local ; second-sequence-kn-sequence-1-kn-channel.default.svc.cluster.local ; second-sequence-kn-sequence-2-kn-channel.default.svc.cluster.local traceparent: 00 -e5abc9de525a89ead80560b8f328de5c-fc12b64a6296f541-00 Data, { \"id\" : 0 , \"message\" : \"Hello world! - Handled by 0 - Handled by 1 - Handled by 2 - Handled by 3 - Handled by 4 - Handled by 5\" } And you can see that the initial PingSource message (\"Hello World!\") has been appended to it by each of the steps in the Sequence.","title":"Using Sequences in series"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/#sequence-wired-to-another-sequence","text":"We are going to create the following logical configuration. We create a PingSource, feeding events to a Sequence , then taking the output of that Sequence and sending it to a second Sequence and finally displaying the resulting output. The functions used in these examples live in https://github.com/knative/eventing/blob/main/cmd/appender/main.go .","title":"Sequence wired to another Sequence"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/#prerequisites","text":"For this example, we'll assume you have set up an InMemoryChannel as well as Knative Serving (for our functions). The examples use default namespace, again, if you want to deploy to another namespace, you will need to modify the examples to reflect this. If you want to use different type of Channel , you will have to modify the Sequence.Spec.ChannelTemplate to create the appropriate Channel resources.","title":"Prerequisites"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/#setup","text":"","title":"Setup"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/#create-the-knative-services","text":"Change default in the following command to create the steps in the namespace where you want resources created: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : first spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 0\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : second spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 1\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : third spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 2\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : fourth spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 3\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : fifth spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 4\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sixth spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 5\" --- kubectl -n default create -f ./steps.yaml","title":"Create the Knative Services"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/#create-the-first-sequence","text":"The sequence1.yaml file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel. apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : first-sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : first - ref : apiVersion : serving.knative.dev/v1 kind : Service name : second - ref : apiVersion : serving.knative.dev/v1 kind : Service name : third reply : ref : kind : Sequence apiVersion : flows.knative.dev/v1 name : second-sequence Change default in the following command to create the Sequence in the namespace where you want your resources created: kubectl -n default create -f ./sequence1.yaml","title":"Create the first Sequence"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/#create-the-second-sequence","text":"The sequence2.yaml file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel. apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : second-sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : fourth - ref : apiVersion : serving.knative.dev/v1 kind : Service name : fifth - ref : apiVersion : serving.knative.dev/v1 kind : Service name : sixth reply : ref : kind : Service apiVersion : serving.knative.dev/v1 name : event-display kubectl -n default create -f ./sequence2.yaml","title":"Create the second Sequence"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/#create-the-service-displaying-the-events-created-by-sequence","text":"apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display Change default in the following command to create the Sequence in the namespace where you want your resources created: kubectl -n default create -f ./event-display.yaml","title":"Create the Service displaying the events created by Sequence"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/#create-the-pingsource-targeting-the-first-sequence","text":"This will create a PingSource which will send a CloudEvent with {\"message\": \"Hello world!\"} as the data payload every 2 minutes. apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : ping-source spec : schedule : \"*/2 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : flows.knative.dev/v1 kind : Sequence name : first-sequence kubectl -n default create -f ./ping-source.yaml","title":"Create the PingSource targeting the first Sequence"},{"location":"eventing/flows/sequence/sequence-reply-to-sequence/#inspecting-the-results","text":"You can now see the final output by inspecting the logs of the event-display pods. kubectl -n default get pods Then look at the logs for the event-display pod: kubectl -n default logs -l serving.knative.dev/service = event-display -c user-container --tail = -1 \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.ping source: /apis/v1/namespaces/default/pingsources/ping-source id: 29d531df-78d8-4d11-9ffd-ba24045241a9 time: 2020 -03-02T21:18:00.0011708Z datacontenttype: application/json Extensions, knativehistory: first-sequence-kn-sequence-0-kn-channel.default.svc.cluster.local ; first-sequence-kn-sequence-1-kn-channel.default.svc.cluster.local ; first-sequence-kn-sequence-2-kn-channel.default.svc.cluster.local ; second-sequence-kn-sequence-0-kn-channel.default.svc.cluster.local ; second-sequence-kn-sequence-1-kn-channel.default.svc.cluster.local ; second-sequence-kn-sequence-2-kn-channel.default.svc.cluster.local traceparent: 00 -e5abc9de525a89ead80560b8f328de5c-fc12b64a6296f541-00 Data, { \"id\" : 0 , \"message\" : \"Hello world! - Handled by 0 - Handled by 1 - Handled by 2 - Handled by 3 - Handled by 4 - Handled by 5\" } And you can see that the initial PingSource message (\"Hello World!\") has been appended to it by each of the steps in the Sequence.","title":"Inspecting the results"},{"location":"eventing/flows/sequence/sequence-terminal/","text":"Sequence terminal \u00b6 We are going to create the following logical configuration. We create a PingSource, feeding events to a Sequence . Sequence can then do either external work, or out of band create additional events. The functions used in these examples live in https://github.com/knative/eventing/blob/main/cmd/appender/main.go . Prerequisites \u00b6 For this example, we'll assume you have set up an InMemoryChannel as well as Knative Serving (for our functions). The examples use default namespace, again, if you want to deploy to another Namespace, you will need to modify the examples to reflect this. If you want to use different type of Channel , you will have to modify the Sequence.Spec.ChannelTemplate to create the appropriate Channel resources. Setup \u00b6 Create the Knative Services \u00b6 First create the 3 steps that will be referenced in the Steps. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : first spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 0\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : second spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 1\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : third spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 2\" --- kubectl -n default create -f ./steps.yaml Create the Sequence \u00b6 The sequence.yaml file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel. apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : first - ref : apiVersion : serving.knative.dev/v1 kind : Service name : second - ref : apiVersion : serving.knative.dev/v1 kind : Service name : third Change default in the following command to create the Sequence in the namespace where you want the resources to be created: kubectl -n default create -f ./sequence.yaml Create the PingSource targeting the Sequence \u00b6 This will create a PingSource which will send a CloudEvent with {\"message\": \"Hello world!\"} as the data payload every 2 minutes. apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : ping-source spec : schedule : \"*/2 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : flows.knative.dev/v1 kind : Sequence name : sequence kubectl -n default create -f ./ping-source.yaml Inspecting the results \u00b6 You can now see the final output by inspecting the logs of the event-display pods. Note that since we set the PingSource to emit every 2 minutes, it might take some time for the events to show up in the logs. kubectl -n default get pods Let's look at the logs for the first Step in the Sequence : kubectl -n default logs -l serving.knative.dev/service = first -c user-container --tail = -1 2020 /03/02 21 :28:00 listening on 8080 , appending \" - Handled by 0\" to events 2020 /03/02 21 :28:01 Received a new event: 2020 /03/02 21 :28:01 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! } 2020 /03/02 21 :28:01 Transform the event to: 2020 /03/02 21 :28:01 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 } And you can see that the initial PingSource message (\"Hello World!\") has now been modified by the first step in the Sequence to include \" - Handled by 0\". Exciting :) Then we can look at the output of the second Step in the Sequence : kubectl -n default logs -l serving.knative.dev/service = second -c user-container --tail = -1 2020 /03/02 21 :28:02 listening on 8080 , appending \" - Handled by 1\" to events 2020 /03/02 21 :28:02 Received a new event: 2020 /03/02 21 :28:02 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 } 2020 /03/02 21 :28:02 Transform the event to: 2020 /03/02 21 :28:02 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 - Handled by 1 } And as expected it's now been handled by both the first and second Step as reflected by the Message being now: \"Hello world! - Handled by 0 - Handled by 1\" Then we can look at the output of the last Step in the Sequence : kubectl -n default logs -l serving.knative.dev/service = third -c user-container --tail = -1 2020 /03/02 21 :28:03 listening on 8080 , appending \" - Handled by 2\" to events 2020 /03/02 21 :28:03 Received a new event: 2020 /03/02 21 :28:03 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 - Handled by 1 } 2020 /03/02 21 :28:03 Transform the event to: 2020 /03/02 21 :28:03 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 - Handled by 1 - Handled by 2 }","title":"Create additional events"},{"location":"eventing/flows/sequence/sequence-terminal/#sequence-terminal","text":"We are going to create the following logical configuration. We create a PingSource, feeding events to a Sequence . Sequence can then do either external work, or out of band create additional events. The functions used in these examples live in https://github.com/knative/eventing/blob/main/cmd/appender/main.go .","title":"Sequence terminal"},{"location":"eventing/flows/sequence/sequence-terminal/#prerequisites","text":"For this example, we'll assume you have set up an InMemoryChannel as well as Knative Serving (for our functions). The examples use default namespace, again, if you want to deploy to another Namespace, you will need to modify the examples to reflect this. If you want to use different type of Channel , you will have to modify the Sequence.Spec.ChannelTemplate to create the appropriate Channel resources.","title":"Prerequisites"},{"location":"eventing/flows/sequence/sequence-terminal/#setup","text":"","title":"Setup"},{"location":"eventing/flows/sequence/sequence-terminal/#create-the-knative-services","text":"First create the 3 steps that will be referenced in the Steps. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : first spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 0\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : second spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 1\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : third spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 2\" --- kubectl -n default create -f ./steps.yaml","title":"Create the Knative Services"},{"location":"eventing/flows/sequence/sequence-terminal/#create-the-sequence","text":"The sequence.yaml file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel. apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : first - ref : apiVersion : serving.knative.dev/v1 kind : Service name : second - ref : apiVersion : serving.knative.dev/v1 kind : Service name : third Change default in the following command to create the Sequence in the namespace where you want the resources to be created: kubectl -n default create -f ./sequence.yaml","title":"Create the Sequence"},{"location":"eventing/flows/sequence/sequence-terminal/#create-the-pingsource-targeting-the-sequence","text":"This will create a PingSource which will send a CloudEvent with {\"message\": \"Hello world!\"} as the data payload every 2 minutes. apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : ping-source spec : schedule : \"*/2 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : flows.knative.dev/v1 kind : Sequence name : sequence kubectl -n default create -f ./ping-source.yaml","title":"Create the PingSource targeting the Sequence"},{"location":"eventing/flows/sequence/sequence-terminal/#inspecting-the-results","text":"You can now see the final output by inspecting the logs of the event-display pods. Note that since we set the PingSource to emit every 2 minutes, it might take some time for the events to show up in the logs. kubectl -n default get pods Let's look at the logs for the first Step in the Sequence : kubectl -n default logs -l serving.knative.dev/service = first -c user-container --tail = -1 2020 /03/02 21 :28:00 listening on 8080 , appending \" - Handled by 0\" to events 2020 /03/02 21 :28:01 Received a new event: 2020 /03/02 21 :28:01 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! } 2020 /03/02 21 :28:01 Transform the event to: 2020 /03/02 21 :28:01 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 } And you can see that the initial PingSource message (\"Hello World!\") has now been modified by the first step in the Sequence to include \" - Handled by 0\". Exciting :) Then we can look at the output of the second Step in the Sequence : kubectl -n default logs -l serving.knative.dev/service = second -c user-container --tail = -1 2020 /03/02 21 :28:02 listening on 8080 , appending \" - Handled by 1\" to events 2020 /03/02 21 :28:02 Received a new event: 2020 /03/02 21 :28:02 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 } 2020 /03/02 21 :28:02 Transform the event to: 2020 /03/02 21 :28:02 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 - Handled by 1 } And as expected it's now been handled by both the first and second Step as reflected by the Message being now: \"Hello world! - Handled by 0 - Handled by 1\" Then we can look at the output of the last Step in the Sequence : kubectl -n default logs -l serving.knative.dev/service = third -c user-container --tail = -1 2020 /03/02 21 :28:03 listening on 8080 , appending \" - Handled by 2\" to events 2020 /03/02 21 :28:03 Received a new event: 2020 /03/02 21 :28:03 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 - Handled by 1 } 2020 /03/02 21 :28:03 Transform the event to: 2020 /03/02 21 :28:03 [ 2020 -03-02T21:28:00.0010247Z ] /apis/v1/namespaces/default/pingsources/ping-source dev.knative.sources.ping: & { Sequence:0 Message:Hello world! - Handled by 0 - Handled by 1 - Handled by 2 }","title":"Inspecting the results"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/","text":"Using Sequence with Broker and Trigger \u00b6 We are going to create the following logical configuration. We create a PingSource, feeding events into the Broker, then we create a Filter that wires those events into a Sequence consisting of 3 steps. Then we take the end of the Sequence and feed newly minted events back into the Broker and create another Trigger which will then display those events. Prerequisites \u00b6 Knative Serving InMemoryChannel Note The examples use the default namespace. If you want to use different type of Channel , you will have to modify the Sequence.Spec.ChannelTemplate to create the appropriate Channel resources. The functions used in these examples live in https://github.com/knative/eventing/blob/main/cmd/appender/main.go . Setup \u00b6 Creating the Broker \u00b6 To create the cluster default Broker type, copy the following YAML into a file: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : default Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Create the Knative Services \u00b6 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : first spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 0\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : second spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 1\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : third spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 2\" - name : TYPE value : \"samples.http.mod3\" --- Change default in the following command to create the services in the namespace where you have configured your broker: kubectl -n default create -f ./steps.yaml Create the Sequence \u00b6 The sequence.yaml file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel. Also, change the spec.reply.name to point to your Broker apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : first - ref : apiVersion : serving.knative.dev/v1 kind : Service name : second - ref : apiVersion : serving.knative.dev/v1 kind : Service name : third reply : ref : kind : Broker apiVersion : eventing.knative.dev/v1 name : default Change default in the following command to create the sequence in the namespace where you have configured your broker: kubectl -n default create -f ./sequence.yaml Create the PingSource targeting the Broker \u00b6 This will create a PingSource which will send a CloudEvent with {\"message\": \"Hello world!\"} as the data payload every 2 minutes. apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : ping-source spec : schedule : \"*/2 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : eventing.knative.dev/v1 kind : Broker name : default Change default in the following command to create the PingSource in the namespace where you have configured your broker and sequence: kubectl -n default create -f ./ping-source.yaml Create the Trigger targeting the Sequence \u00b6 apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : sequence-trigger spec : broker : default filter : attributes : type : dev.knative.sources.ping subscriber : ref : apiVersion : flows.knative.dev/v1 kind : Sequence name : sequence Change default in the following command to create the trigger in the namespace where you have configured your broker and sequence: kubectl -n default create -f ./trigger.yaml Create the Service and Trigger displaying the events created by Sequence \u00b6 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sequence-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : display-trigger spec : broker : default filter : attributes : type : samples.http.mod3 subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : sequence-display --- Change default in the following command to create the service and trigger in the namespace where you have configured your broker: kubectl -n default create -f ./display-trigger.yaml Inspecting the results \u00b6 You can now see the final output by inspecting the logs of the sequence-display pods. kubectl -n default get pods View the logs for the sequence-display pod: kubectl -n default logs -l serving.knative.dev/service = sequence-display -c user-container --tail = -1 \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: samples.http.mod3 source: /apis/v1/namespaces/default/pingsources/ping-source id: 159bba01-054a-4ae7-b7be-d4e7c5f773d2 time: 2020 -03-03T14:56:00.000652027Z datacontenttype: application/json Extensions, knativearrivaltime: 2020 -03-03T14:56:00.018390608Z knativehistory: default-kne-trigger-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-0-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-1-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-2-kn-channel.default.svc.cluster.local ; default-kne-trigger-kn-channel.default.svc.cluster.local traceparent: 00 -e893412106ff417a90a5695e53ffd9cc-5829ae45a14ed462-00 Data, { \"id\" : 0 , \"message\" : \"Hello world! - Handled by 0 - Handled by 1 - Handled by 2\" } And you can see that the initial PingSource message {\"Hello World!\"} has been appended to it by each of the steps in the Sequence.","title":"Using with Broker and Trigger"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#using-sequence-with-broker-and-trigger","text":"We are going to create the following logical configuration. We create a PingSource, feeding events into the Broker, then we create a Filter that wires those events into a Sequence consisting of 3 steps. Then we take the end of the Sequence and feed newly minted events back into the Broker and create another Trigger which will then display those events.","title":"Using Sequence with Broker and Trigger"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#prerequisites","text":"Knative Serving InMemoryChannel Note The examples use the default namespace. If you want to use different type of Channel , you will have to modify the Sequence.Spec.ChannelTemplate to create the appropriate Channel resources. The functions used in these examples live in https://github.com/knative/eventing/blob/main/cmd/appender/main.go .","title":"Prerequisites"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#setup","text":"","title":"Setup"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#creating-the-broker","text":"To create the cluster default Broker type, copy the following YAML into a file: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : default Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"Creating the Broker"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#create-the-knative-services","text":"apiVersion : serving.knative.dev/v1 kind : Service metadata : name : first spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 0\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : second spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 1\" --- apiVersion : serving.knative.dev/v1 kind : Service metadata : name : third spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/appender env : - name : MESSAGE value : \" - Handled by 2\" - name : TYPE value : \"samples.http.mod3\" --- Change default in the following command to create the services in the namespace where you have configured your broker: kubectl -n default create -f ./steps.yaml","title":"Create the Knative Services"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#create-the-sequence","text":"The sequence.yaml file contains the specifications for creating the Sequence. If you are using a different type of Channel, you need to change the spec.channelTemplate to point to your desired Channel. Also, change the spec.reply.name to point to your Broker apiVersion : flows.knative.dev/v1 kind : Sequence metadata : name : sequence spec : channelTemplate : apiVersion : messaging.knative.dev/v1 kind : InMemoryChannel steps : - ref : apiVersion : serving.knative.dev/v1 kind : Service name : first - ref : apiVersion : serving.knative.dev/v1 kind : Service name : second - ref : apiVersion : serving.knative.dev/v1 kind : Service name : third reply : ref : kind : Broker apiVersion : eventing.knative.dev/v1 name : default Change default in the following command to create the sequence in the namespace where you have configured your broker: kubectl -n default create -f ./sequence.yaml","title":"Create the Sequence"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#create-the-pingsource-targeting-the-broker","text":"This will create a PingSource which will send a CloudEvent with {\"message\": \"Hello world!\"} as the data payload every 2 minutes. apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : ping-source spec : schedule : \"*/2 * * * *\" contentType : \"application/json\" data : '{\"message\": \"Hello world!\"}' sink : ref : apiVersion : eventing.knative.dev/v1 kind : Broker name : default Change default in the following command to create the PingSource in the namespace where you have configured your broker and sequence: kubectl -n default create -f ./ping-source.yaml","title":"Create the PingSource targeting the Broker"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#create-the-trigger-targeting-the-sequence","text":"apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : sequence-trigger spec : broker : default filter : attributes : type : dev.knative.sources.ping subscriber : ref : apiVersion : flows.knative.dev/v1 kind : Sequence name : sequence Change default in the following command to create the trigger in the namespace where you have configured your broker and sequence: kubectl -n default create -f ./trigger.yaml","title":"Create the Trigger targeting the Sequence"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#create-the-service-and-trigger-displaying-the-events-created-by-sequence","text":"apiVersion : serving.knative.dev/v1 kind : Service metadata : name : sequence-display spec : template : spec : containers : - image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : display-trigger spec : broker : default filter : attributes : type : samples.http.mod3 subscriber : ref : apiVersion : serving.knative.dev/v1 kind : Service name : sequence-display --- Change default in the following command to create the service and trigger in the namespace where you have configured your broker: kubectl -n default create -f ./display-trigger.yaml","title":"Create the Service and Trigger displaying the events created by Sequence"},{"location":"eventing/flows/sequence/sequence-with-broker-trigger/#inspecting-the-results","text":"You can now see the final output by inspecting the logs of the sequence-display pods. kubectl -n default get pods View the logs for the sequence-display pod: kubectl -n default logs -l serving.knative.dev/service = sequence-display -c user-container --tail = -1 \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: samples.http.mod3 source: /apis/v1/namespaces/default/pingsources/ping-source id: 159bba01-054a-4ae7-b7be-d4e7c5f773d2 time: 2020 -03-03T14:56:00.000652027Z datacontenttype: application/json Extensions, knativearrivaltime: 2020 -03-03T14:56:00.018390608Z knativehistory: default-kne-trigger-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-0-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-1-kn-channel.default.svc.cluster.local ; sequence-kn-sequence-2-kn-channel.default.svc.cluster.local ; default-kne-trigger-kn-channel.default.svc.cluster.local traceparent: 00 -e893412106ff417a90a5695e53ffd9cc-5829ae45a14ed462-00 Data, { \"id\" : 0 , \"message\" : \"Hello world! - Handled by 0 - Handled by 1 - Handled by 2\" } And you can see that the initial PingSource message {\"Hello World!\"} has been appended to it by each of the steps in the Sequence.","title":"Inspecting the results"},{"location":"eventing/observability/logging/collecting-logs/","text":"Logging \u00b6 You can use Fluent Bit , a log processor and forwarder, to collect Kubernetes logs in a central directory. This is not required to run Knative, but can be helpful with Knative Serving , which automatically deletes pods and associated logs when they are no longer needed. Fluent Bit supports exporting to a number of other log providers. If you already have an existing log provider, for example, Splunk, Datadog, ElasticSearch, or Stackdriver, you can follow the FluentBit documentation to configure log forwarders. Setting up logging components \u00b6 Setting up log collection requires two steps: Running a log forwarding DaemonSet on each node. Running a collector somewhere in the cluster. Tip In the following example, a StatefulSet is used, which stores logs on a Kubernetes PersistentVolumeClaim, but you can also use a HostPath. Setting up the collector \u00b6 The fluent-bit-collector.yaml file defines a StatefulSet, as well as a Kubernetes Service which allows accessing and reading the logs from within the cluster. The supplied configuration will create the monitoring configuration in a namespace called logging . Important Set up the collector before the forwarders. You will need the address of the collector when configuring the forwarders, and the forwarders may queue logs until the collector is ready. Procedure \u00b6 Apply the configuration by entering the command: kubectl apply -f https://github.com/knative/docs/raw/main/docs/serving/observability/logging/fluent-bit-collector.yaml The default configuration will classify logs into: Knative services, or pods with an app=Knative label. Non-Knative apps. Note Logs default to logging with the pod name; this can be changed by updating the log-collector-config ConfigMap before or after installation. Warning After the ConfigMap is updated, you must restart Fluent Bit. You can do this by deleting the pod and letting the StatefulSet recreate it. To access the logs through your web browser, enter the command: kubectl port-forward --namespace logging service/log-collector 8080 :80 Navigate to http://localhost:8080/ . Optional: You can open a shell in the nginx pod and search the logs using Unix tools, by entering the command: kubectl exec --namespace logging --stdin --tty --container nginx log-collector-0 Setting up the forwarders \u00b6 See the Fluent Bit documentation to set up a Fluent Bit DaemonSet that forwards logs to ElasticSearch by default. When you create a ConfigMap during the installation steps, you must: Replace the ElasticSearch configuration with the fluent-bit-configmap.yaml , or Add the following block to the ConfigMap, and update the @INCLUDE output-elasticsearch.conf to be @INCLUDE output-forward.conf : output-forward.conf : | [OUTPUT] Name forward Host log-collector.logging Port 24224 Require_ack_response True Setting up a local collector \u00b6 Warning This procedure describes a development environment setup and is not suitable for production use. If you are using a local Kubernetes cluster for development, you can create a hostPath PersistentVolume to store the logs on your desktop operating system. This allows you to use your usual desktop tools on the files without needing Kubernetes-specific tools. The PersistentVolumeClaim will look similar to the following: apiVersion : v1 kind : PersistentVolume metadata : name : shared-logs labels : app : logs-collector spec : accessModes : - \"ReadWriteOnce\" storageClassName : manual claimRef : apiVersion : v1 kind : PersistentVolumeClaim name : logs-log-collector-0 namespace : logging capacity : storage : 5Gi hostPath : path : <see below> Note The hostPath will vary based on your Kubernetes software and host operating system. You must update the StatefulSet volumeClaimTemplates to reference the shared-logs volume, as shown in the following example: volumeClaimTemplates : metadata : name : logs spec : accessModes : [ \"ReadWriteOnce\" ] volumeName : shared-logs Kind \u00b6 When creating your cluster, you must use a kind-config.yaml and specify extraMounts for each node, as shown in the following example: apiversion : kind.x-k8s.io/v1alpha4 kind : Cluster nodes : - role : control-plane extraMounts : - hostPath : ./logs containerPath : /shared/logs - role : worker extraMounts : - hostPath : ./logs containerPath : /shared/logs You can then use /shared/logs as the spec.hostPath.path in your PersistentVolume. Note that the directory path ./logs is relative to the directory that the Kind cluster was created in. Docker Desktop \u00b6 Docker desktop automatically creates some shared mounts between the host and the guest operating systems, so you only need to know the path to your home directory. The following are some examples for different operating systems: Host OS hostPath Mac OS /Users/${USER} Windows /run/desktop/mnt/host/c/Users/${USER}/ Linux /home/${USER} Minikube \u00b6 Minikube requires an explicit command to mount a directory into the virtual machine (VM) running Kubernetes. The following command mounts the logs directory inside the current directory onto /mnt/logs in the VM: minikube mount ./logs:/mnt/logs You must also reference /mnt/logs as the hostPath.path in the PersistentVolume.","title":"Collecting logs"},{"location":"eventing/observability/logging/collecting-logs/#logging","text":"You can use Fluent Bit , a log processor and forwarder, to collect Kubernetes logs in a central directory. This is not required to run Knative, but can be helpful with Knative Serving , which automatically deletes pods and associated logs when they are no longer needed. Fluent Bit supports exporting to a number of other log providers. If you already have an existing log provider, for example, Splunk, Datadog, ElasticSearch, or Stackdriver, you can follow the FluentBit documentation to configure log forwarders.","title":"Logging"},{"location":"eventing/observability/logging/collecting-logs/#setting-up-logging-components","text":"Setting up log collection requires two steps: Running a log forwarding DaemonSet on each node. Running a collector somewhere in the cluster. Tip In the following example, a StatefulSet is used, which stores logs on a Kubernetes PersistentVolumeClaim, but you can also use a HostPath.","title":"Setting up logging components"},{"location":"eventing/observability/logging/collecting-logs/#setting-up-the-collector","text":"The fluent-bit-collector.yaml file defines a StatefulSet, as well as a Kubernetes Service which allows accessing and reading the logs from within the cluster. The supplied configuration will create the monitoring configuration in a namespace called logging . Important Set up the collector before the forwarders. You will need the address of the collector when configuring the forwarders, and the forwarders may queue logs until the collector is ready.","title":"Setting up the collector"},{"location":"eventing/observability/logging/collecting-logs/#procedure","text":"Apply the configuration by entering the command: kubectl apply -f https://github.com/knative/docs/raw/main/docs/serving/observability/logging/fluent-bit-collector.yaml The default configuration will classify logs into: Knative services, or pods with an app=Knative label. Non-Knative apps. Note Logs default to logging with the pod name; this can be changed by updating the log-collector-config ConfigMap before or after installation. Warning After the ConfigMap is updated, you must restart Fluent Bit. You can do this by deleting the pod and letting the StatefulSet recreate it. To access the logs through your web browser, enter the command: kubectl port-forward --namespace logging service/log-collector 8080 :80 Navigate to http://localhost:8080/ . Optional: You can open a shell in the nginx pod and search the logs using Unix tools, by entering the command: kubectl exec --namespace logging --stdin --tty --container nginx log-collector-0","title":"Procedure"},{"location":"eventing/observability/logging/collecting-logs/#setting-up-the-forwarders","text":"See the Fluent Bit documentation to set up a Fluent Bit DaemonSet that forwards logs to ElasticSearch by default. When you create a ConfigMap during the installation steps, you must: Replace the ElasticSearch configuration with the fluent-bit-configmap.yaml , or Add the following block to the ConfigMap, and update the @INCLUDE output-elasticsearch.conf to be @INCLUDE output-forward.conf : output-forward.conf : | [OUTPUT] Name forward Host log-collector.logging Port 24224 Require_ack_response True","title":"Setting up the forwarders"},{"location":"eventing/observability/logging/collecting-logs/#setting-up-a-local-collector","text":"Warning This procedure describes a development environment setup and is not suitable for production use. If you are using a local Kubernetes cluster for development, you can create a hostPath PersistentVolume to store the logs on your desktop operating system. This allows you to use your usual desktop tools on the files without needing Kubernetes-specific tools. The PersistentVolumeClaim will look similar to the following: apiVersion : v1 kind : PersistentVolume metadata : name : shared-logs labels : app : logs-collector spec : accessModes : - \"ReadWriteOnce\" storageClassName : manual claimRef : apiVersion : v1 kind : PersistentVolumeClaim name : logs-log-collector-0 namespace : logging capacity : storage : 5Gi hostPath : path : <see below> Note The hostPath will vary based on your Kubernetes software and host operating system. You must update the StatefulSet volumeClaimTemplates to reference the shared-logs volume, as shown in the following example: volumeClaimTemplates : metadata : name : logs spec : accessModes : [ \"ReadWriteOnce\" ] volumeName : shared-logs","title":"Setting up a local collector"},{"location":"eventing/observability/logging/collecting-logs/#kind","text":"When creating your cluster, you must use a kind-config.yaml and specify extraMounts for each node, as shown in the following example: apiversion : kind.x-k8s.io/v1alpha4 kind : Cluster nodes : - role : control-plane extraMounts : - hostPath : ./logs containerPath : /shared/logs - role : worker extraMounts : - hostPath : ./logs containerPath : /shared/logs You can then use /shared/logs as the spec.hostPath.path in your PersistentVolume. Note that the directory path ./logs is relative to the directory that the Kind cluster was created in.","title":"Kind"},{"location":"eventing/observability/logging/collecting-logs/#docker-desktop","text":"Docker desktop automatically creates some shared mounts between the host and the guest operating systems, so you only need to know the path to your home directory. The following are some examples for different operating systems: Host OS hostPath Mac OS /Users/${USER} Windows /run/desktop/mnt/host/c/Users/${USER}/ Linux /home/${USER}","title":"Docker Desktop"},{"location":"eventing/observability/logging/collecting-logs/#minikube","text":"Minikube requires an explicit command to mount a directory into the virtual machine (VM) running Kubernetes. The following command mounts the logs directory inside the current directory onto /mnt/logs in the VM: minikube mount ./logs:/mnt/logs You must also reference /mnt/logs as the hostPath.path in the PersistentVolume.","title":"Minikube"},{"location":"eventing/observability/logging/config-logging/","text":"Configuring Log Settings \u00b6 Log configuration for all Knative components is managed through the config-logging ConfigMap in the corresponding namespace. For example, Serving components are configured through config-logging in the knative-serving namespace and Eventing components are configured through config-logging in the knative-eventing namespace, etc. Knative components use the zap logging library; options are documented in more detail in that project . In addition to zap-logger-config , which is a general key that applies to all components in that namespace, the config-logging ConfigMap supports overriding the log level for individual components. ConfigMap key Description zap-logger-config A JSON object container for a zap logger configuration. Key fields are highlighted below. zap-logger-config.level The default logging level for components. Messages at or above this severity level will be logged. zap-logger-config.encoding The log encoding format for component logs (defaults to JSON). zap-logger-config.encoderConfig A zap EncoderConfig used to customize record contents. loglevel.<component> Overrides logging level for the given component only. Messages at or above this severity level will be logged. Log levels supported by Zap are: debug - fine-grained debugging info - normal logging warn - unexpected but non-critical errors error - critical errors; unexpected during normal operation dpanic - in debug mode, trigger a panic (crash) panic - trigger a panic (crash) fatal - immediately exit with exit status 1 (failure)","title":"Configuring logging"},{"location":"eventing/observability/logging/config-logging/#configuring-log-settings","text":"Log configuration for all Knative components is managed through the config-logging ConfigMap in the corresponding namespace. For example, Serving components are configured through config-logging in the knative-serving namespace and Eventing components are configured through config-logging in the knative-eventing namespace, etc. Knative components use the zap logging library; options are documented in more detail in that project . In addition to zap-logger-config , which is a general key that applies to all components in that namespace, the config-logging ConfigMap supports overriding the log level for individual components. ConfigMap key Description zap-logger-config A JSON object container for a zap logger configuration. Key fields are highlighted below. zap-logger-config.level The default logging level for components. Messages at or above this severity level will be logged. zap-logger-config.encoding The log encoding format for component logs (defaults to JSON). zap-logger-config.encoderConfig A zap EncoderConfig used to customize record contents. loglevel.<component> Overrides logging level for the given component only. Messages at or above this severity level will be logged. Log levels supported by Zap are: debug - fine-grained debugging info - normal logging warn - unexpected but non-critical errors error - critical errors; unexpected during normal operation dpanic - in debug mode, trigger a panic (crash) panic - trigger a panic (crash) fatal - immediately exit with exit status 1 (failure)","title":"Configuring Log Settings"},{"location":"eventing/observability/metrics/collecting-metrics/","text":"Collecting Metrics in Knative \u00b6 Knative supports different popular tools for collecting metrics: Prometheus OpenTelemetry Collector Grafana dashboards are available for metrics collected directly with Prometheus. You can also set up the OpenTelemetry Collector to receive metrics from Knative components and distribute them to other metrics providers that support OpenTelemetry. Warning You can't use OpenTelemetry Collector and Prometheus at the same time. The default metrics backend is Prometheus. You will need to remove metrics.backend-destination and metrics.request-metrics-backend-destination keys from the config-observability Configmap to enable Prometheus metrics. About Prometheus \u00b6 Prometheus is an open-source tool for collecting, aggregating timeseries metrics and alerting. It can also be used to scrape the OpenTelemetry Collector that is demonstrated below when Prometheus is used. Setting up Prometheus \u00b6 Install the Prometheus Operator by using Helm : helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm install prometheus prometheus-community/kube-prometheus-stack -n default -f values.yaml # values.yaml contains at minimum the configuration below Caution You will need to ensure that the helm chart has following values configured, otherwise the ServiceMonitors/Podmonitors will not work. kube-state-metrics : metricLabelsAllowlist : - pods=[*] - deployments=[app.kubernetes.io/name,app.kubernetes.io/component,app.kubernetes.io/instance] prometheus : prometheusSpec : serviceMonitorSelectorNilUsesHelmValues : false podMonitorSelectorNilUsesHelmValues : false Apply the ServiceMonitors/PodMonitors to collect metrics from Knative. kubectl apply -f https://raw.githubusercontent.com/knative-sandbox/monitoring/main/servicemonitor.yaml Grafana dashboards can be imported from the knative-sandbox repository . If you are using the Grafana Helm Chart with the Dashboard Sidecar enabled, you can load the dashboards by applying the following configmaps. kubectl apply -f https://raw.githubusercontent.com/knative-sandbox/monitoring/main/grafana/dashboards.yaml Access the Prometheus instance locally \u00b6 By default, the Prometheus instance is only exposed on a private service named prometheus-operated . To access the console in your web browser: Enter the command: kubectl port-forward -n default svc/prometheus-operated 9090 Access the console in your browser via http://localhost:9090 . About OpenTelemetry \u00b6 OpenTelemetry is a CNCF observability framework for cloud-native software, which provides a collection of tools, APIs, and SDKs. You can use OpenTelemetry to instrument, generate, collect, and export telemetry data. This data includes metrics, logs, and traces, that you can analyze to understand the performance and behavior of Knative components. OpenTelemetry allows you to easily export metrics to multiple monitoring services without needing to rebuild or reconfigure the Knative binaries. Understanding the collector \u00b6 The collector provides a location where various Knative components can push metrics to be retained and collected by a monitoring service. In the following example, you can configure a single collector instance using a ConfigMap and a Deployment. Tip For more complex deployments, you can automate some of these steps by using the OpenTelemetry Operator . Caution The Grafana dashboards at https://github.com/knative-sandbox/monitoring/tree/main/grafana don't work with metrics scraped from OpenTelemetry Collector. Set up the collector \u00b6 Create a namespace for the collector to run in, by entering the following command: kubectl create namespace metrics The next step uses the metrics namespace for creating the collector. Create a Deployment, Service, and ConfigMap for the collector by entering the following command: kubectl apply -f https://raw.githubusercontent.com/knative/docs/main/docs/serving/observability/metrics/collector.yaml Update the config-observability ConfigMaps in the Knative Serving and Eventing namespaces, by entering the follow command: kubectl patch --namespace knative-serving configmap/config-observability \\ --type merge \\ --patch '{\"data\":{\"metrics.backend-destination\":\"opencensus\",\"metrics.request-metrics-backend-destination\":\"opencensus\",\"metrics.opencensus-address\":\"otel-collector.metrics:55678\"}}' kubectl patch --namespace knative-eventing configmap/config-observability \\ --type merge \\ --patch '{\"data\":{\"metrics.backend-destination\":\"opencensus\",\"metrics.opencensus-address\":\"otel-collector.metrics:55678\"}}' Verify the collector setup \u00b6 You can check that metrics are being forwarded by loading the Prometheus export port on the collector, by entering the following command: kubectl port-forward --namespace metrics deployment/otel-collector 8889 Fetch http://localhost:8889/metrics to see the exported metrics.","title":"Collecting metrics"},{"location":"eventing/observability/metrics/collecting-metrics/#collecting-metrics-in-knative","text":"Knative supports different popular tools for collecting metrics: Prometheus OpenTelemetry Collector Grafana dashboards are available for metrics collected directly with Prometheus. You can also set up the OpenTelemetry Collector to receive metrics from Knative components and distribute them to other metrics providers that support OpenTelemetry. Warning You can't use OpenTelemetry Collector and Prometheus at the same time. The default metrics backend is Prometheus. You will need to remove metrics.backend-destination and metrics.request-metrics-backend-destination keys from the config-observability Configmap to enable Prometheus metrics.","title":"Collecting Metrics in Knative"},{"location":"eventing/observability/metrics/collecting-metrics/#about-prometheus","text":"Prometheus is an open-source tool for collecting, aggregating timeseries metrics and alerting. It can also be used to scrape the OpenTelemetry Collector that is demonstrated below when Prometheus is used.","title":"About Prometheus"},{"location":"eventing/observability/metrics/collecting-metrics/#setting-up-prometheus","text":"Install the Prometheus Operator by using Helm : helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm install prometheus prometheus-community/kube-prometheus-stack -n default -f values.yaml # values.yaml contains at minimum the configuration below Caution You will need to ensure that the helm chart has following values configured, otherwise the ServiceMonitors/Podmonitors will not work. kube-state-metrics : metricLabelsAllowlist : - pods=[*] - deployments=[app.kubernetes.io/name,app.kubernetes.io/component,app.kubernetes.io/instance] prometheus : prometheusSpec : serviceMonitorSelectorNilUsesHelmValues : false podMonitorSelectorNilUsesHelmValues : false Apply the ServiceMonitors/PodMonitors to collect metrics from Knative. kubectl apply -f https://raw.githubusercontent.com/knative-sandbox/monitoring/main/servicemonitor.yaml Grafana dashboards can be imported from the knative-sandbox repository . If you are using the Grafana Helm Chart with the Dashboard Sidecar enabled, you can load the dashboards by applying the following configmaps. kubectl apply -f https://raw.githubusercontent.com/knative-sandbox/monitoring/main/grafana/dashboards.yaml","title":"Setting up Prometheus"},{"location":"eventing/observability/metrics/collecting-metrics/#access-the-prometheus-instance-locally","text":"By default, the Prometheus instance is only exposed on a private service named prometheus-operated . To access the console in your web browser: Enter the command: kubectl port-forward -n default svc/prometheus-operated 9090 Access the console in your browser via http://localhost:9090 .","title":"Access the Prometheus instance locally"},{"location":"eventing/observability/metrics/collecting-metrics/#about-opentelemetry","text":"OpenTelemetry is a CNCF observability framework for cloud-native software, which provides a collection of tools, APIs, and SDKs. You can use OpenTelemetry to instrument, generate, collect, and export telemetry data. This data includes metrics, logs, and traces, that you can analyze to understand the performance and behavior of Knative components. OpenTelemetry allows you to easily export metrics to multiple monitoring services without needing to rebuild or reconfigure the Knative binaries.","title":"About OpenTelemetry"},{"location":"eventing/observability/metrics/collecting-metrics/#understanding-the-collector","text":"The collector provides a location where various Knative components can push metrics to be retained and collected by a monitoring service. In the following example, you can configure a single collector instance using a ConfigMap and a Deployment. Tip For more complex deployments, you can automate some of these steps by using the OpenTelemetry Operator . Caution The Grafana dashboards at https://github.com/knative-sandbox/monitoring/tree/main/grafana don't work with metrics scraped from OpenTelemetry Collector.","title":"Understanding the collector"},{"location":"eventing/observability/metrics/collecting-metrics/#set-up-the-collector","text":"Create a namespace for the collector to run in, by entering the following command: kubectl create namespace metrics The next step uses the metrics namespace for creating the collector. Create a Deployment, Service, and ConfigMap for the collector by entering the following command: kubectl apply -f https://raw.githubusercontent.com/knative/docs/main/docs/serving/observability/metrics/collector.yaml Update the config-observability ConfigMaps in the Knative Serving and Eventing namespaces, by entering the follow command: kubectl patch --namespace knative-serving configmap/config-observability \\ --type merge \\ --patch '{\"data\":{\"metrics.backend-destination\":\"opencensus\",\"metrics.request-metrics-backend-destination\":\"opencensus\",\"metrics.opencensus-address\":\"otel-collector.metrics:55678\"}}' kubectl patch --namespace knative-eventing configmap/config-observability \\ --type merge \\ --patch '{\"data\":{\"metrics.backend-destination\":\"opencensus\",\"metrics.opencensus-address\":\"otel-collector.metrics:55678\"}}'","title":"Set up the collector"},{"location":"eventing/observability/metrics/collecting-metrics/#verify-the-collector-setup","text":"You can check that metrics are being forwarded by loading the Prometheus export port on the collector, by entering the following command: kubectl port-forward --namespace metrics deployment/otel-collector 8889 Fetch http://localhost:8889/metrics to see the exported metrics.","title":"Verify the collector setup"},{"location":"eventing/observability/metrics/eventing-metrics/","text":"Knative Eventing metrics \u00b6 Administrators can view metrics for Knative Eventing components. Broker - Ingress \u00b6 Use the following metrics to debug how broker ingress performs and what events are dispatched via the ingress component. By aggregating the metrics over the http code, events can be separated into two classes, successful (2xx) and failed events (5xx). Metric Name Description Type Tags Unit Status event_count Number of events received by a Broker Counter broker_name event_type namespace_name response_code response_code_class unique_name Dimensionless Stable event_dispatch_latencies The time spent dispatching an event to a Channel Histogram broker_name event_type namespace_name response_code response_code_class unique_name Milliseconds Stable Broker - Filter \u00b6 Use the following metrics to debug how broker filter performs and what events are dispatched via the filter component. Also user can measure the latency of the actual filtering action on an event. By aggregating the metrics over the http code, events can be separated into two classes, successful (2xx) and failed events (5xx). Metric Name Description Type Tags Unit Status event_count Number of events received by a Broker Counter broker_name container_name= filter_type namespace_name response_code response_code_class trigger_name unique_name Dimensionless Stable event_dispatch_latencies The time spent dispatching an event to a Channel Histogram broker_name container_name filter_type namespace_name response_code response_code_class trigger_name unique_name Milliseconds Stable event_processing_latencies The time spent processing an event before it is dispatched to a Trigger subscriber Histogram broker_name container_name filter_type namespace_name trigger_name unique_name Milliseconds Stable In-memory Dispatcher \u00b6 In-memory channel can be evaluated via the following metrics. By aggregating the metrics over the http code, events can be separated into two classes, successful (2xx) and failed events (5xx). Metric Name Description Type Tags Unit Status event_count Number of events dispatched by the in-memory channel Counter container_name event_type= namespace_name= response_code response_code_class unique_name Dimensionless Stable event_dispatch_latencies The time spent dispatching an event from a in-memory Channel Histogram container_name event_type namespace_name= response_code response_code_class unique_name Milliseconds Stable Note A number of metrics eg. controller, Go runtime and others are omitted here as they are common across most components. For more about these metrics check the Serving metrics API section . Eventing sources \u00b6 Eventing sources are created by users who own the related system, so they can trigger applications with events. Every source exposes by default a number of metrics to help user monitor events dispatched. Use the following metrics to verify that events have been delivered from the source side, thus verifying that the source and any connection with the source work as expected. Metric Name Description Type Tags Unit Status event_count Number of events sent by the source Counter event_source event_type name namespace_name resource_group response_code response_code_class response_error response_timeout Dimensionless Stable retry_event_count Number of events sent by the source in retries Counter event_source event_type name namespace_name resource_group response_code response_code_class response_error response_timeout Dimensionless Stable","title":"Knative Eventing metrics"},{"location":"eventing/observability/metrics/eventing-metrics/#knative-eventing-metrics","text":"Administrators can view metrics for Knative Eventing components.","title":"Knative Eventing metrics"},{"location":"eventing/observability/metrics/eventing-metrics/#broker-ingress","text":"Use the following metrics to debug how broker ingress performs and what events are dispatched via the ingress component. By aggregating the metrics over the http code, events can be separated into two classes, successful (2xx) and failed events (5xx). Metric Name Description Type Tags Unit Status event_count Number of events received by a Broker Counter broker_name event_type namespace_name response_code response_code_class unique_name Dimensionless Stable event_dispatch_latencies The time spent dispatching an event to a Channel Histogram broker_name event_type namespace_name response_code response_code_class unique_name Milliseconds Stable","title":"Broker - Ingress"},{"location":"eventing/observability/metrics/eventing-metrics/#broker-filter","text":"Use the following metrics to debug how broker filter performs and what events are dispatched via the filter component. Also user can measure the latency of the actual filtering action on an event. By aggregating the metrics over the http code, events can be separated into two classes, successful (2xx) and failed events (5xx). Metric Name Description Type Tags Unit Status event_count Number of events received by a Broker Counter broker_name container_name= filter_type namespace_name response_code response_code_class trigger_name unique_name Dimensionless Stable event_dispatch_latencies The time spent dispatching an event to a Channel Histogram broker_name container_name filter_type namespace_name response_code response_code_class trigger_name unique_name Milliseconds Stable event_processing_latencies The time spent processing an event before it is dispatched to a Trigger subscriber Histogram broker_name container_name filter_type namespace_name trigger_name unique_name Milliseconds Stable","title":"Broker - Filter"},{"location":"eventing/observability/metrics/eventing-metrics/#in-memory-dispatcher","text":"In-memory channel can be evaluated via the following metrics. By aggregating the metrics over the http code, events can be separated into two classes, successful (2xx) and failed events (5xx). Metric Name Description Type Tags Unit Status event_count Number of events dispatched by the in-memory channel Counter container_name event_type= namespace_name= response_code response_code_class unique_name Dimensionless Stable event_dispatch_latencies The time spent dispatching an event from a in-memory Channel Histogram container_name event_type namespace_name= response_code response_code_class unique_name Milliseconds Stable Note A number of metrics eg. controller, Go runtime and others are omitted here as they are common across most components. For more about these metrics check the Serving metrics API section .","title":"In-memory Dispatcher"},{"location":"eventing/observability/metrics/eventing-metrics/#eventing-sources","text":"Eventing sources are created by users who own the related system, so they can trigger applications with events. Every source exposes by default a number of metrics to help user monitor events dispatched. Use the following metrics to verify that events have been delivered from the source side, thus verifying that the source and any connection with the source work as expected. Metric Name Description Type Tags Unit Status event_count Number of events sent by the source Counter event_source event_type name namespace_name resource_group response_code response_code_class response_error response_timeout Dimensionless Stable retry_event_count Number of events sent by the source in retries Counter event_source event_type name namespace_name resource_group response_code response_code_class response_error response_timeout Dimensionless Stable","title":"Eventing sources"},{"location":"eventing/sinks/","text":"About sinks \u00b6 When you create an event source, you can specify a sink where events are sent to from the source. A sink is an Addressable or a Callable resource that can receive incoming events from other resources. Knative Services, Channels, and Brokers are all examples of sinks. Addressable objects receive and acknowledge an event delivered over HTTP to an address defined in their status.address.url field. As a special case, the core Kubernetes Service object also fulfils the Addressable interface. Callable objects are able to receive an event delivered over HTTP and transform the event, returning 0 or 1 new events in the HTTP response. These returned events may be further processed in the same way that events from an external event source are processed. Sink as a parameter \u00b6 Sink is used as a reference to an object that resolves to a URI to use as the sink. A sink definition supports the following fields: Field Description Required or optional ref This points to an Addressable. Required if not using uri ref.apiVersion API version of the referent. Required if using ref ref.kind Kind of the referent. Required if using ref ref.namespace Namespace of the referent. If omitted this defaults to the object holding it. Optional ref.name Name of the referent. Required if using ref uri This can be an absolute URL with a non-empty scheme and non-empty host that points to the target or a relative URI. Relative URIs are resolved using the base URI retrieved from Ref. Required if not using ref Note At least one of ref or uri is required. If both are specified, uri is resolved into the URL from the Addressable ref result. Sink parameter example \u00b6 Given the following YAML, if ref resolves into \"http://mysink.default.svc.cluster.local\" , then uri is added to this resulting in \"http://mysink.default.svc.cluster.local/extra/path\" . apiVersion : sources.knative.dev/v1 kind : SinkBinding metadata : name : bind-heartbeat spec : ... sink : ref : apiVersion : v1 kind : Service namespace : default name : mysink uri : /extra/path Contract This results in the K_SINK environment variable being set on the subject as \"http://mysink.default.svc.cluster.local/extra/path\" . Using custom resources as sinks \u00b6 To use a Kubernetes custom resource (CR) as a sink for events, you must: Make the CR Addressable. You must ensure that the CR contains a status.address.url . For more information, see the spec for Addressable resources . Create an Addressable-resolver ClusterRole to obtain the necessary RBAC rules for the sink to receive events. For example, you can create a kafkasinks-addressable-resolver ClusterRole to allow get , list , and watch access to KafkaSink objects and statuses: kind : ClusterRole apiVersion : rbac.authorization.k8s.io/v1 metadata : name : kafkasinks-addressable-resolver labels : kafka.eventing.knative.dev/release : devel duck.knative.dev/addressable : \"true\" # Do not use this role directly. These rules will be added to the \"addressable-resolver\" role. rules : - apiGroups : - eventing.knative.dev resources : - kafkasinks - kafkasinks/status verbs : - get - list - watch Filtering events sent to sinks by using Triggers \u00b6 You can connect a Trigger to a sink, so that events are filtered before they are sent to the sink. A sink that is connected to a Trigger is configured as a subscriber in the Trigger resource spec. For example: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : <trigger-name> spec : ... subscriber : ref : apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink name : <kafka-sink-name> Where; <trigger-name> is the name of the Trigger being connected to the sink. <kafka-sink-name> is the name of a KafkaSink object. Specifying sinks using the kn CLI --sink flag \u00b6 When you create an event-producing CR by using the Knative ( kn ) CLI, you can specify a sink where events are sent to from that resource, by using the --sink flag. The following example creates a SinkBinding that uses a Service, http://event-display.svc.cluster.local , as the sink: kn source binding create bind-heartbeat \\ --namespace sinkbinding-example \\ --subject \"Job:batch/v1:app=heartbeat-cron\" \\ --sink http://event-display.svc.cluster.local \\ --ce-override \"sink=bound\" The svc in http://event-display.svc.cluster.local determines that the sink is a Knative Service. Other default sink prefixes include Channel and Broker. Tip You can configure which resources can be used with the --sink flag for kn CLI commands by Customizing kn . Supported third-party sink types \u00b6 Name Maintainer Description KafkaSink Knative Send events to a Kafka topic RedisSink Knative Send events to a Redis Stream","title":"About sinks"},{"location":"eventing/sinks/#about-sinks","text":"When you create an event source, you can specify a sink where events are sent to from the source. A sink is an Addressable or a Callable resource that can receive incoming events from other resources. Knative Services, Channels, and Brokers are all examples of sinks. Addressable objects receive and acknowledge an event delivered over HTTP to an address defined in their status.address.url field. As a special case, the core Kubernetes Service object also fulfils the Addressable interface. Callable objects are able to receive an event delivered over HTTP and transform the event, returning 0 or 1 new events in the HTTP response. These returned events may be further processed in the same way that events from an external event source are processed.","title":"About sinks"},{"location":"eventing/sinks/#sink-as-a-parameter","text":"Sink is used as a reference to an object that resolves to a URI to use as the sink. A sink definition supports the following fields: Field Description Required or optional ref This points to an Addressable. Required if not using uri ref.apiVersion API version of the referent. Required if using ref ref.kind Kind of the referent. Required if using ref ref.namespace Namespace of the referent. If omitted this defaults to the object holding it. Optional ref.name Name of the referent. Required if using ref uri This can be an absolute URL with a non-empty scheme and non-empty host that points to the target or a relative URI. Relative URIs are resolved using the base URI retrieved from Ref. Required if not using ref Note At least one of ref or uri is required. If both are specified, uri is resolved into the URL from the Addressable ref result.","title":"Sink as a parameter"},{"location":"eventing/sinks/#sink-parameter-example","text":"Given the following YAML, if ref resolves into \"http://mysink.default.svc.cluster.local\" , then uri is added to this resulting in \"http://mysink.default.svc.cluster.local/extra/path\" . apiVersion : sources.knative.dev/v1 kind : SinkBinding metadata : name : bind-heartbeat spec : ... sink : ref : apiVersion : v1 kind : Service namespace : default name : mysink uri : /extra/path Contract This results in the K_SINK environment variable being set on the subject as \"http://mysink.default.svc.cluster.local/extra/path\" .","title":"Sink parameter example"},{"location":"eventing/sinks/#using-custom-resources-as-sinks","text":"To use a Kubernetes custom resource (CR) as a sink for events, you must: Make the CR Addressable. You must ensure that the CR contains a status.address.url . For more information, see the spec for Addressable resources . Create an Addressable-resolver ClusterRole to obtain the necessary RBAC rules for the sink to receive events. For example, you can create a kafkasinks-addressable-resolver ClusterRole to allow get , list , and watch access to KafkaSink objects and statuses: kind : ClusterRole apiVersion : rbac.authorization.k8s.io/v1 metadata : name : kafkasinks-addressable-resolver labels : kafka.eventing.knative.dev/release : devel duck.knative.dev/addressable : \"true\" # Do not use this role directly. These rules will be added to the \"addressable-resolver\" role. rules : - apiGroups : - eventing.knative.dev resources : - kafkasinks - kafkasinks/status verbs : - get - list - watch","title":"Using custom resources as sinks"},{"location":"eventing/sinks/#filtering-events-sent-to-sinks-by-using-triggers","text":"You can connect a Trigger to a sink, so that events are filtered before they are sent to the sink. A sink that is connected to a Trigger is configured as a subscriber in the Trigger resource spec. For example: apiVersion : eventing.knative.dev/v1 kind : Trigger metadata : name : <trigger-name> spec : ... subscriber : ref : apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink name : <kafka-sink-name> Where; <trigger-name> is the name of the Trigger being connected to the sink. <kafka-sink-name> is the name of a KafkaSink object.","title":"Filtering events sent to sinks by using Triggers"},{"location":"eventing/sinks/#specifying-sinks-using-the-kn-cli-sink-flag","text":"When you create an event-producing CR by using the Knative ( kn ) CLI, you can specify a sink where events are sent to from that resource, by using the --sink flag. The following example creates a SinkBinding that uses a Service, http://event-display.svc.cluster.local , as the sink: kn source binding create bind-heartbeat \\ --namespace sinkbinding-example \\ --subject \"Job:batch/v1:app=heartbeat-cron\" \\ --sink http://event-display.svc.cluster.local \\ --ce-override \"sink=bound\" The svc in http://event-display.svc.cluster.local determines that the sink is a Knative Service. Other default sink prefixes include Channel and Broker. Tip You can configure which resources can be used with the --sink flag for kn CLI commands by Customizing kn .","title":"Specifying sinks using the kn CLI --sink flag"},{"location":"eventing/sinks/#supported-third-party-sink-types","text":"Name Maintainer Description KafkaSink Knative Send events to a Kafka topic RedisSink Knative Send events to a Redis Stream","title":"Supported third-party sink types"},{"location":"eventing/sinks/kafka-sink/","text":"Apache Kafka Sink \u00b6 This page shows how to install and configure an Apache KafkaSink. Prerequisites \u00b6 You must have access to a Kubernetes cluster with Knative Eventing installed . Installation \u00b6 Install the Kafka controller: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Install the KafkaSink data plane: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-sink.yaml Verify that kafka-controller and kafka-sink-receiver Deployments are running: kubectl get deployments.apps -n knative-eventing Example output: NAME READY UP-TO-DATE AVAILABLE AGE eventing-controller 1 /1 1 1 10s eventing-webhook 1 /1 1 1 9s kafka-controller 1 /1 1 1 3s kafka-sink-receiver 1 /1 1 1 5s KafkaSink example \u00b6 A KafkaSink object looks similar to the following: apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink metadata : name : my-kafka-sink namespace : default spec : topic : mytopic bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 Output Topic Content Mode \u00b6 The CloudEvent specification defines 2 modes to transport a CloudEvent: structured and binary. A \"structured-mode message\" is one where the event is fully encoded using a stand-alone event format and stored in the message body. The structured content mode keeps event metadata and data together in the payload, allowing simple forwarding of the same event across multiple routing hops, and across multiple protocols. A \"binary-mode message\" is one where the event data is stored in the message body, and event attributes are stored as part of message meta-data. The binary content mode accommodates any shape of event data, and allows for efficient transfer and without transcoding effort. A KafkaSink object with a specified contentMode looks similar to the following: apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink metadata : name : my-kafka-sink namespace : default spec : topic : mytopic bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 # CloudEvent content mode of Kafka messages sent to the topic. # Possible values: # - structured # - binary # # default: structured. # # CloudEvent spec references: # - https://github.com/cloudevents/spec/blob/v1.0.1/spec.md#message # - https://github.com/cloudevents/spec/blob/v1.0.1/kafka-protocol-binding.md#33-structured-content-mode # - https://github.com/cloudevents/spec/blob/v1.0.1/kafka-protocol-binding.md#32-binary-content-mode contentMode : binary # or structured Security \u00b6 Knative supports the following Apache Kafka security features: Authentication using SASL without encryption Authentication using SASL and encryption using SSL Authentication and encryption using SSL Encryption using SSL without client authentication Enabling security features \u00b6 To enable security features, in the KafkaSink spec, you can reference a secret: apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink metadata : name : my-kafka-sink namespace : default spec : topic : mytopic bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 auth : secret : ref : name : my_secret Note The secret my_secret must exist in the same namespace of the KafkaSink. Certificates and keys must be in PEM format ._ Authentication using SASL \u00b6 Knative supports the following SASL mechanisms: PLAIN SCRAM-SHA-256 SCRAM-SHA-512 To use a specific SASL mechanism replace <sasl_mechanism> with the mechanism of your choice. Authentication using SASL without encryption \u00b6 kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SASL_PLAINTEXT \\ --from-literal = sasl.mechanism = <sasl_mechanism> \\ --from-literal = user = <my_user> \\ --from-literal = password = <my_password> Authentication using SASL and encryption using SSL \u00b6 kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SASL_SSL \\ --from-literal = sasl.mechanism = <sasl_mechanism> \\ --from-file = ca.crt = caroot.pem \\ --from-literal = user = <my_user> \\ --from-literal = password = <my_password> Encryption using SSL without client authentication \u00b6 kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SSL \\ --from-file = ca.crt = <my_caroot.pem_file_path> \\ --from-literal = user.skip = true Authentication and encryption using SSL \u00b6 kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SSL \\ --from-file = ca.crt = <my_caroot.pem_file_path> \\ --from-file = user.crt = <my_cert.pem_file_path> \\ --from-file = user.key = <my_key.pem_file_path> Note The ca.crt can be omitted to enable fallback and use the system's root CA set. Kafka Producer configurations \u00b6 A Kafka Producer is the component responsible for sending events to the Apache Kafka cluster. You can change the configuration for Kafka Producers in your cluster by modifying the config-kafka-sink-data-plane ConfigMap in the knative-eventing namespace. Documentation for the settings available in this ConfigMap is available on the Apache Kafka website , in particular, Producer configurations . Enable debug logging for data plane components \u00b6 To enable debug logging for data plane components change the logging level to DEBUG in the kafka-config-logging ConfigMap. Create the kafka-config-logging ConfigMap as a YAML file that contains the following: apiVersion : v1 kind : ConfigMap metadata : name : kafka-config-logging namespace : knative-eventing data : config.xml : | <configuration> <appender name=\"jsonConsoleAppender\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder class=\"net.logstash.logback.encoder.LogstashEncoder\"/> </appender> <root level=\"DEBUG\"> <appender-ref ref=\"jsonConsoleAppender\"/> </root> </configuration> Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Restart the kafka-sink-receiver : kubectl rollout restart deployment -n knative-eventing kafka-sink-receiver","title":"KafkaSink"},{"location":"eventing/sinks/kafka-sink/#apache-kafka-sink","text":"This page shows how to install and configure an Apache KafkaSink.","title":"Apache Kafka Sink"},{"location":"eventing/sinks/kafka-sink/#prerequisites","text":"You must have access to a Kubernetes cluster with Knative Eventing installed .","title":"Prerequisites"},{"location":"eventing/sinks/kafka-sink/#installation","text":"Install the Kafka controller: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Install the KafkaSink data plane: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-sink.yaml Verify that kafka-controller and kafka-sink-receiver Deployments are running: kubectl get deployments.apps -n knative-eventing Example output: NAME READY UP-TO-DATE AVAILABLE AGE eventing-controller 1 /1 1 1 10s eventing-webhook 1 /1 1 1 9s kafka-controller 1 /1 1 1 3s kafka-sink-receiver 1 /1 1 1 5s","title":"Installation"},{"location":"eventing/sinks/kafka-sink/#kafkasink-example","text":"A KafkaSink object looks similar to the following: apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink metadata : name : my-kafka-sink namespace : default spec : topic : mytopic bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092","title":"KafkaSink example"},{"location":"eventing/sinks/kafka-sink/#output-topic-content-mode","text":"The CloudEvent specification defines 2 modes to transport a CloudEvent: structured and binary. A \"structured-mode message\" is one where the event is fully encoded using a stand-alone event format and stored in the message body. The structured content mode keeps event metadata and data together in the payload, allowing simple forwarding of the same event across multiple routing hops, and across multiple protocols. A \"binary-mode message\" is one where the event data is stored in the message body, and event attributes are stored as part of message meta-data. The binary content mode accommodates any shape of event data, and allows for efficient transfer and without transcoding effort. A KafkaSink object with a specified contentMode looks similar to the following: apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink metadata : name : my-kafka-sink namespace : default spec : topic : mytopic bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 # CloudEvent content mode of Kafka messages sent to the topic. # Possible values: # - structured # - binary # # default: structured. # # CloudEvent spec references: # - https://github.com/cloudevents/spec/blob/v1.0.1/spec.md#message # - https://github.com/cloudevents/spec/blob/v1.0.1/kafka-protocol-binding.md#33-structured-content-mode # - https://github.com/cloudevents/spec/blob/v1.0.1/kafka-protocol-binding.md#32-binary-content-mode contentMode : binary # or structured","title":"Output Topic Content Mode"},{"location":"eventing/sinks/kafka-sink/#security","text":"Knative supports the following Apache Kafka security features: Authentication using SASL without encryption Authentication using SASL and encryption using SSL Authentication and encryption using SSL Encryption using SSL without client authentication","title":"Security"},{"location":"eventing/sinks/kafka-sink/#enabling-security-features","text":"To enable security features, in the KafkaSink spec, you can reference a secret: apiVersion : eventing.knative.dev/v1alpha1 kind : KafkaSink metadata : name : my-kafka-sink namespace : default spec : topic : mytopic bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 auth : secret : ref : name : my_secret Note The secret my_secret must exist in the same namespace of the KafkaSink. Certificates and keys must be in PEM format ._","title":"Enabling security features"},{"location":"eventing/sinks/kafka-sink/#authentication-using-sasl","text":"Knative supports the following SASL mechanisms: PLAIN SCRAM-SHA-256 SCRAM-SHA-512 To use a specific SASL mechanism replace <sasl_mechanism> with the mechanism of your choice.","title":"Authentication using SASL"},{"location":"eventing/sinks/kafka-sink/#authentication-using-sasl-without-encryption","text":"kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SASL_PLAINTEXT \\ --from-literal = sasl.mechanism = <sasl_mechanism> \\ --from-literal = user = <my_user> \\ --from-literal = password = <my_password>","title":"Authentication using SASL without encryption"},{"location":"eventing/sinks/kafka-sink/#authentication-using-sasl-and-encryption-using-ssl","text":"kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SASL_SSL \\ --from-literal = sasl.mechanism = <sasl_mechanism> \\ --from-file = ca.crt = caroot.pem \\ --from-literal = user = <my_user> \\ --from-literal = password = <my_password>","title":"Authentication using SASL and encryption using SSL"},{"location":"eventing/sinks/kafka-sink/#encryption-using-ssl-without-client-authentication","text":"kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SSL \\ --from-file = ca.crt = <my_caroot.pem_file_path> \\ --from-literal = user.skip = true","title":"Encryption using SSL without client authentication"},{"location":"eventing/sinks/kafka-sink/#authentication-and-encryption-using-ssl","text":"kubectl create secret --namespace <namespace> generic <my_secret> \\ --from-literal = protocol = SSL \\ --from-file = ca.crt = <my_caroot.pem_file_path> \\ --from-file = user.crt = <my_cert.pem_file_path> \\ --from-file = user.key = <my_key.pem_file_path> Note The ca.crt can be omitted to enable fallback and use the system's root CA set.","title":"Authentication and encryption using SSL"},{"location":"eventing/sinks/kafka-sink/#kafka-producer-configurations","text":"A Kafka Producer is the component responsible for sending events to the Apache Kafka cluster. You can change the configuration for Kafka Producers in your cluster by modifying the config-kafka-sink-data-plane ConfigMap in the knative-eventing namespace. Documentation for the settings available in this ConfigMap is available on the Apache Kafka website , in particular, Producer configurations .","title":"Kafka Producer configurations"},{"location":"eventing/sinks/kafka-sink/#enable-debug-logging-for-data-plane-components","text":"To enable debug logging for data plane components change the logging level to DEBUG in the kafka-config-logging ConfigMap. Create the kafka-config-logging ConfigMap as a YAML file that contains the following: apiVersion : v1 kind : ConfigMap metadata : name : kafka-config-logging namespace : knative-eventing data : config.xml : | <configuration> <appender name=\"jsonConsoleAppender\" class=\"ch.qos.logback.core.ConsoleAppender\"> <encoder class=\"net.logstash.logback.encoder.LogstashEncoder\"/> </appender> <root level=\"DEBUG\"> <appender-ref ref=\"jsonConsoleAppender\"/> </root> </configuration> Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Restart the kafka-sink-receiver : kubectl rollout restart deployment -n knative-eventing kafka-sink-receiver","title":"Enable debug logging for data plane components"},{"location":"eventing/sources/","text":"Event sources \u00b6 An event source is a Kubernetes custom resource (CR), created by a developer or cluster administrator, that acts as a link between an event producer and an event sink . A sink can be a k8s service, including Knative Services, a Channel, or a Broker that receives events from an event source. Event sources are created by instantiating a CR from a Source object. The Source object defines the arguments and parameters needed to instantiate a CR. All Sources are part of the sources category. kn kubectl You can list existing event sources on your cluster by entering the kn command: kn source list You can list existing event sources on your cluster by entering the command: kubectl get sources Note Event Sources that import events from other messaging technologies such as Kafka or RabbitMQ are not responsible for setting Optional Attributes such as the datacontenttype . This is a responsibility of the original event producer; the Source just appends attributes if they exist. Knative Sources \u00b6 Name API Version Maintainer Description APIServerSource v1 Knative Brings Kubernetes API server events into Knative. The APIServerSource fires a new event each time a Kubernetes resource is created, updated or deleted. Apache Camel N/A Apache Software Foundation Enables use of Apache Camel components for pushing events into Knative. Camel sources are now provided via Kamelets as part of the Apache Camel K project. Apache CouchDB v1alpha1 Knative Brings Apache CouchDB messages into Knative. Apache Kafka v1beta1 Knative Brings Apache Kafka messages into Knative. The KafkaSource reads events from an Apache Kafka Cluster, and passes these events to a sink so that they can be consumed. See the Kafka Source example for more details. ContainerSource v1 Knative The ContainerSource instantiates container image(s) that can generate events until the ContainerSource is deleted. This may be used, for example, to poll an FTP server for new files or generate events at a set time interval. Given a spec.template with at least a container image specified, the ContainerSource keeps a Pod running with the specified image(s). K_SINK (destination address) and KE_CE_OVERRIDES (JSON CloudEvents attributes) environment variables are injected into the running image(s). It is used by multiple other Sources as underlying infrastructure. Refer to the Container Source example for more details. GitHub v1alpha1 Knative Registers for events of the specified types on the specified GitHub organization or repository, and brings those events into Knative. The GitHubSource fires a new event for selected GitHub event types . See the GitHub Source example for more details. GitLab v1alpha1 Knative Registers for events of the specified types on the specified GitLab repository, and brings those events into Knative. The GitLabSource creates a webhooks for specified event types , listens for incoming events, and passes them to a consumer. See the GitLab Source example for more details. Heartbeats N/A Knative Uses an in-memory timer to produce events at the specified interval. KogitoSource v1alpha1 Knative An implementation of the Kogito Runtime custom resource managed by the Kogito Operator . PingSource v1beta2 Knative Produces events with a fixed payload on a specified Cron schedule. See the Ping Source example for more details. RabbitMQ Active development Knative Brings RabbitMQ messages into Knative. SinkBinding v1 Knative The SinkBinding can be used to author new event sources using any of the familiar compute abstractions that Kubernetes makes available (e.g. Deployment, Job, DaemonSet, StatefulSet), or Knative abstractions (e.g. Service, Configuration). SinkBinding provides a framework for injecting K_SINK (destination address) and K_CE_OVERRIDES (JSON cloudevents attributes) environment variables into any Kubernetes resource which has a spec.template that looks like a Pod (aka PodSpecable). See the SinkBinding example for more details. WebSocket N/A Knative Opens a WebSocket to the specified source and packages each received message as a Knative event. Third-Party Sources \u00b6 Name API Version Maintainer Description Amazon CloudWatch Supported TriggerMesh Collects metrics from Amazon CloudWatch . ( installation ) ( example ) Amazon CloudWatch Logs Supported TriggerMesh Subscribes to log events from an Amazon CloudWatch Logs stream. ( installation ) ( example ) AWS CodeCommit Supported TriggerMesh Registers for events emitted by an AWS CodeCommit source code repository. ( installation ) ( example ) Amazon Cognito Identity Supported TriggerMesh Registers for events from Amazon Cognito identity pools. ( installation ) ( example ) Amazon Cognito User Supported TriggerMesh Registers for events from Amazon Cognito user pools. ( installation ) ( example ) Amazon DynamoDB Supported TriggerMesh Reads records from an Amazon DynamoDB stream. ( installation ) ( example ) Amazon Kinesis Supported TriggerMesh Reads records from an Amazon Kinesis stream. ( installation ) ( example ) Amazon RDS Performance Insights Supported TriggerMesh Subscribes to metrics from Amazon RDS Performance Insights . ( installation ) ( example ) Amazon S3 Supported TriggerMesh Subscribes to event notifications from an Amazon S3 bucket. ( installation ) ( example ) Amazon SNS Supported TriggerMesh Subscribes to messages from an Amazon SNS topic. ( installation ) ( example ) Amazon SQS Supported TriggerMesh Consumes messages from an Amazon SQS queue. ( installation ) ( example ) Auto Container Source Proof of Concept None AutoContainerSource is a controller that allows the Source CRDs without needing a controller. It notices CRDs with a specific label and starts controlling resources of that type. It utilizes Container Source as underlying infrastructure. Azure Activity Logs Supported TriggerMesh Capture activity logs from Azure Activity Logs . ( installation ) ( example ) Azure Blob Storage Supported TriggerMesh Subscribes to events from an Azure Blob Storage account. ( installation ) ( example ) Azure Event Grid Supported TriggerMesh Retrieves events from Azure Event Grid . ( installation ) ( example ) Azure Event Hubs Supported TriggerMesh Consumes events from Azure Event Hubs . ( installation ) ( example ) Azure IoT Hub Supported TriggerMesh Consumes event from Azure IoT Hub . ( installation ) ( example ) Azure Queue Storage Supported TriggerMesh Retrieves messages from Azure Queue Storage . ( installation ) ( example ) Azure Service Bus Queues Supported TriggerMesh Consumes messages from an Azure Service Bus queue. ( installation ) ( example ) Azure Service Bus Topics Supported TriggerMesh Subscribes to messages from an Azure Service Bus topic. ( installation ) ( example ) BitBucket Proof of Concept None Registers for events of the specified types on the specified BitBucket organization/repository. Brings those events into Knative. CloudAuditLogsSource v1 Google Registers for events of the specified types on the specified Google Cloud Audit Logs . Brings those events into Knative. Refer to the CloudAuditLogsSource example for more details. CloudPubSubSource v1 Google Brings Cloud Pub/Sub messages into Knative. The CloudPubSubSource fires a new event each time a message is published on a Google Cloud Platform PubSub topic . See the CloudPubSubSource example for more details. CloudSchedulerSource v1 Google Create, update, and delete Google Cloud Scheduler Jobs. When those jobs are triggered, receive the event inside Knative. See the CloudSchedulerSource example for further details. CloudStorageSource v1 Google Registers for events of the specified types on the specified Google Cloud Storage bucket and optional object prefix. Brings those events into Knative. See the CloudStorageSource example. Direktiv Proof of concept Direktiv Receive events from Direktiv . DockerHubSource v1alpha1 None Retrieves events from Docker Hub Webhooks and transforms them into CloudEvents for consumption in Knative. FTP / SFTP Proof of concept None Watches for files being uploaded into a FTP/SFTP and generates events for those. GitHub Issue Comments Proof of Concept None Polls a specific GitHub issue for new comments. Google Cloud Audit Logs Supported TriggerMesh Captures audit logs from Google Cloud Audit Logs . ( installation ) ( example ) Google Cloud Billing Supported TriggerMesh Captures budget notifications from Google Cloud Billing . ( installation ) ( example ) Google Cloud IoT Supported TriggerMesh Subscribes to messages from a Google Cloud IoT registry. ( installation ) ( example ) Google Cloud Pub/Sub Supported TriggerMesh Subscribes to messages from a Google Cloud Pub/Sub topic. ( installation ) ( example ) Google Cloud Source Repositories Supported TriggerMesh Consumes events from Google Cloud Source Repositories . ( installation ) ( example ) Google Cloud Storage Supported TriggerMesh Captures change notifications from a Google Cloud Storage bucket. ( installation ) ( example ) HTTP Poller Supported TriggerMesh Periodically pulls events from an HTTP/S URL. ( installation ) ( example ) Heartbeat Proof of Concept None Uses an in-memory timer to produce events as the specified interval. Uses AutoContainerSource for underlying infrastructure. K8s Proof of Concept None Brings Kubernetes cluster events into Knative. Uses AutoContainerSource for underlying infrastructure. Konnek Active Development None Retrieves events from cloud platforms (like AWS and GCP) and transforms them into CloudEvents for consumption in Knative. Oracle Cloud Infrastructure Supported TriggerMesh Retrieves metrics from Oracle Cloud Infrastructure . ( installation ) ( example ) RedisSource v1alpha1 None Brings Redis Stream into Knative. Salesforce Supported TriggerMesh Consumes events from a Salesforce channel. ( installation ) ( example ) Slack Supported TriggerMesh Subscribes to events from Slack . ( installation ) ( example ) SNMP Proof of concept Direktiv Receive events via SNMP. Twilio Supported TriggerMesh Receive events from Twilio . ( installation ) ( example ) VMware Active Development VMware Brings vSphere events into Knative. Webhook Supported TriggerMesh Ingest events from a webhook using HTTP. ( installation ) ( example ) Zendesk Supported TriggerMesh Subscribes to events from Zendesk. ( installation ) ( example ) Additional resources \u00b6 If your code needs to send events as part of its business logic and doesn't fit the model of a Source, consider feeding events directly to a Broker . For more information about using kn Source related commands, see the kn source reference documentation .","title":"About event sources"},{"location":"eventing/sources/#event-sources","text":"An event source is a Kubernetes custom resource (CR), created by a developer or cluster administrator, that acts as a link between an event producer and an event sink . A sink can be a k8s service, including Knative Services, a Channel, or a Broker that receives events from an event source. Event sources are created by instantiating a CR from a Source object. The Source object defines the arguments and parameters needed to instantiate a CR. All Sources are part of the sources category. kn kubectl You can list existing event sources on your cluster by entering the kn command: kn source list You can list existing event sources on your cluster by entering the command: kubectl get sources Note Event Sources that import events from other messaging technologies such as Kafka or RabbitMQ are not responsible for setting Optional Attributes such as the datacontenttype . This is a responsibility of the original event producer; the Source just appends attributes if they exist.","title":"Event sources"},{"location":"eventing/sources/#knative-sources","text":"Name API Version Maintainer Description APIServerSource v1 Knative Brings Kubernetes API server events into Knative. The APIServerSource fires a new event each time a Kubernetes resource is created, updated or deleted. Apache Camel N/A Apache Software Foundation Enables use of Apache Camel components for pushing events into Knative. Camel sources are now provided via Kamelets as part of the Apache Camel K project. Apache CouchDB v1alpha1 Knative Brings Apache CouchDB messages into Knative. Apache Kafka v1beta1 Knative Brings Apache Kafka messages into Knative. The KafkaSource reads events from an Apache Kafka Cluster, and passes these events to a sink so that they can be consumed. See the Kafka Source example for more details. ContainerSource v1 Knative The ContainerSource instantiates container image(s) that can generate events until the ContainerSource is deleted. This may be used, for example, to poll an FTP server for new files or generate events at a set time interval. Given a spec.template with at least a container image specified, the ContainerSource keeps a Pod running with the specified image(s). K_SINK (destination address) and KE_CE_OVERRIDES (JSON CloudEvents attributes) environment variables are injected into the running image(s). It is used by multiple other Sources as underlying infrastructure. Refer to the Container Source example for more details. GitHub v1alpha1 Knative Registers for events of the specified types on the specified GitHub organization or repository, and brings those events into Knative. The GitHubSource fires a new event for selected GitHub event types . See the GitHub Source example for more details. GitLab v1alpha1 Knative Registers for events of the specified types on the specified GitLab repository, and brings those events into Knative. The GitLabSource creates a webhooks for specified event types , listens for incoming events, and passes them to a consumer. See the GitLab Source example for more details. Heartbeats N/A Knative Uses an in-memory timer to produce events at the specified interval. KogitoSource v1alpha1 Knative An implementation of the Kogito Runtime custom resource managed by the Kogito Operator . PingSource v1beta2 Knative Produces events with a fixed payload on a specified Cron schedule. See the Ping Source example for more details. RabbitMQ Active development Knative Brings RabbitMQ messages into Knative. SinkBinding v1 Knative The SinkBinding can be used to author new event sources using any of the familiar compute abstractions that Kubernetes makes available (e.g. Deployment, Job, DaemonSet, StatefulSet), or Knative abstractions (e.g. Service, Configuration). SinkBinding provides a framework for injecting K_SINK (destination address) and K_CE_OVERRIDES (JSON cloudevents attributes) environment variables into any Kubernetes resource which has a spec.template that looks like a Pod (aka PodSpecable). See the SinkBinding example for more details. WebSocket N/A Knative Opens a WebSocket to the specified source and packages each received message as a Knative event.","title":"Knative Sources"},{"location":"eventing/sources/#third-party-sources","text":"Name API Version Maintainer Description Amazon CloudWatch Supported TriggerMesh Collects metrics from Amazon CloudWatch . ( installation ) ( example ) Amazon CloudWatch Logs Supported TriggerMesh Subscribes to log events from an Amazon CloudWatch Logs stream. ( installation ) ( example ) AWS CodeCommit Supported TriggerMesh Registers for events emitted by an AWS CodeCommit source code repository. ( installation ) ( example ) Amazon Cognito Identity Supported TriggerMesh Registers for events from Amazon Cognito identity pools. ( installation ) ( example ) Amazon Cognito User Supported TriggerMesh Registers for events from Amazon Cognito user pools. ( installation ) ( example ) Amazon DynamoDB Supported TriggerMesh Reads records from an Amazon DynamoDB stream. ( installation ) ( example ) Amazon Kinesis Supported TriggerMesh Reads records from an Amazon Kinesis stream. ( installation ) ( example ) Amazon RDS Performance Insights Supported TriggerMesh Subscribes to metrics from Amazon RDS Performance Insights . ( installation ) ( example ) Amazon S3 Supported TriggerMesh Subscribes to event notifications from an Amazon S3 bucket. ( installation ) ( example ) Amazon SNS Supported TriggerMesh Subscribes to messages from an Amazon SNS topic. ( installation ) ( example ) Amazon SQS Supported TriggerMesh Consumes messages from an Amazon SQS queue. ( installation ) ( example ) Auto Container Source Proof of Concept None AutoContainerSource is a controller that allows the Source CRDs without needing a controller. It notices CRDs with a specific label and starts controlling resources of that type. It utilizes Container Source as underlying infrastructure. Azure Activity Logs Supported TriggerMesh Capture activity logs from Azure Activity Logs . ( installation ) ( example ) Azure Blob Storage Supported TriggerMesh Subscribes to events from an Azure Blob Storage account. ( installation ) ( example ) Azure Event Grid Supported TriggerMesh Retrieves events from Azure Event Grid . ( installation ) ( example ) Azure Event Hubs Supported TriggerMesh Consumes events from Azure Event Hubs . ( installation ) ( example ) Azure IoT Hub Supported TriggerMesh Consumes event from Azure IoT Hub . ( installation ) ( example ) Azure Queue Storage Supported TriggerMesh Retrieves messages from Azure Queue Storage . ( installation ) ( example ) Azure Service Bus Queues Supported TriggerMesh Consumes messages from an Azure Service Bus queue. ( installation ) ( example ) Azure Service Bus Topics Supported TriggerMesh Subscribes to messages from an Azure Service Bus topic. ( installation ) ( example ) BitBucket Proof of Concept None Registers for events of the specified types on the specified BitBucket organization/repository. Brings those events into Knative. CloudAuditLogsSource v1 Google Registers for events of the specified types on the specified Google Cloud Audit Logs . Brings those events into Knative. Refer to the CloudAuditLogsSource example for more details. CloudPubSubSource v1 Google Brings Cloud Pub/Sub messages into Knative. The CloudPubSubSource fires a new event each time a message is published on a Google Cloud Platform PubSub topic . See the CloudPubSubSource example for more details. CloudSchedulerSource v1 Google Create, update, and delete Google Cloud Scheduler Jobs. When those jobs are triggered, receive the event inside Knative. See the CloudSchedulerSource example for further details. CloudStorageSource v1 Google Registers for events of the specified types on the specified Google Cloud Storage bucket and optional object prefix. Brings those events into Knative. See the CloudStorageSource example. Direktiv Proof of concept Direktiv Receive events from Direktiv . DockerHubSource v1alpha1 None Retrieves events from Docker Hub Webhooks and transforms them into CloudEvents for consumption in Knative. FTP / SFTP Proof of concept None Watches for files being uploaded into a FTP/SFTP and generates events for those. GitHub Issue Comments Proof of Concept None Polls a specific GitHub issue for new comments. Google Cloud Audit Logs Supported TriggerMesh Captures audit logs from Google Cloud Audit Logs . ( installation ) ( example ) Google Cloud Billing Supported TriggerMesh Captures budget notifications from Google Cloud Billing . ( installation ) ( example ) Google Cloud IoT Supported TriggerMesh Subscribes to messages from a Google Cloud IoT registry. ( installation ) ( example ) Google Cloud Pub/Sub Supported TriggerMesh Subscribes to messages from a Google Cloud Pub/Sub topic. ( installation ) ( example ) Google Cloud Source Repositories Supported TriggerMesh Consumes events from Google Cloud Source Repositories . ( installation ) ( example ) Google Cloud Storage Supported TriggerMesh Captures change notifications from a Google Cloud Storage bucket. ( installation ) ( example ) HTTP Poller Supported TriggerMesh Periodically pulls events from an HTTP/S URL. ( installation ) ( example ) Heartbeat Proof of Concept None Uses an in-memory timer to produce events as the specified interval. Uses AutoContainerSource for underlying infrastructure. K8s Proof of Concept None Brings Kubernetes cluster events into Knative. Uses AutoContainerSource for underlying infrastructure. Konnek Active Development None Retrieves events from cloud platforms (like AWS and GCP) and transforms them into CloudEvents for consumption in Knative. Oracle Cloud Infrastructure Supported TriggerMesh Retrieves metrics from Oracle Cloud Infrastructure . ( installation ) ( example ) RedisSource v1alpha1 None Brings Redis Stream into Knative. Salesforce Supported TriggerMesh Consumes events from a Salesforce channel. ( installation ) ( example ) Slack Supported TriggerMesh Subscribes to events from Slack . ( installation ) ( example ) SNMP Proof of concept Direktiv Receive events via SNMP. Twilio Supported TriggerMesh Receive events from Twilio . ( installation ) ( example ) VMware Active Development VMware Brings vSphere events into Knative. Webhook Supported TriggerMesh Ingest events from a webhook using HTTP. ( installation ) ( example ) Zendesk Supported TriggerMesh Subscribes to events from Zendesk. ( installation ) ( example )","title":"Third-Party Sources"},{"location":"eventing/sources/#additional-resources","text":"If your code needs to send events as part of its business logic and doesn't fit the model of a Source, consider feeding events directly to a Broker . For more information about using kn Source related commands, see the kn source reference documentation .","title":"Additional resources"},{"location":"eventing/sources/apache-camel-source/","text":"Apache Camel source \u00b6 Apache Camel based sources are now provided via Kamelets as part of the Apache Camel K project. Kamelets are a new technology that allow anyone to pick and use event sources and sinks from a catalog , without having to dig into the internal details of Apache Camel. The old Knative CamelSource controller has been archived.","title":"CamelSource"},{"location":"eventing/sources/apache-camel-source/#apache-camel-source","text":"Apache Camel based sources are now provided via Kamelets as part of the Apache Camel K project. Kamelets are a new technology that allow anyone to pick and use event sources and sinks from a catalog , without having to dig into the internal details of Apache Camel. The old Knative CamelSource controller has been archived.","title":"Apache Camel source"},{"location":"eventing/sources/apiserversource/","text":"ApiServerSource \u00b6 The API server source is a Knative Eventing Kubernetes custom resource that listens for events emitted by the Kubernetes API server (eg. pod creation, deployment updates, etc...) and forwards them as CloudEvents to a sink. The API server source is part of the core Knative Eventing component, and is provided by default when Knative Eventing is installed. Multiple instances of an ApiServerSource object can be created by users.","title":"About ApiServerSource"},{"location":"eventing/sources/apiserversource/#apiserversource","text":"The API server source is a Knative Eventing Kubernetes custom resource that listens for events emitted by the Kubernetes API server (eg. pod creation, deployment updates, etc...) and forwards them as CloudEvents to a sink. The API server source is part of the core Knative Eventing component, and is provided by default when Knative Eventing is installed. Multiple instances of an ApiServerSource object can be created by users.","title":"ApiServerSource"},{"location":"eventing/sources/apiserversource/getting-started/","text":"Creating an ApiServerSource object \u00b6 This topic describes how to create an ApiServerSource object. Before you begin \u00b6 Before you can create an ApiServerSource object: You must have Knative Eventing installed on your cluster. You must install the kubectl CLI tool. Optional: If you want to use the kn commands, install the kn tool. Create an ApiServerSource object \u00b6 Optional: Create a namespace for the API server source instance by running the command: kubectl create namespace <namespace> Where <namespace> is the name of the namespace that you want to create. Note Creating a namespace for your ApiServerSource and related components allows you to view changes and events for this workflow more easily, because these are isolated from the other components that might exist in your default namespace. It also makes removing the source easier, because you can delete the namespace to remove all of the resources. Create a ServiceAccount: Create a YAML file using the following template: apiVersion : v1 kind : ServiceAccount metadata : name : <service-account> namespace : <namespace> Where: <service-account> is the name of the ServiceAccount that you want to create. <namespace> is the namespace that you created in step 1 earlier. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Create a Role: Create a YAML file using the following template: apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : <role> namespace : <namespace> rules : <rules> Where: <role> is the name of the Role that you want to create. <namespace> is the name of the namespace that you created in step 1 earlier. <rules> are the set of permissions you want to grant to the APIServerSource object. This set of permissions must match the resources you want to receive events from. For example, to receive events related to the events resource, use the following set of permissions: - apiGroups : - \"\" resources : - events verbs : - get - list - watch Note The only required verbs are get , list and watch . Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Create a RoleBinding: Create a YAML file using the following template: apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : <role-binding> namespace : <namespace> roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : <role> subjects : - kind : ServiceAccount name : <service-account> namespace : <namespace> Where: <role-binding> is the name of the RoleBinding that you want to create. <namespace> is the name of the namespace that you created in step 1 earlier. <role> is the name of the Role that you created in step 3 earlier. <service-account> is the name of the ServiceAccount that you created in step 2 earlier. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Create a sink. If you do not have your own sink, you can use the following example Service that dumps incoming messages to a log: Copy the YAML below into a file: apiVersion : apps/v1 kind : Deployment metadata : name : event-display namespace : <namespace> spec : replicas : 1 selector : matchLabels : &labels app : event-display template : metadata : labels : *labels spec : containers : - name : event-display image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- kind : Service apiVersion : v1 metadata : name : event-display namespace : <namespace> spec : selector : app : event-display ports : - protocol : TCP port : 80 targetPort : 8080 Where <namespace> is the name of the namespace that you created in step 1 above. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Create the ApiServerSource object: kn YAML To create the ApiServerSource, run the command: kn source apiserver create <apiserversource> \\ --namespace <namespace> \\ --mode \"Resource\" \\ --resource \"Event:v1\" \\ --service-account <service-account> \\ --sink <sink-name> Where: <apiserversource> is the name of the source that you want to create. <namespace> is the name of the namespace that you created in step 1 earlier. <service-account> is the name of the ServiceAccount that you created in step 2 earlier. <sink-name> is the name of your sink, for example, http://event-display.pingsource-example.svc.cluster.local . For a list of available options, see the Knative client documentation . Create a YAML file using the following template: apiVersion : sources.knative.dev/v1 kind : ApiServerSource metadata : name : <apiserversource-name> namespace : <namespace> spec : serviceAccountName : <service-account> mode : <event-mode> resources : - apiVersion : v1 kind : Event sink : ref : apiVersion : v1 kind : <sink-kind> name : <sink-name> Where: <apiserversource-name> is the name of the source that you want to create. <namespace> is the name of the namespace that you created in step 1 earlier. <service-account> is the name of the ServiceAccount that you created in step 2 earlier. <event-mode> is either Resource or Reference . If set to Resource , the event payload contains the entire resource that the event is for. If set to Reference , the event payload only contains a reference to the resource that the event is for. The default is Reference . <sink-kind> is any supported Addressable object that you want to use as a sink, for example, Service or Deployment . <sink-name> is the name of your sink. For more information about the fields you can configure for the ApiServerSource object, see ApiServerSource reference . Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Verify the ApiServerSource object \u00b6 Make the Kubernetes API server create events by launching a test Pod in your namespace by running the command: kubectl run busybox --image = busybox --namespace = <namespace> --restart = Never -- ls Where <namespace> is the name of the namespace that you created in step 1 earlier. Delete the test Pod by running the command: kubectl --namespace = <namespace> delete pod busybox Where <namespace> is the name of the namespace that you created in step 1 earlier. View the logs to verify that Kubernetes events were sent to the sink by the Knative Eventing system by running the command: kubectl logs --namespace = <namespace> -l app = <sink> --tail = 100 Where: <namespace> is the name of the namespace that you created in step 1 earlier. <sink> is the name of the PodSpecable object that you used as a sink in step 5 earlier. Example log output: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.apiserver.resource.update source: https://10.96.0.1:443 subject: /apis/v1/namespaces/apiserversource-example/events/testevents.15dd3050eb1e6f50 id: e0447eb7-36b5-443b-9d37-faf4fe5c62f0 time: 2020 -07-28T19:14:54.719501054Z datacontenttype: application/json Extensions, kind: Event name: busybox.1626008649e617e3 namespace: apiserversource-example Data, { \"apiVersion\" : \"v1\" , \"count\" : 1 , \"eventTime\" : null, \"firstTimestamp\" : \"2020-07-28T19:14:54Z\" , \"involvedObject\" : { \"apiVersion\" : \"v1\" , \"fieldPath\" : \"spec.containers{busybox}\" , \"kind\" : \"Pod\" , \"name\" : \"busybox\" , \"namespace\" : \"apiserversource-example\" , \"resourceVersion\" : \"28987493\" , \"uid\" : \"1efb342a-737b-11e9-a6c5-42010a8a00ed\" } , \"kind\" : \"Event\" , \"lastTimestamp\" : \"2020-07-28T19:14:54Z\" , \"message\" : \"Started container\" , \"metadata\" : { \"creationTimestamp\" : \"2020-07-28T19:14:54Z\" , \"name\" : \"busybox.1626008649e617e3\" , \"namespace\" : \"default\" , \"resourceVersion\" : \"506088\" , \"selfLink\" : \"/api/v1/namespaces/apiserversource-example/events/busybox.1626008649e617e3\" , \"uid\" : \"2005af47-737b-11e9-a6c5-42010a8a00ed\" } , \"reason\" : \"Started\" , \"reportingComponent\" : \"\" , \"reportingInstance\" : \"\" , \"source\" : { \"component\" : \"kubelet\" , \"host\" : \"gke-knative-auto-cluster-default-pool-23c23c4f-xdj0\" } , \"type\" : \"Normal\" } Delete the ApiServerSource object \u00b6 To remove the ApiServerSource object and all of the related resources: Delete the namespace by running the command: kubectl delete namespace <namespace> Where <namespace> is the name of the namespace that you created in step 1 earlier.","title":"Creating an ApiServerSource object"},{"location":"eventing/sources/apiserversource/getting-started/#creating-an-apiserversource-object","text":"This topic describes how to create an ApiServerSource object.","title":"Creating an ApiServerSource object"},{"location":"eventing/sources/apiserversource/getting-started/#before-you-begin","text":"Before you can create an ApiServerSource object: You must have Knative Eventing installed on your cluster. You must install the kubectl CLI tool. Optional: If you want to use the kn commands, install the kn tool.","title":"Before you begin"},{"location":"eventing/sources/apiserversource/getting-started/#create-an-apiserversource-object","text":"Optional: Create a namespace for the API server source instance by running the command: kubectl create namespace <namespace> Where <namespace> is the name of the namespace that you want to create. Note Creating a namespace for your ApiServerSource and related components allows you to view changes and events for this workflow more easily, because these are isolated from the other components that might exist in your default namespace. It also makes removing the source easier, because you can delete the namespace to remove all of the resources. Create a ServiceAccount: Create a YAML file using the following template: apiVersion : v1 kind : ServiceAccount metadata : name : <service-account> namespace : <namespace> Where: <service-account> is the name of the ServiceAccount that you want to create. <namespace> is the namespace that you created in step 1 earlier. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Create a Role: Create a YAML file using the following template: apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : <role> namespace : <namespace> rules : <rules> Where: <role> is the name of the Role that you want to create. <namespace> is the name of the namespace that you created in step 1 earlier. <rules> are the set of permissions you want to grant to the APIServerSource object. This set of permissions must match the resources you want to receive events from. For example, to receive events related to the events resource, use the following set of permissions: - apiGroups : - \"\" resources : - events verbs : - get - list - watch Note The only required verbs are get , list and watch . Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Create a RoleBinding: Create a YAML file using the following template: apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : <role-binding> namespace : <namespace> roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : <role> subjects : - kind : ServiceAccount name : <service-account> namespace : <namespace> Where: <role-binding> is the name of the RoleBinding that you want to create. <namespace> is the name of the namespace that you created in step 1 earlier. <role> is the name of the Role that you created in step 3 earlier. <service-account> is the name of the ServiceAccount that you created in step 2 earlier. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Create a sink. If you do not have your own sink, you can use the following example Service that dumps incoming messages to a log: Copy the YAML below into a file: apiVersion : apps/v1 kind : Deployment metadata : name : event-display namespace : <namespace> spec : replicas : 1 selector : matchLabels : &labels app : event-display template : metadata : labels : *labels spec : containers : - name : event-display image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- kind : Service apiVersion : v1 metadata : name : event-display namespace : <namespace> spec : selector : app : event-display ports : - protocol : TCP port : 80 targetPort : 8080 Where <namespace> is the name of the namespace that you created in step 1 above. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Create the ApiServerSource object: kn YAML To create the ApiServerSource, run the command: kn source apiserver create <apiserversource> \\ --namespace <namespace> \\ --mode \"Resource\" \\ --resource \"Event:v1\" \\ --service-account <service-account> \\ --sink <sink-name> Where: <apiserversource> is the name of the source that you want to create. <namespace> is the name of the namespace that you created in step 1 earlier. <service-account> is the name of the ServiceAccount that you created in step 2 earlier. <sink-name> is the name of your sink, for example, http://event-display.pingsource-example.svc.cluster.local . For a list of available options, see the Knative client documentation . Create a YAML file using the following template: apiVersion : sources.knative.dev/v1 kind : ApiServerSource metadata : name : <apiserversource-name> namespace : <namespace> spec : serviceAccountName : <service-account> mode : <event-mode> resources : - apiVersion : v1 kind : Event sink : ref : apiVersion : v1 kind : <sink-kind> name : <sink-name> Where: <apiserversource-name> is the name of the source that you want to create. <namespace> is the name of the namespace that you created in step 1 earlier. <service-account> is the name of the ServiceAccount that you created in step 2 earlier. <event-mode> is either Resource or Reference . If set to Resource , the event payload contains the entire resource that the event is for. If set to Reference , the event payload only contains a reference to the resource that the event is for. The default is Reference . <sink-kind> is any supported Addressable object that you want to use as a sink, for example, Service or Deployment . <sink-name> is the name of your sink. For more information about the fields you can configure for the ApiServerSource object, see ApiServerSource reference . Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"Create an ApiServerSource object"},{"location":"eventing/sources/apiserversource/getting-started/#verify-the-apiserversource-object","text":"Make the Kubernetes API server create events by launching a test Pod in your namespace by running the command: kubectl run busybox --image = busybox --namespace = <namespace> --restart = Never -- ls Where <namespace> is the name of the namespace that you created in step 1 earlier. Delete the test Pod by running the command: kubectl --namespace = <namespace> delete pod busybox Where <namespace> is the name of the namespace that you created in step 1 earlier. View the logs to verify that Kubernetes events were sent to the sink by the Knative Eventing system by running the command: kubectl logs --namespace = <namespace> -l app = <sink> --tail = 100 Where: <namespace> is the name of the namespace that you created in step 1 earlier. <sink> is the name of the PodSpecable object that you used as a sink in step 5 earlier. Example log output: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.apiserver.resource.update source: https://10.96.0.1:443 subject: /apis/v1/namespaces/apiserversource-example/events/testevents.15dd3050eb1e6f50 id: e0447eb7-36b5-443b-9d37-faf4fe5c62f0 time: 2020 -07-28T19:14:54.719501054Z datacontenttype: application/json Extensions, kind: Event name: busybox.1626008649e617e3 namespace: apiserversource-example Data, { \"apiVersion\" : \"v1\" , \"count\" : 1 , \"eventTime\" : null, \"firstTimestamp\" : \"2020-07-28T19:14:54Z\" , \"involvedObject\" : { \"apiVersion\" : \"v1\" , \"fieldPath\" : \"spec.containers{busybox}\" , \"kind\" : \"Pod\" , \"name\" : \"busybox\" , \"namespace\" : \"apiserversource-example\" , \"resourceVersion\" : \"28987493\" , \"uid\" : \"1efb342a-737b-11e9-a6c5-42010a8a00ed\" } , \"kind\" : \"Event\" , \"lastTimestamp\" : \"2020-07-28T19:14:54Z\" , \"message\" : \"Started container\" , \"metadata\" : { \"creationTimestamp\" : \"2020-07-28T19:14:54Z\" , \"name\" : \"busybox.1626008649e617e3\" , \"namespace\" : \"default\" , \"resourceVersion\" : \"506088\" , \"selfLink\" : \"/api/v1/namespaces/apiserversource-example/events/busybox.1626008649e617e3\" , \"uid\" : \"2005af47-737b-11e9-a6c5-42010a8a00ed\" } , \"reason\" : \"Started\" , \"reportingComponent\" : \"\" , \"reportingInstance\" : \"\" , \"source\" : { \"component\" : \"kubelet\" , \"host\" : \"gke-knative-auto-cluster-default-pool-23c23c4f-xdj0\" } , \"type\" : \"Normal\" }","title":"Verify the ApiServerSource object"},{"location":"eventing/sources/apiserversource/getting-started/#delete-the-apiserversource-object","text":"To remove the ApiServerSource object and all of the related resources: Delete the namespace by running the command: kubectl delete namespace <namespace> Where <namespace> is the name of the namespace that you created in step 1 earlier.","title":"Delete the ApiServerSource object"},{"location":"eventing/sources/apiserversource/reference/","text":"ApiServerSource reference \u00b6 This topic provides reference information about the configurable fields for the ApiServerSource object. ApiServerSource \u00b6 An ApiServerSource definition supports the following fields: Field Description Required or optional apiVersion Specifies the API version, for example sources.knative.dev/v1 . Required kind Identifies this resource object as an ApiServerSource object. Required metadata Specifies metadata that uniquely identifies the ApiServerSource object. For example, a name . Required spec Specifies the configuration information for this ApiServerSource object. Required spec.resources The resources that the source tracks so it can send related lifecycle events from the Kubernetes ApiServer. Includes an optional label selector to help filter. Required spec.mode EventMode controls the format of the event. Set to Reference to send a dataref event type for the resource being watched. Only a reference to the resource is included in the event payload. Set to Resource to have the full resource lifecycle event in the payload. Defaults to Reference . Optional spec.owner ResourceOwner is an additional filter to only track resources that are owned by a specific resource type. If ResourceOwner matches Resources[n] then Resources[n] is allowed to pass the ResourceOwner filter. Optional spec.serviceAccountName The name of the ServiceAccount to use to run this source. Defaults to default if not set. Optional spec.sink A reference to an object that resolves to a URI to use as the sink. Required spec.ceOverrides Defines overrides to control the output format and modifications to the event sent to the sink. Optional Resources parameter \u00b6 The resources parameter specifies the resources that the source tracks so that it can send related lifecycle events from the Kubernetes ApiServer. The parameter includes an optional label selector to help filter. A resources definition supports the following fields: Field Description Required or optional apiVersion API version of the resource to watch. Required kind Kind of the resource to watch. Required selector LabelSelector filters this source to objects to those resources pass the label selector. Optional selector.matchExpressions A list of label selector requirements. The requirements are ANDed. Use one of matchExpressions or matchLabels selector.matchExpressions.key The label key that the selector applies to. Required if using matchExpressions selector.matchExpressions.operator Represents a key's relationship to a set of values. Valid operators are In , NotIn , Exists and DoesNotExist . Required if using matchExpressions selector.matchExpressions.values An array of string values. If operator is In or NotIn , the values array must be non-empty. If operator is Exists or DoesNotExist , the values array must be empty. This array is replaced during a strategic merge patch. Required if using matchExpressions selector.matchLabels A map of key-value pairs. Each key-value pair in the matchLabels map is equivalent to an element of matchExpressions , where the key field is matchLabels.<key> , the operator is In , and the values array contains only \"matchLabels. \". The requirements are ANDed. Use one of matchExpressions or matchLabels Example: Resources parameter \u00b6 Given the following YAML, the ApiServerSource object receives events for all Pods and Deployments in the namespace: apiVersion : sources.knative.dev/v1 kind : ApiServerSource metadata : name : <apiserversource> namespace : <namespace> spec : # ... resources : - apiVersion : v1 kind : Pod - apiVersion : apps/v1 kind : Deployment Example: Resources parameter using matchExpressions \u00b6 Given the following YAML, ApiServerSource object receives events for all Pods in the namespace that have a label app=myapp or app=yourapp : apiVersion : sources.knative.dev/v1 kind : ApiServerSource metadata : name : <apiserversource> namespace : <namespace> spec : # ... resources : - apiVersion : v1 kind : Pod selector : matchExpressions : - key : app operator : In values : - myapp - yourapp Example: Resources parameter using matchLabels \u00b6 Given the following YAML, the ApiServerSource object receives events for all Pods in the namespace that have a label app=myapp : apiVersion : sources.knative.dev/v1 kind : ApiServerSource metadata : name : <apiserversource> namespace : <namespace> spec : # ... resources : - apiVersion : v1 kind : Pod selector : matchLabels : app : myapp ServiceAccountName parameter \u00b6 ServiceAccountName is a reference to a Kubernetes service account. To track the lifecycle events of the specified resources , you must assign the proper permissions to the ApiServerSource object. Example: tracking Pods \u00b6 The following YAML files create a ServiceAccount, Role and RoleBinding and grant the permission to get, list and watch Pod resources in the namespace apiserversource-example for the ApiServerSource. Example ServiceAccount: apiVersion : v1 kind : ServiceAccount metadata : name : test-service-account namespace : apiserversource-example Example Role with permission to get, list and watch Pod resources: apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : test-role rules : - apiGroups : - \"\" resources : - pods verbs : - get - list - watch Example RoleBinding: apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : test-role-binding roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : test-role subjects : - kind : ServiceAccount name : test-service-account namespace : apiserversource-example Example ApiServerSource using test-service-account : apiVersion : sources.knative.dev/v1 kind : ApiServerSource metadata : name : test-apiserversource namespace : apiserversource-example spec : # ... serviceAccountName : test-service-account ... Owner parameter \u00b6 ResourceOwner is an additional filter to only track resources that are owned by a specific resource type. If ResourceOwner matches Resources[n] then Resources[n] is allowed to pass the ResourceOwner filter. An owner definition supports the following fields: Field Description Required or optional apiVersion API version of the resource to watch. Required kind Kind of the resource to watch. Required Example: Owner parameter \u00b6 apiVersion : sources.knative.dev/v1 kind : ApiServerSource metadata : name : <apiserversource> namespace : <namespace> spec : ... owner : apiVersion : apps/v1 kind : Deployment ... CloudEvent Overrides \u00b6 CloudEvent Overrides defines overrides to control the output format and modifications of the event sent to the sink. A ceOverrides definition supports the following fields: Field Description Required or optional extensions Specifies which attributes are added or overridden on the outbound event. Each extensions key-value pair is set independently on the event as an attribute extension. Optional Note Only valid CloudEvent attribute names are allowed as extensions. You cannot set the spec defined attributes from the extensions override configuration. For example, you can not modify the type attribute. Example: CloudEvent Overrides \u00b6 apiVersion : sources.knative.dev/v1 kind : ApiServerSource metadata : name : <apiserversource> namespace : <namespace> spec : ... ceOverrides : extensions : extra : this is an extra attribute additional : 42 Contract This results in the K_CE_OVERRIDES environment variable being set on the sink container as follows: { \"extensions\" : { \"extra\" : \"this is an extra attribute\" , \"additional\" : \"42\" } }","title":"ApiServerSource reference"},{"location":"eventing/sources/apiserversource/reference/#apiserversource-reference","text":"This topic provides reference information about the configurable fields for the ApiServerSource object.","title":"ApiServerSource reference"},{"location":"eventing/sources/apiserversource/reference/#apiserversource","text":"An ApiServerSource definition supports the following fields: Field Description Required or optional apiVersion Specifies the API version, for example sources.knative.dev/v1 . Required kind Identifies this resource object as an ApiServerSource object. Required metadata Specifies metadata that uniquely identifies the ApiServerSource object. For example, a name . Required spec Specifies the configuration information for this ApiServerSource object. Required spec.resources The resources that the source tracks so it can send related lifecycle events from the Kubernetes ApiServer. Includes an optional label selector to help filter. Required spec.mode EventMode controls the format of the event. Set to Reference to send a dataref event type for the resource being watched. Only a reference to the resource is included in the event payload. Set to Resource to have the full resource lifecycle event in the payload. Defaults to Reference . Optional spec.owner ResourceOwner is an additional filter to only track resources that are owned by a specific resource type. If ResourceOwner matches Resources[n] then Resources[n] is allowed to pass the ResourceOwner filter. Optional spec.serviceAccountName The name of the ServiceAccount to use to run this source. Defaults to default if not set. Optional spec.sink A reference to an object that resolves to a URI to use as the sink. Required spec.ceOverrides Defines overrides to control the output format and modifications to the event sent to the sink. Optional","title":"ApiServerSource"},{"location":"eventing/sources/apiserversource/reference/#resources-parameter","text":"The resources parameter specifies the resources that the source tracks so that it can send related lifecycle events from the Kubernetes ApiServer. The parameter includes an optional label selector to help filter. A resources definition supports the following fields: Field Description Required or optional apiVersion API version of the resource to watch. Required kind Kind of the resource to watch. Required selector LabelSelector filters this source to objects to those resources pass the label selector. Optional selector.matchExpressions A list of label selector requirements. The requirements are ANDed. Use one of matchExpressions or matchLabels selector.matchExpressions.key The label key that the selector applies to. Required if using matchExpressions selector.matchExpressions.operator Represents a key's relationship to a set of values. Valid operators are In , NotIn , Exists and DoesNotExist . Required if using matchExpressions selector.matchExpressions.values An array of string values. If operator is In or NotIn , the values array must be non-empty. If operator is Exists or DoesNotExist , the values array must be empty. This array is replaced during a strategic merge patch. Required if using matchExpressions selector.matchLabels A map of key-value pairs. Each key-value pair in the matchLabels map is equivalent to an element of matchExpressions , where the key field is matchLabels.<key> , the operator is In , and the values array contains only \"matchLabels. \". The requirements are ANDed. Use one of matchExpressions or matchLabels","title":"Resources parameter"},{"location":"eventing/sources/apiserversource/reference/#example-resources-parameter","text":"Given the following YAML, the ApiServerSource object receives events for all Pods and Deployments in the namespace: apiVersion : sources.knative.dev/v1 kind : ApiServerSource metadata : name : <apiserversource> namespace : <namespace> spec : # ... resources : - apiVersion : v1 kind : Pod - apiVersion : apps/v1 kind : Deployment","title":"Example: Resources parameter"},{"location":"eventing/sources/apiserversource/reference/#example-resources-parameter-using-matchexpressions","text":"Given the following YAML, ApiServerSource object receives events for all Pods in the namespace that have a label app=myapp or app=yourapp : apiVersion : sources.knative.dev/v1 kind : ApiServerSource metadata : name : <apiserversource> namespace : <namespace> spec : # ... resources : - apiVersion : v1 kind : Pod selector : matchExpressions : - key : app operator : In values : - myapp - yourapp","title":"Example: Resources parameter using matchExpressions"},{"location":"eventing/sources/apiserversource/reference/#example-resources-parameter-using-matchlabels","text":"Given the following YAML, the ApiServerSource object receives events for all Pods in the namespace that have a label app=myapp : apiVersion : sources.knative.dev/v1 kind : ApiServerSource metadata : name : <apiserversource> namespace : <namespace> spec : # ... resources : - apiVersion : v1 kind : Pod selector : matchLabels : app : myapp","title":"Example: Resources parameter using matchLabels"},{"location":"eventing/sources/apiserversource/reference/#serviceaccountname-parameter","text":"ServiceAccountName is a reference to a Kubernetes service account. To track the lifecycle events of the specified resources , you must assign the proper permissions to the ApiServerSource object.","title":"ServiceAccountName parameter"},{"location":"eventing/sources/apiserversource/reference/#example-tracking-pods","text":"The following YAML files create a ServiceAccount, Role and RoleBinding and grant the permission to get, list and watch Pod resources in the namespace apiserversource-example for the ApiServerSource. Example ServiceAccount: apiVersion : v1 kind : ServiceAccount metadata : name : test-service-account namespace : apiserversource-example Example Role with permission to get, list and watch Pod resources: apiVersion : rbac.authorization.k8s.io/v1 kind : Role metadata : name : test-role rules : - apiGroups : - \"\" resources : - pods verbs : - get - list - watch Example RoleBinding: apiVersion : rbac.authorization.k8s.io/v1 kind : RoleBinding metadata : name : test-role-binding roleRef : apiGroup : rbac.authorization.k8s.io kind : Role name : test-role subjects : - kind : ServiceAccount name : test-service-account namespace : apiserversource-example Example ApiServerSource using test-service-account : apiVersion : sources.knative.dev/v1 kind : ApiServerSource metadata : name : test-apiserversource namespace : apiserversource-example spec : # ... serviceAccountName : test-service-account ...","title":"Example: tracking Pods"},{"location":"eventing/sources/apiserversource/reference/#owner-parameter","text":"ResourceOwner is an additional filter to only track resources that are owned by a specific resource type. If ResourceOwner matches Resources[n] then Resources[n] is allowed to pass the ResourceOwner filter. An owner definition supports the following fields: Field Description Required or optional apiVersion API version of the resource to watch. Required kind Kind of the resource to watch. Required","title":"Owner parameter"},{"location":"eventing/sources/apiserversource/reference/#example-owner-parameter","text":"apiVersion : sources.knative.dev/v1 kind : ApiServerSource metadata : name : <apiserversource> namespace : <namespace> spec : ... owner : apiVersion : apps/v1 kind : Deployment ...","title":"Example: Owner parameter"},{"location":"eventing/sources/apiserversource/reference/#cloudevent-overrides","text":"CloudEvent Overrides defines overrides to control the output format and modifications of the event sent to the sink. A ceOverrides definition supports the following fields: Field Description Required or optional extensions Specifies which attributes are added or overridden on the outbound event. Each extensions key-value pair is set independently on the event as an attribute extension. Optional Note Only valid CloudEvent attribute names are allowed as extensions. You cannot set the spec defined attributes from the extensions override configuration. For example, you can not modify the type attribute.","title":"CloudEvent Overrides"},{"location":"eventing/sources/apiserversource/reference/#example-cloudevent-overrides","text":"apiVersion : sources.knative.dev/v1 kind : ApiServerSource metadata : name : <apiserversource> namespace : <namespace> spec : ... ceOverrides : extensions : extra : this is an extra attribute additional : 42 Contract This results in the K_CE_OVERRIDES environment variable being set on the sink container as follows: { \"extensions\" : { \"extra\" : \"this is an extra attribute\" , \"additional\" : \"42\" } }","title":"Example: CloudEvent Overrides"},{"location":"eventing/sources/kafka-source/","text":"Apache Kafka Source \u00b6 Tutorial on how to build and deploy a KafkaSource event source. Background \u00b6 The KafkaSource reads all the messages, from all partitions, and sends those messages as CloudEvents through HTTP to its configured sink . The KafkaSource supports an ordered consumer delivery guaranty, which is a per-partition blocking consumer that waits for a successful response from the CloudEvent subscriber before it delivers the next message of the partition. Installing Kafka source \u00b6 Install the Kafka controller by entering the following command: kubectl apply --filename https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Install the Kafka Source data plane by entering the following command: kubectl apply --filename https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-source.yaml Verify that kafka-controller and kafka-source-dispatcher are running, by entering the following command: kubectl get deployments.apps -n knative-eventing Example output: NAME READY UP-TO-DATE AVAILABLE AGE kafka-controller 1 /1 1 1 3s kafka-source-dispatcher 1 /1 1 1 4s Create a Kafka topic \u00b6 Note The create a Kafka topic section assumes you're using Strimzi to operate Apache Kafka, however equivalent operations can be replicated using the Apache Kafka CLI or any other tool. If you are using Strimzi: Create a KafkaTopic YAML file: apiVersion : kafka.strimzi.io/v1beta2 kind : KafkaTopic metadata : name : knative-demo-topic namespace : kafka labels : strimzi.io/cluster : my-cluster spec : partitions : 3 replicas : 1 config : retention.ms : 7200000 segment.bytes : 1073741824 Deploy the KafkaTopic YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of your KafkaTopic YAML file. Example output: kafkatopic.kafka.strimzi.io/knative-demo-topic created Ensure that the KafkaTopic is running by running the command: kubectl -n kafka get kafkatopics.kafka.strimzi.io Example output: NAME AGE knative-demo-topic 16s Create a Service \u00b6 Clone the sample code GitHub repository, and navigate to the local directory of the repository: git clone -b \"main\" https://github.com/knative/docs knative-docs cd knative-docs/code-samples/eventing/kafka/source 2. Create the event-display Service as a YAML file: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display namespace : default spec : template : spec : containers : - # This corresponds to # https://github.com/knative/eventing/tree/main/cmd/event_display/main.go image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Example output: service.serving.knative.dev/event-display created Ensure that the Service Pod is running, by running the command: kubectl get pods The Pod name is prefixed with event-display : NAME READY STATUS RESTARTS AGE event-display-00001-deployment-5d5df6c7-gv2j4 2 /2 Running 0 72s Kafka event source \u00b6 Modify source/event-source.yaml accordingly with bootstrap servers, topics, and so on: apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-source spec : consumerGroup : knative-group bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 # note the kafka namespace topics : - knative-demo-topic sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Deploy the event source: kubectl apply -f event-source.yaml Example output: kafkasource.sources.knative.dev/kafka-source created Verify that the KafkaSource is ready: kubectl get kafkasource kafka-source Example output: NAME TOPICS BOOTSTRAPSERVERS READY REASON AGE kafka-source [ \"knative-demo-topic\" ] [ \"my-cluster-kafka-bootstrap.kafka:9092\" ] True 26h Verify \u00b6 Produce a message ( {\"msg\": \"This is a test!\"} ) to the Apache Kafka topic as in the following example: kubectl -n kafka run kafka-producer -ti --image = strimzi/kafka:0.14.0-kafka-2.3.0 --rm = true --restart = Never -- bin/kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap:9092 --topic knative-demo-topic Tip If you don't see a command prompt, try pressing Enter . Verify that the Service received the message from the event source: kubectl logs --selector = 'serving.knative.dev/service=event-display' -c user-container Example output: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.kafka.event source: /apis/v1/namespaces/default/kafkasources/kafka-source#my-topic subject: partition:0#564 id: partition:0/offset:564 time: 2020 -02-10T18:10:23.861866615Z datacontenttype: application/json Extensions, key: Data, { \"msg\" : \"This is a test!\" } Clean up steps \u00b6 Delete the Kafka event source: kubectl delete -f source/source.yaml kafkasource.sources.knative.dev Example output: \"kafka-source\" deleted Delete the event-display Service: kubectl delete -f source/event-display.yaml service.serving.knative.dev Example output: \"event-display\" deleted Optional: Remove the Apache Kafka Topic kubectl delete -f kafka-topic.yaml Example output: kafkatopic.kafka.strimzi.io \"knative-demo-topic\" deleted Optional: Specify the key deserializer \u00b6 When KafkaSource receives a message from Kafka, it dumps the key in the Event extension called Key and dumps Kafka message headers in the extensions starting with kafkaheader . You can specify the key deserializer among four types: string (default) for UTF-8 encoded strings int for 32-bit & 64-bit signed integers float for 32-bit & 64-bit floating points byte-array for a Base64 encoded byte array To specify the key deserializer, add the label kafkasources.sources.knative.dev/key-type to the KafkaSource definition, as shown in the following example: apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-source labels : kafkasources.sources.knative.dev/key-type : int spec : consumerGroup : knative-group bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 # note the kafka namespace topics : - knative-demo-topic sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Optional: Specify the initial offset \u00b6 By default the KafkaSource starts consuming from the latest offset in each partition. If you want to consume from the earliest offset, set the initialOffset field to earliest , for example: apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-source spec : consumerGroup : knative-group initialOffset : earliest bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 # note the kafka namespace topics : - knative-demo-topic sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Note The valid values for initialOffset are earliest and latest . Any other value results in a validation error. This field is honored only if there are no committed offsets for that consumer group. Connecting to a TLS-enabled Kafka Broker \u00b6 The KafkaSource supports TLS and SASL authentication methods. To enable TLS authentication, you must have the following files: CA Certificate Client Certificate and Key KafkaSource expects these files to be in PEM format. If they are in another format, such as JKS, convert them to PEM. Create the certificate files as secrets in the namespace where KafkaSource is going to be set up, by running the commands: kubectl create secret generic cacert --from-file = caroot.pem kubectl create secret tls kafka-secret --cert = certificate.pem --key = key.pem Apply the KafkaSource. Modify the bootstrapServers and topics fields accordingly. apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-source-with-tls spec : net : tls : enable : true cert : secretKeyRef : key : tls.crt name : kafka-secret key : secretKeyRef : key : tls.key name : kafka-secret caCert : secretKeyRef : key : caroot.pem name : cacert consumerGroup : knative-group bootstrapServers : - my-secure-kafka-bootstrap.kafka:443 topics : - knative-demo-topic sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display","title":"KafkaSource"},{"location":"eventing/sources/kafka-source/#apache-kafka-source","text":"Tutorial on how to build and deploy a KafkaSource event source.","title":"Apache Kafka Source"},{"location":"eventing/sources/kafka-source/#background","text":"The KafkaSource reads all the messages, from all partitions, and sends those messages as CloudEvents through HTTP to its configured sink . The KafkaSource supports an ordered consumer delivery guaranty, which is a per-partition blocking consumer that waits for a successful response from the CloudEvent subscriber before it delivers the next message of the partition.","title":"Background"},{"location":"eventing/sources/kafka-source/#installing-kafka-source","text":"Install the Kafka controller by entering the following command: kubectl apply --filename https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Install the Kafka Source data plane by entering the following command: kubectl apply --filename https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-source.yaml Verify that kafka-controller and kafka-source-dispatcher are running, by entering the following command: kubectl get deployments.apps -n knative-eventing Example output: NAME READY UP-TO-DATE AVAILABLE AGE kafka-controller 1 /1 1 1 3s kafka-source-dispatcher 1 /1 1 1 4s","title":"Installing Kafka source"},{"location":"eventing/sources/kafka-source/#create-a-kafka-topic","text":"Note The create a Kafka topic section assumes you're using Strimzi to operate Apache Kafka, however equivalent operations can be replicated using the Apache Kafka CLI or any other tool. If you are using Strimzi: Create a KafkaTopic YAML file: apiVersion : kafka.strimzi.io/v1beta2 kind : KafkaTopic metadata : name : knative-demo-topic namespace : kafka labels : strimzi.io/cluster : my-cluster spec : partitions : 3 replicas : 1 config : retention.ms : 7200000 segment.bytes : 1073741824 Deploy the KafkaTopic YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of your KafkaTopic YAML file. Example output: kafkatopic.kafka.strimzi.io/knative-demo-topic created Ensure that the KafkaTopic is running by running the command: kubectl -n kafka get kafkatopics.kafka.strimzi.io Example output: NAME AGE knative-demo-topic 16s","title":"Create a Kafka topic"},{"location":"eventing/sources/kafka-source/#create-a-service","text":"Clone the sample code GitHub repository, and navigate to the local directory of the repository: git clone -b \"main\" https://github.com/knative/docs knative-docs cd knative-docs/code-samples/eventing/kafka/source 2. Create the event-display Service as a YAML file: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : event-display namespace : default spec : template : spec : containers : - # This corresponds to # https://github.com/knative/eventing/tree/main/cmd/event_display/main.go image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Example output: service.serving.knative.dev/event-display created Ensure that the Service Pod is running, by running the command: kubectl get pods The Pod name is prefixed with event-display : NAME READY STATUS RESTARTS AGE event-display-00001-deployment-5d5df6c7-gv2j4 2 /2 Running 0 72s","title":"Create a Service"},{"location":"eventing/sources/kafka-source/#kafka-event-source","text":"Modify source/event-source.yaml accordingly with bootstrap servers, topics, and so on: apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-source spec : consumerGroup : knative-group bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 # note the kafka namespace topics : - knative-demo-topic sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Deploy the event source: kubectl apply -f event-source.yaml Example output: kafkasource.sources.knative.dev/kafka-source created Verify that the KafkaSource is ready: kubectl get kafkasource kafka-source Example output: NAME TOPICS BOOTSTRAPSERVERS READY REASON AGE kafka-source [ \"knative-demo-topic\" ] [ \"my-cluster-kafka-bootstrap.kafka:9092\" ] True 26h","title":"Kafka event source"},{"location":"eventing/sources/kafka-source/#verify","text":"Produce a message ( {\"msg\": \"This is a test!\"} ) to the Apache Kafka topic as in the following example: kubectl -n kafka run kafka-producer -ti --image = strimzi/kafka:0.14.0-kafka-2.3.0 --rm = true --restart = Never -- bin/kafka-console-producer.sh --broker-list my-cluster-kafka-bootstrap:9092 --topic knative-demo-topic Tip If you don't see a command prompt, try pressing Enter . Verify that the Service received the message from the event source: kubectl logs --selector = 'serving.knative.dev/service=event-display' -c user-container Example output: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.kafka.event source: /apis/v1/namespaces/default/kafkasources/kafka-source#my-topic subject: partition:0#564 id: partition:0/offset:564 time: 2020 -02-10T18:10:23.861866615Z datacontenttype: application/json Extensions, key: Data, { \"msg\" : \"This is a test!\" }","title":"Verify"},{"location":"eventing/sources/kafka-source/#clean-up-steps","text":"Delete the Kafka event source: kubectl delete -f source/source.yaml kafkasource.sources.knative.dev Example output: \"kafka-source\" deleted Delete the event-display Service: kubectl delete -f source/event-display.yaml service.serving.knative.dev Example output: \"event-display\" deleted Optional: Remove the Apache Kafka Topic kubectl delete -f kafka-topic.yaml Example output: kafkatopic.kafka.strimzi.io \"knative-demo-topic\" deleted","title":"Clean up steps"},{"location":"eventing/sources/kafka-source/#optional-specify-the-key-deserializer","text":"When KafkaSource receives a message from Kafka, it dumps the key in the Event extension called Key and dumps Kafka message headers in the extensions starting with kafkaheader . You can specify the key deserializer among four types: string (default) for UTF-8 encoded strings int for 32-bit & 64-bit signed integers float for 32-bit & 64-bit floating points byte-array for a Base64 encoded byte array To specify the key deserializer, add the label kafkasources.sources.knative.dev/key-type to the KafkaSource definition, as shown in the following example: apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-source labels : kafkasources.sources.knative.dev/key-type : int spec : consumerGroup : knative-group bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 # note the kafka namespace topics : - knative-demo-topic sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display","title":"Optional: Specify the key deserializer"},{"location":"eventing/sources/kafka-source/#optional-specify-the-initial-offset","text":"By default the KafkaSource starts consuming from the latest offset in each partition. If you want to consume from the earliest offset, set the initialOffset field to earliest , for example: apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-source spec : consumerGroup : knative-group initialOffset : earliest bootstrapServers : - my-cluster-kafka-bootstrap.kafka:9092 # note the kafka namespace topics : - knative-demo-topic sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display Note The valid values for initialOffset are earliest and latest . Any other value results in a validation error. This field is honored only if there are no committed offsets for that consumer group.","title":"Optional: Specify the initial offset"},{"location":"eventing/sources/kafka-source/#connecting-to-a-tls-enabled-kafka-broker","text":"The KafkaSource supports TLS and SASL authentication methods. To enable TLS authentication, you must have the following files: CA Certificate Client Certificate and Key KafkaSource expects these files to be in PEM format. If they are in another format, such as JKS, convert them to PEM. Create the certificate files as secrets in the namespace where KafkaSource is going to be set up, by running the commands: kubectl create secret generic cacert --from-file = caroot.pem kubectl create secret tls kafka-secret --cert = certificate.pem --key = key.pem Apply the KafkaSource. Modify the bootstrapServers and topics fields accordingly. apiVersion : sources.knative.dev/v1beta1 kind : KafkaSource metadata : name : kafka-source-with-tls spec : net : tls : enable : true cert : secretKeyRef : key : tls.crt name : kafka-secret key : secretKeyRef : key : tls.key name : kafka-secret caCert : secretKeyRef : key : caroot.pem name : cacert consumerGroup : knative-group bootstrapServers : - my-secure-kafka-bootstrap.kafka:443 topics : - knative-demo-topic sink : ref : apiVersion : serving.knative.dev/v1 kind : Service name : event-display","title":"Connecting to a TLS-enabled Kafka Broker"},{"location":"eventing/sources/ping-source/","text":"Creating a PingSource object \u00b6 This topic describes how to create a PingSource object. A PingSource is an event source that produces events with a fixed payload on a specified cron schedule. The following example shows how you can configure a PingSource as an event source that sends events every minute to a Knative service named event-display that is used as a sink. If you have an existing sink, you can replace the examples with your own values. Before you begin \u00b6 To create a PingSource: You must install Knative Eventing . The PingSource event source type is enabled by default when you install Knative Eventing. You can use either kubectl or kn commands to create components such as a sink and PingSource. You can use either kubectl or kail for logging during the verification step in this procedure. Create a PingSource object \u00b6 Optional: Create a namespace for your PingSource by running the command: kubectl create namespace <namespace> Where <namespace> is the namespace that you want your PingSource to use. For example, pingsource-example . Note Creating a namespace for your PingSource and related components allows you to view changes and events for this workflow more easily, because these are isolated from the other components that might exist in your default namespace. It also makes removing the source easier, because you can delete the namespace to remove all of the resources. Create a sink. If you do not have your own sink, you can use the following example Service that dumps incoming messages to a log: Copy the YAML below into a file: apiVersion : apps/v1 kind : Deployment metadata : name : event-display namespace : <namespace> spec : replicas : 1 selector : matchLabels : &labels app : event-display template : metadata : labels : *labels spec : containers : - name : event-display image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- kind : Service apiVersion : v1 metadata : name : event-display namespace : <namespace> spec : selector : app : event-display ports : - protocol : TCP port : 80 targetPort : 8080 Where <namespace> is the name of the namespace that you created in step 1 above. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Create the PingSource object. Note The data you want to send must be represented as text in the PingSource YAML file. Events that send binary data cannot be directly serialized in YAML. However, you can send binary data that is base64 encoded by using dataBase64 in place of data in the PingSource spec. Use one of the following options: kn YAML YAML: binary data To create a PingSource that sends data that can be represented as plain text, such as text, JSON, or XML, run the command: kn source ping create <pingsource-name> \\ --namespace <namespace> \\ --schedule \"<cron-schedule>\" \\ --data '<data>' \\ --sink <sink-name> Where: <pingsource-name> is the name of the PingSource that you want to create, for example, test-ping-source . <namespace> is the name of the namespace that you created in step 1 above. <cron-schedule> is a cron expression for the schedule for the PingSource to send events, for example, */1 * * * * sends an event every minute. <data> is the data you want to send. This data must be represented as text, not binary. For example, a JSON object such as {\"message\": \"Hello world!\"} . <sink-name> is the name of your sink, for example, http://event-display.pingsource-example.svc.cluster.local . For a list of available options, see the Knative client documentation . To create a PingSource that sends data that can be represented as plain text, such as text, JSON, or XML: Create a YAML file using the template below: apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : <pingsource-name> namespace : <namespace> spec : schedule : \"<cron-schedule>\" contentType : \"<content-type>\" data : '<data>' sink : ref : apiVersion : v1 kind : <sink-kind> name : <sink-name> Where: <pingsource-name> is the name of the PingSource that you want to create, for example, test-ping-source . <namespace> is the name of the namespace that you created in step 1 above. <cron-schedule> is a cron expression for the schedule for the PingSource to send events, for example, */1 * * * * sends an event every minute. <content-type> is the media type of the data you want to send, for example, application/json . <data> is the data you want to send. This data must be represented as text, not binary. For example, a JSON object such as {\"message\": \"Hello world!\"} . <sink-kind> is any supported Addressable object that you want to use as a sink, for example, Service or Deployment . <sink-name> is the name of your sink, for example, event-display . For more information about the fields you can configure for the PingSource object, see PingSource reference . Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To create a PingSource that sends binary data: Create a YAML file using the template below: apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : <pingsource-name> namespace : <namespace> spec : schedule : \"<cron-schedule>\" contentType : \"<content-type>\" dataBase64 : \"<base64-data>\" sink : ref : apiVersion : v1 kind : <sink-kind> name : <sink-name> Where: <pingsource-name> is the name of the PingSource that you want to create, for example, test-ping-source-binary . <namespace> is the name of the namespace that you created in step 1 above. <cron-schedule> is a cron expression for the schedule for the PingSource to send events, for example, */1 * * * * sends an event every minute. <content-type> is the media type of the data you want to send, for example, application/json . <base64-data> is the base64 encoded binary data that you want to send, for example, ZGF0YQ== . <sink-kind> is any supported Addressable object that you want to use as a sink, for example, Service or Deployment . <sink-name> is the name of your sink, for example, event-display . For more information about the fields you can configure for the PingSource object, see PingSource reference . Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Verify the PingSource object \u00b6 View the logs for the event-display event consumer by running the command: kubectl kail kubectl -n pingsource-example logs -l app = event-display --tail = 100 kail -l serving.knative.dev/service = event-display -c user-container --since = 10m Verify that the output returns the properties of the events that your PingSource sent to your sink. In the example below, the command has returned the Attributes and Data properties of the events that the PingSource sent to the event-display Service: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.ping source: /apis/v1/namespaces/pingsource-example/pingsources/test-ping-source id: 49f04fe2-7708-453d-ae0a-5fbaca9586a8 time: 2021 -03-25T19:41:00.444508332Z datacontenttype: application/json Data, { \"message\" : \"Hello world!\" } Delete the PingSource object \u00b6 You can either delete the PingSource and all related resources, or delete the resources individually: To remove the PingSource object and all of the related resources, delete the namespace by running the command: kubectl delete namespace <namespace> Where <namespace> is the namespace that contains the PingSource object. To delete the PingSource instance only, run the command: kn kubectl kn source ping delete <pingsource-name> Where <pingsource-name> is the name of the PingSource you want to delete, for example, test-ping-source . kubectl delete pingsources.sources.knative.dev <pingsource-name> Where <pingsource-name> is the name of the PingSource you want to delete, for example, test-ping-source . To delete the sink only, run the command: kn kubectl kn service delete <sink-name> Where <sink-name> is the name of your sink, for example, event-display . kubectl delete service.serving.knative.dev <sink-name> Where <sink-name> is the name of your sink, for example, event-display .","title":"Creating a PingSource object"},{"location":"eventing/sources/ping-source/#creating-a-pingsource-object","text":"This topic describes how to create a PingSource object. A PingSource is an event source that produces events with a fixed payload on a specified cron schedule. The following example shows how you can configure a PingSource as an event source that sends events every minute to a Knative service named event-display that is used as a sink. If you have an existing sink, you can replace the examples with your own values.","title":"Creating a PingSource object"},{"location":"eventing/sources/ping-source/#before-you-begin","text":"To create a PingSource: You must install Knative Eventing . The PingSource event source type is enabled by default when you install Knative Eventing. You can use either kubectl or kn commands to create components such as a sink and PingSource. You can use either kubectl or kail for logging during the verification step in this procedure.","title":"Before you begin"},{"location":"eventing/sources/ping-source/#create-a-pingsource-object","text":"Optional: Create a namespace for your PingSource by running the command: kubectl create namespace <namespace> Where <namespace> is the namespace that you want your PingSource to use. For example, pingsource-example . Note Creating a namespace for your PingSource and related components allows you to view changes and events for this workflow more easily, because these are isolated from the other components that might exist in your default namespace. It also makes removing the source easier, because you can delete the namespace to remove all of the resources. Create a sink. If you do not have your own sink, you can use the following example Service that dumps incoming messages to a log: Copy the YAML below into a file: apiVersion : apps/v1 kind : Deployment metadata : name : event-display namespace : <namespace> spec : replicas : 1 selector : matchLabels : &labels app : event-display template : metadata : labels : *labels spec : containers : - name : event-display image : gcr.io/knative-releases/knative.dev/eventing/cmd/event_display --- kind : Service apiVersion : v1 metadata : name : event-display namespace : <namespace> spec : selector : app : event-display ports : - protocol : TCP port : 80 targetPort : 8080 Where <namespace> is the name of the namespace that you created in step 1 above. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Create the PingSource object. Note The data you want to send must be represented as text in the PingSource YAML file. Events that send binary data cannot be directly serialized in YAML. However, you can send binary data that is base64 encoded by using dataBase64 in place of data in the PingSource spec. Use one of the following options: kn YAML YAML: binary data To create a PingSource that sends data that can be represented as plain text, such as text, JSON, or XML, run the command: kn source ping create <pingsource-name> \\ --namespace <namespace> \\ --schedule \"<cron-schedule>\" \\ --data '<data>' \\ --sink <sink-name> Where: <pingsource-name> is the name of the PingSource that you want to create, for example, test-ping-source . <namespace> is the name of the namespace that you created in step 1 above. <cron-schedule> is a cron expression for the schedule for the PingSource to send events, for example, */1 * * * * sends an event every minute. <data> is the data you want to send. This data must be represented as text, not binary. For example, a JSON object such as {\"message\": \"Hello world!\"} . <sink-name> is the name of your sink, for example, http://event-display.pingsource-example.svc.cluster.local . For a list of available options, see the Knative client documentation . To create a PingSource that sends data that can be represented as plain text, such as text, JSON, or XML: Create a YAML file using the template below: apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : <pingsource-name> namespace : <namespace> spec : schedule : \"<cron-schedule>\" contentType : \"<content-type>\" data : '<data>' sink : ref : apiVersion : v1 kind : <sink-kind> name : <sink-name> Where: <pingsource-name> is the name of the PingSource that you want to create, for example, test-ping-source . <namespace> is the name of the namespace that you created in step 1 above. <cron-schedule> is a cron expression for the schedule for the PingSource to send events, for example, */1 * * * * sends an event every minute. <content-type> is the media type of the data you want to send, for example, application/json . <data> is the data you want to send. This data must be represented as text, not binary. For example, a JSON object such as {\"message\": \"Hello world!\"} . <sink-kind> is any supported Addressable object that you want to use as a sink, for example, Service or Deployment . <sink-name> is the name of your sink, for example, event-display . For more information about the fields you can configure for the PingSource object, see PingSource reference . Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To create a PingSource that sends binary data: Create a YAML file using the template below: apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : <pingsource-name> namespace : <namespace> spec : schedule : \"<cron-schedule>\" contentType : \"<content-type>\" dataBase64 : \"<base64-data>\" sink : ref : apiVersion : v1 kind : <sink-kind> name : <sink-name> Where: <pingsource-name> is the name of the PingSource that you want to create, for example, test-ping-source-binary . <namespace> is the name of the namespace that you created in step 1 above. <cron-schedule> is a cron expression for the schedule for the PingSource to send events, for example, */1 * * * * sends an event every minute. <content-type> is the media type of the data you want to send, for example, application/json . <base64-data> is the base64 encoded binary data that you want to send, for example, ZGF0YQ== . <sink-kind> is any supported Addressable object that you want to use as a sink, for example, Service or Deployment . <sink-name> is the name of your sink, for example, event-display . For more information about the fields you can configure for the PingSource object, see PingSource reference . Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"Create a PingSource object"},{"location":"eventing/sources/ping-source/#verify-the-pingsource-object","text":"View the logs for the event-display event consumer by running the command: kubectl kail kubectl -n pingsource-example logs -l app = event-display --tail = 100 kail -l serving.knative.dev/service = event-display -c user-container --since = 10m Verify that the output returns the properties of the events that your PingSource sent to your sink. In the example below, the command has returned the Attributes and Data properties of the events that the PingSource sent to the event-display Service: \u2601\ufe0f cloudevents.Event Validation: valid Context Attributes, specversion: 1 .0 type: dev.knative.sources.ping source: /apis/v1/namespaces/pingsource-example/pingsources/test-ping-source id: 49f04fe2-7708-453d-ae0a-5fbaca9586a8 time: 2021 -03-25T19:41:00.444508332Z datacontenttype: application/json Data, { \"message\" : \"Hello world!\" }","title":"Verify the PingSource object"},{"location":"eventing/sources/ping-source/#delete-the-pingsource-object","text":"You can either delete the PingSource and all related resources, or delete the resources individually: To remove the PingSource object and all of the related resources, delete the namespace by running the command: kubectl delete namespace <namespace> Where <namespace> is the namespace that contains the PingSource object. To delete the PingSource instance only, run the command: kn kubectl kn source ping delete <pingsource-name> Where <pingsource-name> is the name of the PingSource you want to delete, for example, test-ping-source . kubectl delete pingsources.sources.knative.dev <pingsource-name> Where <pingsource-name> is the name of the PingSource you want to delete, for example, test-ping-source . To delete the sink only, run the command: kn kubectl kn service delete <sink-name> Where <sink-name> is the name of your sink, for example, event-display . kubectl delete service.serving.knative.dev <sink-name> Where <sink-name> is the name of your sink, for example, event-display .","title":"Delete the PingSource object"},{"location":"eventing/sources/ping-source/reference/","text":"PingSource reference \u00b6 This topic provides reference information about the configurable fields for the PingSource object. PingSource \u00b6 A PingSource definition supports the following fields: Field Description Required or optional apiVersion Specifies the API version, for example sources.knative.dev/v1 . Required kind Identifies this resource object as a PingSource object. Required metadata Specifies metadata that uniquely identifies the PingSource object. For example, a name . Required spec Specifies the configuration information for this PingSource object. Required spec.contentType The media type of data or dataBase64 . Default is empty. Optional spec.data The data used as the body of the event posted to the sink. Default is empty. Mutually exclusive with dataBase64 . Required if not sending base64 encoded data spec.dataBase64 A base64-encoded string of the actual event's body posted to the sink. Default is empty. Mutually exclusive with data . Required if sending base64 encoded data spec.schedule Specifies the cron schedule. Defaults to * * * * * . Optional spec.sink A reference to an object that resolves to a URI to use as the sink. Required spec.timezone Modifies the actual time relative to the specified timezone. Defaults to the system time zone. See the list of valid tz database time zones on Wikipedia. For general information about time zones, see the IANA website. Optional spec.ceOverrides Defines overrides to control the output format and modifications to the event sent to the sink. Optional status Defines the observed state of PingSource. Optional status.observedGeneration The 'Generation' of the Service that was last processed by the controller. Optional status.conditions The latest available observations of a resource's current state. Optional status.sinkUri The current active sink URI that has been configured for the Source. Optional CloudEvent Overrides \u00b6 CloudEvent Overrides defines overrides to control the output format and modifications of the event sent to the sink. A ceOverrides definition supports the following fields: Field Description Required or optional extensions Specifies which attributes are added or overridden on the outbound event. Each extensions key-value pair is set independently on the event as an attribute extension. Optional Note Only valid CloudEvent attribute names are allowed as extensions. You cannot set the spec defined attributes from the extensions override configuration. For example, you can not modify the type attribute. Example: CloudEvent Overrides \u00b6 apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : test-heartbeats spec : ... ceOverrides : extensions : extra : this is an extra attribute additional : 42 Contract This results in the K_CE_OVERRIDES environment variable being set on the subject as follows: { \"extensions\" : { \"extra\" : \"this is an extra attribute\" , \"additional\" : \"42\" } }","title":"PingSource reference"},{"location":"eventing/sources/ping-source/reference/#pingsource-reference","text":"This topic provides reference information about the configurable fields for the PingSource object.","title":"PingSource reference"},{"location":"eventing/sources/ping-source/reference/#pingsource","text":"A PingSource definition supports the following fields: Field Description Required or optional apiVersion Specifies the API version, for example sources.knative.dev/v1 . Required kind Identifies this resource object as a PingSource object. Required metadata Specifies metadata that uniquely identifies the PingSource object. For example, a name . Required spec Specifies the configuration information for this PingSource object. Required spec.contentType The media type of data or dataBase64 . Default is empty. Optional spec.data The data used as the body of the event posted to the sink. Default is empty. Mutually exclusive with dataBase64 . Required if not sending base64 encoded data spec.dataBase64 A base64-encoded string of the actual event's body posted to the sink. Default is empty. Mutually exclusive with data . Required if sending base64 encoded data spec.schedule Specifies the cron schedule. Defaults to * * * * * . Optional spec.sink A reference to an object that resolves to a URI to use as the sink. Required spec.timezone Modifies the actual time relative to the specified timezone. Defaults to the system time zone. See the list of valid tz database time zones on Wikipedia. For general information about time zones, see the IANA website. Optional spec.ceOverrides Defines overrides to control the output format and modifications to the event sent to the sink. Optional status Defines the observed state of PingSource. Optional status.observedGeneration The 'Generation' of the Service that was last processed by the controller. Optional status.conditions The latest available observations of a resource's current state. Optional status.sinkUri The current active sink URI that has been configured for the Source. Optional","title":"PingSource"},{"location":"eventing/sources/ping-source/reference/#cloudevent-overrides","text":"CloudEvent Overrides defines overrides to control the output format and modifications of the event sent to the sink. A ceOverrides definition supports the following fields: Field Description Required or optional extensions Specifies which attributes are added or overridden on the outbound event. Each extensions key-value pair is set independently on the event as an attribute extension. Optional Note Only valid CloudEvent attribute names are allowed as extensions. You cannot set the spec defined attributes from the extensions override configuration. For example, you can not modify the type attribute.","title":"CloudEvent Overrides"},{"location":"eventing/sources/ping-source/reference/#example-cloudevent-overrides","text":"apiVersion : sources.knative.dev/v1 kind : PingSource metadata : name : test-heartbeats spec : ... ceOverrides : extensions : extra : this is an extra attribute additional : 42 Contract This results in the K_CE_OVERRIDES environment variable being set on the subject as follows: { \"extensions\" : { \"extra\" : \"this is an extra attribute\" , \"additional\" : \"42\" } }","title":"Example: CloudEvent Overrides"},{"location":"eventing/sugar/","text":"Knative Eventing Sugar Controller \u00b6 Knative Eventing Sugar Controller will react to special labels and annotations to produce or control eventing resources in a cluster or namespace. This allows cluster operators and developers to focus on creating fewer resources, and the underlying eventing infrastructure is created on-demand, and cleaned up when no longer needed. Installing \u00b6 The following command installs the Eventing Sugar Controller: kubectl apply --filename https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-sugar-controller.yaml Automatic Broker Creation \u00b6 One way to create a Broker is to manually apply a resource to a cluster using the default settings: Copy the following YAML into a file: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : default namespace : default Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. There might be cases where automated Broker creation is desirable, such as on namespace creation, or on Trigger creation. The Sugar controller enables those use-cases: When a Namespace is labeled with eventing.knative.dev/injection=enabled , the sugar controller will create a default Broker named \"default\" in that namespace. When a Trigger is annotated with eventing.knative.dev/injection=enabled , the controller will create a Broker named by that Trigger in the Trigger's Namespace. When a Broker is deleted and the mentioned labels or annotations are in use, the Sugar Controller will automatically recreate a default Broker. Namespace Examples \u00b6 Creating a \"default\" Broker when creating a Namespace: Copy the following YAML into a file: apiVersion : v1 kind : Namespace metadata : name : example labels : eventing.knative.dev/injection : enabled Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To automatically create a Broker after a namespace exists, label the Namespace: kubectl label namespace default eventing.knative.dev/injection = enabled If the Broker named \"default\" already exists in the Namespace, the Sugar Controller will do nothing. Trigger Example \u00b6 Create a \"default\" Broker in the Trigger's Namespace when creating a Trigger: kubectl apply -f - << EOF apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: hello-sugar namespace: hello annotations: eventing.knative.dev/injection: enabled spec: broker: default subscriber: ref: apiVersion: v1 kind: Service name: event-display EOF This will make a Broker called \"default\" in the Namespace \"hello\", and attempt to send events to the \"event-display\" service. If the Broker named \"default\" already exists in the Namespace, the Sugar Controller will do nothing and the Trigger will not own the existing Broker.","title":"Sugar Controller"},{"location":"eventing/sugar/#knative-eventing-sugar-controller","text":"Knative Eventing Sugar Controller will react to special labels and annotations to produce or control eventing resources in a cluster or namespace. This allows cluster operators and developers to focus on creating fewer resources, and the underlying eventing infrastructure is created on-demand, and cleaned up when no longer needed.","title":"Knative Eventing Sugar Controller"},{"location":"eventing/sugar/#installing","text":"The following command installs the Eventing Sugar Controller: kubectl apply --filename https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-sugar-controller.yaml","title":"Installing"},{"location":"eventing/sugar/#automatic-broker-creation","text":"One way to create a Broker is to manually apply a resource to a cluster using the default settings: Copy the following YAML into a file: apiVersion : eventing.knative.dev/v1 kind : Broker metadata : name : default namespace : default Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. There might be cases where automated Broker creation is desirable, such as on namespace creation, or on Trigger creation. The Sugar controller enables those use-cases: When a Namespace is labeled with eventing.knative.dev/injection=enabled , the sugar controller will create a default Broker named \"default\" in that namespace. When a Trigger is annotated with eventing.knative.dev/injection=enabled , the controller will create a Broker named by that Trigger in the Trigger's Namespace. When a Broker is deleted and the mentioned labels or annotations are in use, the Sugar Controller will automatically recreate a default Broker.","title":"Automatic Broker Creation"},{"location":"eventing/sugar/#namespace-examples","text":"Creating a \"default\" Broker when creating a Namespace: Copy the following YAML into a file: apiVersion : v1 kind : Namespace metadata : name : example labels : eventing.knative.dev/injection : enabled Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To automatically create a Broker after a namespace exists, label the Namespace: kubectl label namespace default eventing.knative.dev/injection = enabled If the Broker named \"default\" already exists in the Namespace, the Sugar Controller will do nothing.","title":"Namespace Examples"},{"location":"eventing/sugar/#trigger-example","text":"Create a \"default\" Broker in the Trigger's Namespace when creating a Trigger: kubectl apply -f - << EOF apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: hello-sugar namespace: hello annotations: eventing.knative.dev/injection: enabled spec: broker: default subscriber: ref: apiVersion: v1 kind: Service name: event-display EOF This will make a Broker called \"default\" in the Namespace \"hello\", and attempt to send events to the \"event-display\" service. If the Broker named \"default\" already exists in the Namespace, the Sugar Controller will do nothing and the Trigger will not own the existing Broker.","title":"Trigger Example"},{"location":"eventing/troubleshooting/","text":"Debugging Knative Eventing \u00b6 This is an evolving document on how to debug a non-working Knative Eventing setup. Audience \u00b6 This document is intended for people that are familiar with the object model of Knative Eventing . You don't need to be an expert, but do need to know roughly how things fit together. Prerequisites \u00b6 Setup Knative Eventing and an Eventing-contrib resource . Example \u00b6 This guide uses an example consisting of an event source that sends events to a function. See example.yaml for the entire YAML. For any commands in this guide to work, you must apply example.yaml : kubectl apply --filename example.yaml Triggering Events \u00b6 Knative events will occur whenever a Kubernetes Event occurs in the knative-debug namespace. We can cause this to occur with the following commands: kubectl --namespace knative-debug run to-be-deleted --image = image-that-doesnt-exist --restart = Never # 5 seconds is arbitrary. We want K8s to notice that the Pod needs to be scheduled and generate at least one event. sleep 5 kubectl --namespace knative-debug delete pod to-be-deleted Then we can see the Kubernetes Event s (note that these are not Knative events!): kubectl --namespace knative-debug get events This should produce output along the lines of: LAST SEEN FIRST SEEN COUNT NAME KIND SUBOBJECT TYPE REASON SOURCE MESSAGE 20s 20s 1 to-be-deleted.157aadb9f376fc4e Pod Normal Scheduled default-scheduler Successfully assigned knative-debug/to-be-deleted to gke-kn24-default-pool-c12ac83b-pjf2 Where are my events? \u00b6 You've applied example.yaml and you are inspecting fn 's logs: kubectl --namespace knative-debug logs -l app = fn -c user-container But you don't see any events arrive. Where is the problem? Check created resources \u00b6 The first thing to check are all the created resources, do their statuses contain ready true? We will attempt to determine why from the most basic pieces out: fn - The Deployment has no dependencies inside Knative. svc - The Service has no dependencies inside Knative. chan - The Channel depends on its backing channel implementation and somewhat depends on sub . src - The Source depends on chan . sub - The Subscription depends on both chan and svc . fn \u00b6 kubectl --namespace knative-debug get deployment fn -o jsonpath = '{.status.availableReplicas}' We want to see 1 . If you don't, then you need to debug the Deployment . Is there anything obviously wrong mentioned in the status ? kubectl --namespace knative-debug get deployment fn --output yaml If it is not obvious what is wrong, then you need to debug the Deployment , which is out of scope of this document. Verify that the Pod is Ready : kubectl --namespace knative-debug get pod -l app = fn -o jsonpath = '{.items[*].status.conditions[?(@.type == \"Ready\")].status}' This should return True . If it doesn't, then try to debug the Deployment using the Kubernetes Application Debugging guide. svc \u00b6 kubectl --namespace knative-debug get service svc We just want to ensure this exists and has the correct name. If it doesn't exist, then you probably need to re-apply example.yaml . Verify it points at the expected pod. svcLabels = $( kubectl --namespace knative-debug get service svc -o go-template = '{{range $k, $v := .spec.selector}}{{ $k }}={{ $v }},{{ end }}' | sed 's/.$//' ) kubectl --namespace knative-debug get pods -l $svcLabels This should return a single Pod, which if you inspect is the one generated by fn . chan \u00b6 chan uses the in-memory-channel . This is a very basic channel and has few failure modes that will be exhibited in chan 's status . kubectl --namespace knative-debug get channel.messaging.knative.dev chan -o jsonpath = '{.status.conditions[?(@.type == \"Ready\")].status}' This should return True . If it doesn't, get the full resource: kubectl --namespace knative-debug get channel.messaging.knative.dev chan --output yaml If status is completely missing, it implies that something is wrong with the in-memory-channel controller. See Channel Controller . Next verify that chan is addressable: kubectl --namespace knative-debug get channel.messaging.knative.dev chan -o jsonpath = '{.status.address.hostname}' This should return a URI, likely ending in '.cluster.local'. If it doesn't, then it implies that something went wrong during reconciliation. See Channel Controller . We will verify that the two resources that the chan creates exist and are Ready . Service \u00b6 chan creates a K8s Service . kubectl --namespace knative-debug get service -l messaging.knative.dev/role = in -memory-channel It's spec is completely unimportant, as Istio will ignore it. It just needs to exist so that src can send events to it. If it doesn't exist, it implies that something went wrong during chan reconciliation. See Channel Controller . src \u00b6 src is a ApiServerSource . First we will verify that src is writing to chan . kubectl --namespace knative-debug get apiserversource src -o jsonpath = '{.spec.sink}' Which should return map[apiVersion:messaging.knative.dev/v1 kind:Channel name:chan] . If it doesn't, then src was setup incorrectly and its spec needs to be fixed. Fixing should be as simple as updating its spec to have the correct sink (see example.yaml ). Now that we know src is sending to chan , let's verify that it is Ready . kubectl --namespace knative-debug get apiserversource src -o jsonpath = '{.status.conditions[?(.type == \"Ready\")].status}' sub \u00b6 sub is a Subscription from chan to fn . Verify that sub is Ready : kubectl --namespace knative-debug get subscription sub -o jsonpath = '{.status.conditions[?(.type == \"Ready\")].status}' This should return True . If it doesn't then, look at all the status entries. kubectl --namespace knative-debug get subscription sub --output yaml Controllers \u00b6 Each of the resources has a Controller that is watching it. As of today, they tend to do a poor job of writing failure status messages and events, so we need to look at the Controller's logs. Note The Kubernetes Deployment Controller, which controls fn , is out of scope for this document. Service Controller \u00b6 The Kubernetes Service Controller, controlling svc , is out of scope for this document. Channel Controller \u00b6 There is not a single Channel Controller. Instead, there is one Controller for each Channel CRD. chan uses the InMemoryChannel Channel CRD , whose Controller is: kubectl --namespace knative-eventing get pod -l messaging.knative.dev/channel = in -memory-channel,messaging.knative.dev/role = controller --output yaml See its logs with: kubectl --namespace knative-eventing logs -l messaging.knative.dev/channel = in -memory-channel,messaging.knative.dev/role = controller Pay particular attention to any lines that have a logging level of warning or error . Source Controller \u00b6 Each Source will have its own Controller. src is a ApiServerSource , so its Controller is: kubectl --namespace knative-eventing get pod -l app = sources-controller This is actually a single binary that runs multiple Source Controllers, importantly including ApiServerSource Controller . ApiServerSource Controller \u00b6 The ApiServerSource Controller is run in the same binary as some other Source Controllers from Eventing. It is: kubectl --namespace knative-debug get pod -l eventing.knative.dev/sourceName = src,eventing.knative.dev/source = apiserver-source-controller View its logs with: kubectl --namespace knative-debug logs -l eventing.knative.dev/sourceName = src,eventing.knative.dev/source = apiserver-source-controller Pay particular attention to any lines that have a logging level of warning or error . Subscription Controller \u00b6 The Subscription Controller controls sub . It attempts to resolve the addresses that a Channel should send events to, and once resolved, inject those into the Channel 's spec.subscribable . kubectl --namespace knative-eventing get pod -l app = eventing-controller View its logs with: kubectl --namespace knative-eventing logs -l app = eventing-controller Pay particular attention to any lines that have a logging level of warning or error . Data Plane \u00b6 The entire Control Plane looks healthy, but we're still not getting any events. Now we need to investigate the data plane. The Knative event takes the following path: Event is generated by src . In this case, it is caused by having a Kubernetes Event trigger it, but as far as Knative is concerned, the Source is generating the event denovo (from nothing). src is POSTing the event to chan 's address, http://chan-kn-channel.knative-debug.svc.cluster.local . The Channel Dispatcher receives the request and introspects the Host header to determine which Channel it corresponds to. It sees that it corresponds to knative-debug/chan so forwards the request to the subscribers defined in sub , in particular svc , which is backed by fn . fn receives the request and logs it. We will investigate components in the order in which events should travel. Channel Dispatcher \u00b6 The Channel Dispatcher is the component that receives POSTs pushing events into Channel s and then POSTs to subscribers of those Channel s when an event is received. For the in-memory-channel used in this example, there is a single binary that handles both the receiving and dispatching sides for all in-memory-channel Channel s. First we will inspect the Dispatcher's logs to see if it is anything obvious: kubectl --namespace knative-eventing logs -l messaging.knative.dev/channel = in -memory-channel,messaging.knative.dev/role = dispatcher -c dispatcher Ideally we will see lines like: { \"level\" : \"info\" , \"ts\" : \"2019-08-16T13:50:55.424Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_receiver.go:147\" , \"msg\" : \"Request mapped to channel: knative-debug/chan-kn-channel\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"info\" , \"ts\" : \"2019-08-16T13:50:55.425Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_dispatcher.go:112\" , \"msg\" : \"Dispatching message to http://svc.knative-debug.svc.cluster.local/\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"info\" , \"ts\" : \"2019-08-16T13:50:55.981Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_receiver.go:140\" , \"msg\" : \"Received request for chan-kn-channel.knative-debug.svc.cluster.local\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } Which shows that the request is being received and then sent to svc , which is returning a 2XX response code (likely 200, 202, or 204). However if we see something like: { \"level\" : \"info\" , \"ts\" : \"2019-08-16T16:10:16.859Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_receiver.go:140\" , \"msg\" : \"Received request for chan-kn-channel.knative-debug.svc.cluster.local\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"info\" , \"ts\" : \"2019-08-16T16:10:16.859Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_receiver.go:147\" , \"msg\" : \"Request mapped to channel: knative-debug/chan-kn-channel\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"info\" , \"ts\" : \"2019-08-16T16:10:16.859Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_dispatcher.go:112\" , \"msg\" : \"Dispatching message to http://svc.knative-debug.svc.cluster.local/\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"error\" , \"ts\" : \"2019-08-16T16:10:38.169Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"fanout/fanout_handler.go:121\" , \"msg\" : \"Fanout had an error\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" , \"error\" : \"Unable to complete request Post http://svc.knative-debug.svc.cluster.local/: dial tcp 10.4.44.156:80: i/o timeout\" , \"stacktrace\" : \"knative.dev/eventing/pkg/provisioners/fanout.(*Handler).dispatch\\n\\t/Users/xxxxxx/go/src/knative.dev/eventing/pkg/provisioners/fanout/fanout_handler.go:121\\nknative.dev/eventing/pkg/provisioners/fanout.createReceiverFunction.func1.1\\n\\t/Users/i512777/go/src/knative.dev/eventing/pkg/provisioners/fanout/fanout_handler.go:95\" } Then we know there was a problem posting to http://svc.knative-debug.svc.cluster.local/ .","title":"Debugging"},{"location":"eventing/troubleshooting/#debugging-knative-eventing","text":"This is an evolving document on how to debug a non-working Knative Eventing setup.","title":"Debugging Knative Eventing"},{"location":"eventing/troubleshooting/#audience","text":"This document is intended for people that are familiar with the object model of Knative Eventing . You don't need to be an expert, but do need to know roughly how things fit together.","title":"Audience"},{"location":"eventing/troubleshooting/#prerequisites","text":"Setup Knative Eventing and an Eventing-contrib resource .","title":"Prerequisites"},{"location":"eventing/troubleshooting/#example","text":"This guide uses an example consisting of an event source that sends events to a function. See example.yaml for the entire YAML. For any commands in this guide to work, you must apply example.yaml : kubectl apply --filename example.yaml","title":"Example"},{"location":"eventing/troubleshooting/#triggering-events","text":"Knative events will occur whenever a Kubernetes Event occurs in the knative-debug namespace. We can cause this to occur with the following commands: kubectl --namespace knative-debug run to-be-deleted --image = image-that-doesnt-exist --restart = Never # 5 seconds is arbitrary. We want K8s to notice that the Pod needs to be scheduled and generate at least one event. sleep 5 kubectl --namespace knative-debug delete pod to-be-deleted Then we can see the Kubernetes Event s (note that these are not Knative events!): kubectl --namespace knative-debug get events This should produce output along the lines of: LAST SEEN FIRST SEEN COUNT NAME KIND SUBOBJECT TYPE REASON SOURCE MESSAGE 20s 20s 1 to-be-deleted.157aadb9f376fc4e Pod Normal Scheduled default-scheduler Successfully assigned knative-debug/to-be-deleted to gke-kn24-default-pool-c12ac83b-pjf2","title":"Triggering Events"},{"location":"eventing/troubleshooting/#where-are-my-events","text":"You've applied example.yaml and you are inspecting fn 's logs: kubectl --namespace knative-debug logs -l app = fn -c user-container But you don't see any events arrive. Where is the problem?","title":"Where are my events?"},{"location":"eventing/troubleshooting/#check-created-resources","text":"The first thing to check are all the created resources, do their statuses contain ready true? We will attempt to determine why from the most basic pieces out: fn - The Deployment has no dependencies inside Knative. svc - The Service has no dependencies inside Knative. chan - The Channel depends on its backing channel implementation and somewhat depends on sub . src - The Source depends on chan . sub - The Subscription depends on both chan and svc .","title":"Check created resources"},{"location":"eventing/troubleshooting/#fn","text":"kubectl --namespace knative-debug get deployment fn -o jsonpath = '{.status.availableReplicas}' We want to see 1 . If you don't, then you need to debug the Deployment . Is there anything obviously wrong mentioned in the status ? kubectl --namespace knative-debug get deployment fn --output yaml If it is not obvious what is wrong, then you need to debug the Deployment , which is out of scope of this document. Verify that the Pod is Ready : kubectl --namespace knative-debug get pod -l app = fn -o jsonpath = '{.items[*].status.conditions[?(@.type == \"Ready\")].status}' This should return True . If it doesn't, then try to debug the Deployment using the Kubernetes Application Debugging guide.","title":"fn"},{"location":"eventing/troubleshooting/#svc","text":"kubectl --namespace knative-debug get service svc We just want to ensure this exists and has the correct name. If it doesn't exist, then you probably need to re-apply example.yaml . Verify it points at the expected pod. svcLabels = $( kubectl --namespace knative-debug get service svc -o go-template = '{{range $k, $v := .spec.selector}}{{ $k }}={{ $v }},{{ end }}' | sed 's/.$//' ) kubectl --namespace knative-debug get pods -l $svcLabels This should return a single Pod, which if you inspect is the one generated by fn .","title":"svc"},{"location":"eventing/troubleshooting/#chan","text":"chan uses the in-memory-channel . This is a very basic channel and has few failure modes that will be exhibited in chan 's status . kubectl --namespace knative-debug get channel.messaging.knative.dev chan -o jsonpath = '{.status.conditions[?(@.type == \"Ready\")].status}' This should return True . If it doesn't, get the full resource: kubectl --namespace knative-debug get channel.messaging.knative.dev chan --output yaml If status is completely missing, it implies that something is wrong with the in-memory-channel controller. See Channel Controller . Next verify that chan is addressable: kubectl --namespace knative-debug get channel.messaging.knative.dev chan -o jsonpath = '{.status.address.hostname}' This should return a URI, likely ending in '.cluster.local'. If it doesn't, then it implies that something went wrong during reconciliation. See Channel Controller . We will verify that the two resources that the chan creates exist and are Ready .","title":"chan"},{"location":"eventing/troubleshooting/#service","text":"chan creates a K8s Service . kubectl --namespace knative-debug get service -l messaging.knative.dev/role = in -memory-channel It's spec is completely unimportant, as Istio will ignore it. It just needs to exist so that src can send events to it. If it doesn't exist, it implies that something went wrong during chan reconciliation. See Channel Controller .","title":"Service"},{"location":"eventing/troubleshooting/#src","text":"src is a ApiServerSource . First we will verify that src is writing to chan . kubectl --namespace knative-debug get apiserversource src -o jsonpath = '{.spec.sink}' Which should return map[apiVersion:messaging.knative.dev/v1 kind:Channel name:chan] . If it doesn't, then src was setup incorrectly and its spec needs to be fixed. Fixing should be as simple as updating its spec to have the correct sink (see example.yaml ). Now that we know src is sending to chan , let's verify that it is Ready . kubectl --namespace knative-debug get apiserversource src -o jsonpath = '{.status.conditions[?(.type == \"Ready\")].status}'","title":"src"},{"location":"eventing/troubleshooting/#sub","text":"sub is a Subscription from chan to fn . Verify that sub is Ready : kubectl --namespace knative-debug get subscription sub -o jsonpath = '{.status.conditions[?(.type == \"Ready\")].status}' This should return True . If it doesn't then, look at all the status entries. kubectl --namespace knative-debug get subscription sub --output yaml","title":"sub"},{"location":"eventing/troubleshooting/#controllers","text":"Each of the resources has a Controller that is watching it. As of today, they tend to do a poor job of writing failure status messages and events, so we need to look at the Controller's logs. Note The Kubernetes Deployment Controller, which controls fn , is out of scope for this document.","title":"Controllers"},{"location":"eventing/troubleshooting/#service-controller","text":"The Kubernetes Service Controller, controlling svc , is out of scope for this document.","title":"Service Controller"},{"location":"eventing/troubleshooting/#channel-controller","text":"There is not a single Channel Controller. Instead, there is one Controller for each Channel CRD. chan uses the InMemoryChannel Channel CRD , whose Controller is: kubectl --namespace knative-eventing get pod -l messaging.knative.dev/channel = in -memory-channel,messaging.knative.dev/role = controller --output yaml See its logs with: kubectl --namespace knative-eventing logs -l messaging.knative.dev/channel = in -memory-channel,messaging.knative.dev/role = controller Pay particular attention to any lines that have a logging level of warning or error .","title":"Channel Controller"},{"location":"eventing/troubleshooting/#source-controller","text":"Each Source will have its own Controller. src is a ApiServerSource , so its Controller is: kubectl --namespace knative-eventing get pod -l app = sources-controller This is actually a single binary that runs multiple Source Controllers, importantly including ApiServerSource Controller .","title":"Source Controller"},{"location":"eventing/troubleshooting/#apiserversource-controller","text":"The ApiServerSource Controller is run in the same binary as some other Source Controllers from Eventing. It is: kubectl --namespace knative-debug get pod -l eventing.knative.dev/sourceName = src,eventing.knative.dev/source = apiserver-source-controller View its logs with: kubectl --namespace knative-debug logs -l eventing.knative.dev/sourceName = src,eventing.knative.dev/source = apiserver-source-controller Pay particular attention to any lines that have a logging level of warning or error .","title":"ApiServerSource Controller"},{"location":"eventing/troubleshooting/#subscription-controller","text":"The Subscription Controller controls sub . It attempts to resolve the addresses that a Channel should send events to, and once resolved, inject those into the Channel 's spec.subscribable . kubectl --namespace knative-eventing get pod -l app = eventing-controller View its logs with: kubectl --namespace knative-eventing logs -l app = eventing-controller Pay particular attention to any lines that have a logging level of warning or error .","title":"Subscription Controller"},{"location":"eventing/troubleshooting/#data-plane","text":"The entire Control Plane looks healthy, but we're still not getting any events. Now we need to investigate the data plane. The Knative event takes the following path: Event is generated by src . In this case, it is caused by having a Kubernetes Event trigger it, but as far as Knative is concerned, the Source is generating the event denovo (from nothing). src is POSTing the event to chan 's address, http://chan-kn-channel.knative-debug.svc.cluster.local . The Channel Dispatcher receives the request and introspects the Host header to determine which Channel it corresponds to. It sees that it corresponds to knative-debug/chan so forwards the request to the subscribers defined in sub , in particular svc , which is backed by fn . fn receives the request and logs it. We will investigate components in the order in which events should travel.","title":"Data Plane"},{"location":"eventing/troubleshooting/#channel-dispatcher","text":"The Channel Dispatcher is the component that receives POSTs pushing events into Channel s and then POSTs to subscribers of those Channel s when an event is received. For the in-memory-channel used in this example, there is a single binary that handles both the receiving and dispatching sides for all in-memory-channel Channel s. First we will inspect the Dispatcher's logs to see if it is anything obvious: kubectl --namespace knative-eventing logs -l messaging.knative.dev/channel = in -memory-channel,messaging.knative.dev/role = dispatcher -c dispatcher Ideally we will see lines like: { \"level\" : \"info\" , \"ts\" : \"2019-08-16T13:50:55.424Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_receiver.go:147\" , \"msg\" : \"Request mapped to channel: knative-debug/chan-kn-channel\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"info\" , \"ts\" : \"2019-08-16T13:50:55.425Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_dispatcher.go:112\" , \"msg\" : \"Dispatching message to http://svc.knative-debug.svc.cluster.local/\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"info\" , \"ts\" : \"2019-08-16T13:50:55.981Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_receiver.go:140\" , \"msg\" : \"Received request for chan-kn-channel.knative-debug.svc.cluster.local\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } Which shows that the request is being received and then sent to svc , which is returning a 2XX response code (likely 200, 202, or 204). However if we see something like: { \"level\" : \"info\" , \"ts\" : \"2019-08-16T16:10:16.859Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_receiver.go:140\" , \"msg\" : \"Received request for chan-kn-channel.knative-debug.svc.cluster.local\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"info\" , \"ts\" : \"2019-08-16T16:10:16.859Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_receiver.go:147\" , \"msg\" : \"Request mapped to channel: knative-debug/chan-kn-channel\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"info\" , \"ts\" : \"2019-08-16T16:10:16.859Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"provisioners/message_dispatcher.go:112\" , \"msg\" : \"Dispatching message to http://svc.knative-debug.svc.cluster.local/\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" } { \"level\" : \"error\" , \"ts\" : \"2019-08-16T16:10:38.169Z\" , \"logger\" : \"inmemorychannel-dispatcher.in-memory-channel-dispatcher\" , \"caller\" : \"fanout/fanout_handler.go:121\" , \"msg\" : \"Fanout had an error\" , \"knative.dev/controller\" : \"in-memory-channel-dispatcher\" , \"error\" : \"Unable to complete request Post http://svc.knative-debug.svc.cluster.local/: dial tcp 10.4.44.156:80: i/o timeout\" , \"stacktrace\" : \"knative.dev/eventing/pkg/provisioners/fanout.(*Handler).dispatch\\n\\t/Users/xxxxxx/go/src/knative.dev/eventing/pkg/provisioners/fanout/fanout_handler.go:121\\nknative.dev/eventing/pkg/provisioners/fanout.createReceiverFunction.func1.1\\n\\t/Users/i512777/go/src/knative.dev/eventing/pkg/provisioners/fanout/fanout_handler.go:95\" } Then we know there was a problem posting to http://svc.knative-debug.svc.cluster.local/ .","title":"Channel Dispatcher"},{"location":"getting-started/","text":"\u6b22\u8fce\u6765\u5230 Knative \u5feb\u901f\u5165\u95e8\u6559\u7a0b \u00b6 \u672c\u6559\u7a0b\u8ba9\u60a8\u8bd5\u7528 Knative \u4e2d\u7684\u4e00\u4e9b\u5e38\u7528\u529f\u80fd\u3002 \u9996\u5148\uff0c\u4f60\u9700\u8981\u901a\u8fc7 quickstart \u63d2\u4ef6\u5728\u672c\u5730\u5b89\u88c5Knative\u3002\u8fd9\u662f\u4f60\u5b66\u4e60\u540e\u9762\u6559\u7a0b\u7684\u524d\u63d0\u3002 \u8be5\u63d2\u4ef6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7528\u4e8e\u5f00\u53d1\u7684 Knative \u90e8\u7f72\uff0c\u5b83\u5df2\u7ecf\u914d\u7f6e\u4e86\u672c\u6559\u7a0b\u6240\u9700\u7684\u7ec4\u4ef6\u3002 \u7136\u540e\uff0c\u5728\u60a8\u7684\u65b0\u96c6\u7fa4\u4e0a\uff0c\u60a8\u5c06\u6267\u884c Knative Serving \u548c Knative Eventing \u7684\u5e38\u89c1\u4efb\u52a1\u3002 \u6211\u4eec\u5efa\u8bae\u60a8\u6309\u987a\u5e8f\u5b8c\u6210\u672c\u6559\u7a0b\u4e2d\u7684\u4e3b\u9898\u3002 \u63d0\u793a \u70b9\u51fb\u952e\u76d8\u4e0a\u7684 . (\u53e5\u70b9) \u4ee5\u5728\u6559\u7a0b\u4e2d\u7ee7\u7eed\u524d\u8fdb. \u4f7f\u7528 , (\u9017\u53f7) \u968f\u65f6\u8fd4\u56de\u3002","title":"\u6559\u7a0b\u4ecb\u7ecd"},{"location":"getting-started/#knative","text":"\u672c\u6559\u7a0b\u8ba9\u60a8\u8bd5\u7528 Knative \u4e2d\u7684\u4e00\u4e9b\u5e38\u7528\u529f\u80fd\u3002 \u9996\u5148\uff0c\u4f60\u9700\u8981\u901a\u8fc7 quickstart \u63d2\u4ef6\u5728\u672c\u5730\u5b89\u88c5Knative\u3002\u8fd9\u662f\u4f60\u5b66\u4e60\u540e\u9762\u6559\u7a0b\u7684\u524d\u63d0\u3002 \u8be5\u63d2\u4ef6\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7528\u4e8e\u5f00\u53d1\u7684 Knative \u90e8\u7f72\uff0c\u5b83\u5df2\u7ecf\u914d\u7f6e\u4e86\u672c\u6559\u7a0b\u6240\u9700\u7684\u7ec4\u4ef6\u3002 \u7136\u540e\uff0c\u5728\u60a8\u7684\u65b0\u96c6\u7fa4\u4e0a\uff0c\u60a8\u5c06\u6267\u884c Knative Serving \u548c Knative Eventing \u7684\u5e38\u89c1\u4efb\u52a1\u3002 \u6211\u4eec\u5efa\u8bae\u60a8\u6309\u987a\u5e8f\u5b8c\u6210\u672c\u6559\u7a0b\u4e2d\u7684\u4e3b\u9898\u3002 \u63d0\u793a \u70b9\u51fb\u952e\u76d8\u4e0a\u7684 . (\u53e5\u70b9) \u4ee5\u5728\u6559\u7a0b\u4e2d\u7ee7\u7eed\u524d\u8fdb. \u4f7f\u7528 , (\u9017\u53f7) \u968f\u65f6\u8fd4\u56de\u3002","title":"\u6b22\u8fce\u6765\u5230 Knative \u5feb\u901f\u5165\u95e8\u6559\u7a0b"},{"location":"getting-started/clean-up/","text":"Clean Up \u00b6 We recommend that you delete the cluster used for this tutorial to free up resources on your local machine. If you want to continue experimenting with Knative after deleting the cluster, you can reinstall Knative on a new cluster using the quickstart plugin again. Delete the Cluster \u00b6 kind minikube Delete your kind cluster by running the command: kind delete clusters knative Example output Deleted clusters: [ \"knative\" ] Delete your minikube cluster by running the command: minikube delete -p knative Example output \ud83d\udd25 Deleting \"knative\" in hyperkit ... \ud83d\udc80 Removed all traces of the \"knative\" cluster.","title":"Clean Up"},{"location":"getting-started/clean-up/#clean-up","text":"We recommend that you delete the cluster used for this tutorial to free up resources on your local machine. If you want to continue experimenting with Knative after deleting the cluster, you can reinstall Knative on a new cluster using the quickstart plugin again.","title":"Clean Up"},{"location":"getting-started/clean-up/#delete-the-cluster","text":"kind minikube Delete your kind cluster by running the command: kind delete clusters knative Example output Deleted clusters: [ \"knative\" ] Delete your minikube cluster by running the command: minikube delete -p knative Example output \ud83d\udd25 Deleting \"knative\" in hyperkit ... \ud83d\udc80 Removed all traces of the \"knative\" cluster.","title":"Delete the Cluster"},{"location":"getting-started/first-autoscale/","text":"Scaling to Zero \u00b6 Remember those super powers we talked about? One of Knative Serving's powers is built-in automatic scaling, also known as autoscaling . This means your Knative Service only spins up your application to perform its job (in this case, saying \"Hello world!\") if it is needed. Otherwise, it will scale to zero by spinning down and waiting for a new request to come in. What about scaling up to meet increased demand? Knative Autoscaling also allows you to easily configure your service to scale up (horizontal autoscaling) to meet increased demand as well as control the number of instances that spin up using concurrency limits and other options , but that's beyond the scope of this tutorial. Let's see this in action! We're going to peek under the hood at the Pod in Kubernetes where our Knative Service is running to watch our \"Hello world!\" Service scale up and down. Watch your Knative Service scale to zero \u00b6 Let's run our \"Hello world!\" Service just one more time. This time, try the Knative Service URL in your browser or you can use your terminal with curl . echo \"Accessing URL $( kn service describe hello -o url ) \" curl \" $( kn service describe hello -o url ) \" Now watch the pods and see how they scale to zero after traffic stops going to the URL. kubectl get pod -l serving.knative.dev/service = hello -w Warning It may take up to 2 minutes for your Pods to scale down. Pinging your service again will reset this timer. Expected output NAME READY STATUS hello-world 2 /2 Running hello-world 2 /2 Terminating hello-world 1 /2 Terminating hello-world 0 /2 Terminating Scale up your Knative Service \u00b6 Rerun the Knative Service in your browser and you will see a new pod running again. Expected output NAME READY STATUS hello-world 0 /2 Pending hello-world 0 /2 ContainerCreating hello-world 1 /2 Running hello-world 2 /2 Running Exit the kubectl watch command with Ctrl+c . Some people call this Serverless Up next, traffic splitting! Want to go deeper on Autoscaling? Interested in getting in the weeds with Knative Autoscaling? Check out the autoscaling documentation for concepts, samples, and more!","title":"\u7f29\u5bb9\u5230\u96f6"},{"location":"getting-started/first-autoscale/#scaling-to-zero","text":"Remember those super powers we talked about? One of Knative Serving's powers is built-in automatic scaling, also known as autoscaling . This means your Knative Service only spins up your application to perform its job (in this case, saying \"Hello world!\") if it is needed. Otherwise, it will scale to zero by spinning down and waiting for a new request to come in. What about scaling up to meet increased demand? Knative Autoscaling also allows you to easily configure your service to scale up (horizontal autoscaling) to meet increased demand as well as control the number of instances that spin up using concurrency limits and other options , but that's beyond the scope of this tutorial. Let's see this in action! We're going to peek under the hood at the Pod in Kubernetes where our Knative Service is running to watch our \"Hello world!\" Service scale up and down.","title":"Scaling to Zero"},{"location":"getting-started/first-autoscale/#watch-your-knative-service-scale-to-zero","text":"Let's run our \"Hello world!\" Service just one more time. This time, try the Knative Service URL in your browser or you can use your terminal with curl . echo \"Accessing URL $( kn service describe hello -o url ) \" curl \" $( kn service describe hello -o url ) \" Now watch the pods and see how they scale to zero after traffic stops going to the URL. kubectl get pod -l serving.knative.dev/service = hello -w Warning It may take up to 2 minutes for your Pods to scale down. Pinging your service again will reset this timer. Expected output NAME READY STATUS hello-world 2 /2 Running hello-world 2 /2 Terminating hello-world 1 /2 Terminating hello-world 0 /2 Terminating","title":"Watch your Knative Service scale to zero"},{"location":"getting-started/first-autoscale/#scale-up-your-knative-service","text":"Rerun the Knative Service in your browser and you will see a new pod running again. Expected output NAME READY STATUS hello-world 0 /2 Pending hello-world 0 /2 ContainerCreating hello-world 1 /2 Running hello-world 2 /2 Running Exit the kubectl watch command with Ctrl+c . Some people call this Serverless Up next, traffic splitting! Want to go deeper on Autoscaling? Interested in getting in the weeds with Knative Autoscaling? Check out the autoscaling documentation for concepts, samples, and more!","title":"Scale up your Knative Service"},{"location":"getting-started/first-broker/","text":"Sources, Brokers, Triggers, Sinks, oh my! \u00b6 For the purposes of this tutorial, let's keep it simple. You will focus on four powerful Eventing components: Source , Trigger , Broker , and Sink . Let's take a look at how these components interact: Component Basic Definition Source A Kubernetes Custom Resource which emits events to the Broker. Broker A \"hub\" for events in your infrastructure; a central location to send events for delivery. Trigger Acts as a filter for events entering the broker, can be configured with desired event attributes. Sink A destination for events. A note on Sources and Sinks A Knative Service can act as both a Source and a Sink for events, and for good reason. You may want to consume events from the Broker and send modified events back to the Broker, as you would in any pipeline use-case. CloudEvents \u00b6 Knative Eventing uses CloudEvents to send information back and forth between your Services and these components. What are CloudEvents? For our purposes, the only thing you need to know about CloudEvents are: CloudEvents can carry some attributes (like id, Source, type, etc) as well as data payloads (JSON, plaintext, reference to data that lives elsewhere, etc). CloudEvents can be \"emitted\" by almost anything and can be transported to anywhere in your deployment. CloudEvents follow the CloudEvents 1.0 Specification , with required and optional attributes. To find out more about CloudEvents, check out the CloudEvents website ! Examining the Broker \u00b6 As part of the kn quickstart install, an In-Memory Broker should already be installed in your Cluster. Check to see that it is installed by running the command: kn broker list Expected output NAME URL AGE CONDITIONS READY REASON example-broker http://broker-ingress.knative-eventing.svc.cluster.local/default/example-broker 5m 5 OK / 5 True Warning In-Memory Brokers are for development use only and must not be used in a production deployment. Are there any other components of Knative Eventing? Though it is out of scope for this tutorial, Knative Eventing has many components which can be used in many ways to suit your needs. If you want to find out more about the different components of Knative Eventing, such as Channels, Sequences and Parallel flows, check out the Knative Eventing documentation . Next, you'll take a look at a simple implementation of Sources, Brokers, Triggers and Sinks using an app called the CloudEvents Player.","title":"\u4e8b\u4ef6\u6e90, \u4ee3\u7406, \u89e6\u53d1\u5668, Sinks"},{"location":"getting-started/first-broker/#sources-brokers-triggers-sinks-oh-my","text":"For the purposes of this tutorial, let's keep it simple. You will focus on four powerful Eventing components: Source , Trigger , Broker , and Sink . Let's take a look at how these components interact: Component Basic Definition Source A Kubernetes Custom Resource which emits events to the Broker. Broker A \"hub\" for events in your infrastructure; a central location to send events for delivery. Trigger Acts as a filter for events entering the broker, can be configured with desired event attributes. Sink A destination for events. A note on Sources and Sinks A Knative Service can act as both a Source and a Sink for events, and for good reason. You may want to consume events from the Broker and send modified events back to the Broker, as you would in any pipeline use-case.","title":"Sources, Brokers, Triggers, Sinks, oh my!"},{"location":"getting-started/first-broker/#cloudevents","text":"Knative Eventing uses CloudEvents to send information back and forth between your Services and these components. What are CloudEvents? For our purposes, the only thing you need to know about CloudEvents are: CloudEvents can carry some attributes (like id, Source, type, etc) as well as data payloads (JSON, plaintext, reference to data that lives elsewhere, etc). CloudEvents can be \"emitted\" by almost anything and can be transported to anywhere in your deployment. CloudEvents follow the CloudEvents 1.0 Specification , with required and optional attributes. To find out more about CloudEvents, check out the CloudEvents website !","title":"CloudEvents"},{"location":"getting-started/first-broker/#examining-the-broker","text":"As part of the kn quickstart install, an In-Memory Broker should already be installed in your Cluster. Check to see that it is installed by running the command: kn broker list Expected output NAME URL AGE CONDITIONS READY REASON example-broker http://broker-ingress.knative-eventing.svc.cluster.local/default/example-broker 5m 5 OK / 5 True Warning In-Memory Brokers are for development use only and must not be used in a production deployment. Are there any other components of Knative Eventing? Though it is out of scope for this tutorial, Knative Eventing has many components which can be used in many ways to suit your needs. If you want to find out more about the different components of Knative Eventing, such as Channels, Sequences and Parallel flows, check out the Knative Eventing documentation . Next, you'll take a look at a simple implementation of Sources, Brokers, Triggers and Sinks using an app called the CloudEvents Player.","title":"Examining the Broker"},{"location":"getting-started/first-service/","text":"\u90e8\u7f72\u4f60\u7684\u7b2c\u4e00\u4e2a Knative Service \u00b6 \u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u60a8\u5c06\u90e8\u7f72\u4e00\u4e2a\u201cHello world\u201d service \u3002 \u7531\u4e8e\u6211\u4eec\u7684\u201cHello world\u201d\u670d\u52a1\u88ab\u90e8\u7f72\u4e3a Knative Service\uff0c\u800c\u4e0d\u662f Kubernetes Service\uff0c\u56e0\u6b64\u5b83\u83b7\u5f97\u4e86\u4e00\u4e9b \u5f00\u7bb1\u5373\u7528\u7684\u8d85\u80fd\u529b . Knative Service: \"Hello world!\" \u00b6 \u9996\u5148, \u90e8\u7f72 Knative Service. \u8fd9\u4e2a service \u63a5\u53d7\u73af\u5883\u53d8\u91cf, TARGET , \u5e76\u6253\u5370 Hello ${TARGET}! . kn YAML \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u90e8\u7f72\u670d\u52a1\uff1a kn service create hello \\ --image gcr.io/knative-samples/helloworld-go \\ --port 8080 \\ --env TARGET = World \u9884\u671f\u8f93\u51fa Service hello created to latest revision 'hello-world' is available at URL: http://hello.default. ${ LOADBALANCER_IP } .sslip.io \u4e0a\u9762 ${LOADBALANCER_IP} \u7684\u503c\u53d6\u51b3\u4e8e\u4f60\u7684\u96c6\u7fa4\u7c7b\u578b\u3002\u5982\u679c\u662f kind , \u503c\u4e3a 127.0.0.1 ; \u5982\u679c\u662f minikube \uff0c\u5219\u53d6\u51b3\u4e8e\u672c\u5730\u96a7\u9053\uff08local tunnel\uff09\u3002 \u5c06\u4ee5\u4e0b\u5185\u5bb9\u590d\u5236\u5230\u4e00\u4e2a\u540d\u4e3a hello.yaml \u7684\u6587\u4ef6: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : hello spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go ports : - containerPort : 8080 env : - name : TARGET value : \"World\" \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u90e8\u7f72 Knative Service\uff1a kubectl apply -f hello.yaml \u9884\u671f\u8f93\u51fa service.serving.knative.dev/hello created \u5217\u51fa\u4f60\u7684Knative Service \u00b6 \u8981\u67e5\u770b\u6258\u7ba1 Knative Service\u7684 URL\uff0c\u8bf7\u5229\u7528 kn CLI\uff1a kn kubectl \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u67e5\u770b Knative services\u5217\u8868\uff1a kn service list \u9884\u671f\u8f93\u51fa NAME URL LATEST AGE CONDITIONS READY hello http://hello.default. ${ LOADBALANCER_IP } .sslip.io hello-00001 13s 3 OK / 3 True \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u67e5\u770b Knative \u670d\u52a1\u5217\u8868\uff1a kubectl get ksvc \u9884\u671f\u8f93\u51fa NAME URL LATESTCREATED LATESTREADY READY REASON hello http://hello.default. ${ LOADBALANCER_IP } .sslip.io hello-00001 hello-00001 True \u8bbf\u95ee\u4f60\u7684 Knative Service \u00b6 \u901a\u8fc7\u5728\u6d4f\u89c8\u5668\u4e2d\u6253\u5f00\u4e4b\u524d\u7684 URL \u6216\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6765\u8bbf\u95ee\u60a8\u7684 Knative Service\uff1a echo \"Accessing URL $( kn service describe hello -o url ) \" curl \" $( kn service describe hello -o url ) \" \u9884\u671f\u8f93\u51fa Hello World! \u4f60\u9047\u5230\u4ee5\u4e0b\u9519\u8bef\u5417\uff1f curl: (6) Could not resolve host: hello.default.${LOADBALANCER_IP}.sslip.io \u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4f60\u7684DNS\u670d\u52a1\u53ef\u80fd\u6ca1\u6709\u914d\u7f6e\u5bf9 *.sslip.io \u57df\u540d\u8fdb\u884c\u89e3\u6790\u3002\u5982\u679c\u60a8\u9047\u5230\u6b64\u95ee\u9898\uff0c\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528\u4e0d\u540c\u7684\u540d\u5b57\u670d\u52a1\u5668\u6765\u89e3\u6790\u8fd9\u4e9b\u5730\u5740\u6765\u89e3\u51b3\u6b64\u95ee\u9898\u3002 \u786e\u5207\u7684\u6b65\u9aa4\u5c06\u6839\u636e\u60a8\u7684\u64cd\u4f5c\u7cfb\u7edf\u7248\u672c\u800c\u6709\u6240\u4e0d\u540c\u3002\u4f8b\u5982\uff0c\u5bf9\u4e8e\u4f7f\u7528 systemd-resolved \u7684Ubuntu \u7cfb\u7edf\uff0c\u60a8\u53ef\u4ee5\u5c06\u4ee5\u4e0b\u6761\u76ee\u6dfb\u52a0\u5230 /etc/systemd/resolved.conf \uff1a [Resolve] DNS = 8.8.8.8 Domains = ~sslip.io. Then simply restart the service with sudo service systemd-resolved restart . For MacOS users, you can add the DNS and domain using the network settings as explained here . \u606d\u559c , \u60a8\u521a\u521a\u521b\u5efa\u4e86\u60a8\u7684\u7b2c\u4e00\u4e2a Knative \u670d\u52a1\u3002\u63a5\u4e0b\u6765\uff0c\u81ea\u52a8\u4f38\u7f29\uff01","title":"\u7b2c\u4e00\u4e2aKnative Service"},{"location":"getting-started/first-service/#knative-service","text":"\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u60a8\u5c06\u90e8\u7f72\u4e00\u4e2a\u201cHello world\u201d service \u3002 \u7531\u4e8e\u6211\u4eec\u7684\u201cHello world\u201d\u670d\u52a1\u88ab\u90e8\u7f72\u4e3a Knative Service\uff0c\u800c\u4e0d\u662f Kubernetes Service\uff0c\u56e0\u6b64\u5b83\u83b7\u5f97\u4e86\u4e00\u4e9b \u5f00\u7bb1\u5373\u7528\u7684\u8d85\u80fd\u529b .","title":"\u90e8\u7f72\u4f60\u7684\u7b2c\u4e00\u4e2a Knative Service"},{"location":"getting-started/first-service/#knative-service-hello-world","text":"\u9996\u5148, \u90e8\u7f72 Knative Service. \u8fd9\u4e2a service \u63a5\u53d7\u73af\u5883\u53d8\u91cf, TARGET , \u5e76\u6253\u5370 Hello ${TARGET}! . kn YAML \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u90e8\u7f72\u670d\u52a1\uff1a kn service create hello \\ --image gcr.io/knative-samples/helloworld-go \\ --port 8080 \\ --env TARGET = World \u9884\u671f\u8f93\u51fa Service hello created to latest revision 'hello-world' is available at URL: http://hello.default. ${ LOADBALANCER_IP } .sslip.io \u4e0a\u9762 ${LOADBALANCER_IP} \u7684\u503c\u53d6\u51b3\u4e8e\u4f60\u7684\u96c6\u7fa4\u7c7b\u578b\u3002\u5982\u679c\u662f kind , \u503c\u4e3a 127.0.0.1 ; \u5982\u679c\u662f minikube \uff0c\u5219\u53d6\u51b3\u4e8e\u672c\u5730\u96a7\u9053\uff08local tunnel\uff09\u3002 \u5c06\u4ee5\u4e0b\u5185\u5bb9\u590d\u5236\u5230\u4e00\u4e2a\u540d\u4e3a hello.yaml \u7684\u6587\u4ef6: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : hello spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go ports : - containerPort : 8080 env : - name : TARGET value : \"World\" \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u90e8\u7f72 Knative Service\uff1a kubectl apply -f hello.yaml \u9884\u671f\u8f93\u51fa service.serving.knative.dev/hello created","title":"Knative Service: \"Hello world!\""},{"location":"getting-started/first-service/#knative-service_1","text":"\u8981\u67e5\u770b\u6258\u7ba1 Knative Service\u7684 URL\uff0c\u8bf7\u5229\u7528 kn CLI\uff1a kn kubectl \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u67e5\u770b Knative services\u5217\u8868\uff1a kn service list \u9884\u671f\u8f93\u51fa NAME URL LATEST AGE CONDITIONS READY hello http://hello.default. ${ LOADBALANCER_IP } .sslip.io hello-00001 13s 3 OK / 3 True \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u67e5\u770b Knative \u670d\u52a1\u5217\u8868\uff1a kubectl get ksvc \u9884\u671f\u8f93\u51fa NAME URL LATESTCREATED LATESTREADY READY REASON hello http://hello.default. ${ LOADBALANCER_IP } .sslip.io hello-00001 hello-00001 True","title":"\u5217\u51fa\u4f60\u7684Knative Service"},{"location":"getting-started/first-service/#knative-service_2","text":"\u901a\u8fc7\u5728\u6d4f\u89c8\u5668\u4e2d\u6253\u5f00\u4e4b\u524d\u7684 URL \u6216\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u6765\u8bbf\u95ee\u60a8\u7684 Knative Service\uff1a echo \"Accessing URL $( kn service describe hello -o url ) \" curl \" $( kn service describe hello -o url ) \" \u9884\u671f\u8f93\u51fa Hello World! \u4f60\u9047\u5230\u4ee5\u4e0b\u9519\u8bef\u5417\uff1f curl: (6) Could not resolve host: hello.default.${LOADBALANCER_IP}.sslip.io \u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u4f60\u7684DNS\u670d\u52a1\u53ef\u80fd\u6ca1\u6709\u914d\u7f6e\u5bf9 *.sslip.io \u57df\u540d\u8fdb\u884c\u89e3\u6790\u3002\u5982\u679c\u60a8\u9047\u5230\u6b64\u95ee\u9898\uff0c\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528\u4e0d\u540c\u7684\u540d\u5b57\u670d\u52a1\u5668\u6765\u89e3\u6790\u8fd9\u4e9b\u5730\u5740\u6765\u89e3\u51b3\u6b64\u95ee\u9898\u3002 \u786e\u5207\u7684\u6b65\u9aa4\u5c06\u6839\u636e\u60a8\u7684\u64cd\u4f5c\u7cfb\u7edf\u7248\u672c\u800c\u6709\u6240\u4e0d\u540c\u3002\u4f8b\u5982\uff0c\u5bf9\u4e8e\u4f7f\u7528 systemd-resolved \u7684Ubuntu \u7cfb\u7edf\uff0c\u60a8\u53ef\u4ee5\u5c06\u4ee5\u4e0b\u6761\u76ee\u6dfb\u52a0\u5230 /etc/systemd/resolved.conf \uff1a [Resolve] DNS = 8.8.8.8 Domains = ~sslip.io. Then simply restart the service with sudo service systemd-resolved restart . For MacOS users, you can add the DNS and domain using the network settings as explained here . \u606d\u559c , \u60a8\u521a\u521a\u521b\u5efa\u4e86\u60a8\u7684\u7b2c\u4e00\u4e2a Knative \u670d\u52a1\u3002\u63a5\u4e0b\u6765\uff0c\u81ea\u52a8\u4f38\u7f29\uff01","title":"\u8bbf\u95ee\u4f60\u7684 Knative Service"},{"location":"getting-started/first-source/","text":"Using a Knative Service as a source \u00b6 In this tutorial, you will use the CloudEvents Player app to showcase the core concepts of Knative Eventing. By the end of this tutorial, you should have an architecture that looks like this: The above image is Figure 6.6 from Knative in Action . Creating your first source \u00b6 The CloudEvents Player acts as a Source for CloudEvents by intaking the URL of the Broker as an environment variable, BROKER_URL . You will send CloudEvents to the Broker through the CloudEvents Player application. Create the CloudEvents Player Service: kn YAML Run the command: kn service create cloudevents-player \\ --image ruromero/cloudevents-player:latest \\ --env BROKER_URL = http://broker-ingress.knative-eventing.svc.cluster.local/default/example-broker Expected output Service 'cloudevents-player' created to latest revision 'cloudevents-player-vwybw-1' is available at URL: http://cloudevents-player.default. ${ LOADBALANCER_IP } .sslip.io Why is my Revision named something different! Because we didn't assign a revision-name , Knative Serving automatically created one for us. It's okay if your Revision is named something different. Copy the following YAML into a file named cloudevents-player.yaml : apiVersion: serving.knative.dev/v1 kind: Service metadata: name: cloudevents-player spec: template: metadata: annotations: autoscaling.knative.dev/min-scale: \"1\" spec: containers: - image: ruromero/cloudevents-player:latest env: - name: BROKER_URL value: http://broker-ingress.knative-eventing.svc.cluster.local/default/example-broker Apply the YAML file by running the command: kubectl apply -f cloudevents-player.yaml Expected output service.serving.knative.dev/cloudevents-player created Examining the CloudEvents Player \u00b6 You can use the CloudEvents Player to send and receive CloudEvents. If you open the Service URL in your browser, the Create Event form appears. The Service URL is http://cloudevents-player.default.${LOADBALANCER_IP}.sslip.io , for example, http://cloudevents-player.default.127.0.0.1.sslip.io for kind . What do these fields mean? Field Description Event ID A unique ID. Click the loop icon to generate a new one. Event Type An event type. Event Source An event source. Specversion Demarcates which CloudEvents spec you're using (should always be 1.0). Message The data section of the CloudEvent, a payload which is carrying the data you care to be delivered. For more information on the CloudEvents Specification, check out the CloudEvents Spec . Sending an event \u00b6 Try sending an event using the CloudEvents Player interface: Fill in the form with whatever you data you want. Ensure your Event Source does not contain any spaces. Click SEND EVENT . Clicking the shows you the CloudEvent as the Broker sees it. Want to send events using the command line instead? As an alternative to the Web form, events can also be sent/viewed using the command line. To post an event: curl -i http://cloudevents-player.default. ${ LOADBALANCER_IP } .sslip.io \\ -H \"Content-Type: application/json\" \\ -H \"Ce-Id: 123456789\" \\ -H \"Ce-Specversion: 1.0\" \\ -H \"Ce-Type: some-type\" \\ -H \"Ce-Source: command-line\" \\ -d '{\"msg\":\"Hello CloudEvents!\"}' And to view events: curl http://cloudevents-player.default. ${ LOADBALANCER_IP } .sslip.io/messages The icon in the \"Status\" column implies that the event has been sent to our Broker... but where has the event gone? Well, right now, nowhere! A Broker is simply a receptacle for events. In order for your events to be sent anywhere, you must create a Trigger which listens for your events and places them somewhere. And, you're in luck; you'll create your first Trigger on the next page!","title":"\u5c06 Knative Service \u4f5c\u4e3a\u4e8b\u4ef6\u6e90"},{"location":"getting-started/first-source/#using-a-knative-service-as-a-source","text":"In this tutorial, you will use the CloudEvents Player app to showcase the core concepts of Knative Eventing. By the end of this tutorial, you should have an architecture that looks like this: The above image is Figure 6.6 from Knative in Action .","title":"Using a Knative Service as a source"},{"location":"getting-started/first-source/#creating-your-first-source","text":"The CloudEvents Player acts as a Source for CloudEvents by intaking the URL of the Broker as an environment variable, BROKER_URL . You will send CloudEvents to the Broker through the CloudEvents Player application. Create the CloudEvents Player Service: kn YAML Run the command: kn service create cloudevents-player \\ --image ruromero/cloudevents-player:latest \\ --env BROKER_URL = http://broker-ingress.knative-eventing.svc.cluster.local/default/example-broker Expected output Service 'cloudevents-player' created to latest revision 'cloudevents-player-vwybw-1' is available at URL: http://cloudevents-player.default. ${ LOADBALANCER_IP } .sslip.io Why is my Revision named something different! Because we didn't assign a revision-name , Knative Serving automatically created one for us. It's okay if your Revision is named something different. Copy the following YAML into a file named cloudevents-player.yaml : apiVersion: serving.knative.dev/v1 kind: Service metadata: name: cloudevents-player spec: template: metadata: annotations: autoscaling.knative.dev/min-scale: \"1\" spec: containers: - image: ruromero/cloudevents-player:latest env: - name: BROKER_URL value: http://broker-ingress.knative-eventing.svc.cluster.local/default/example-broker Apply the YAML file by running the command: kubectl apply -f cloudevents-player.yaml Expected output service.serving.knative.dev/cloudevents-player created","title":"Creating your first source"},{"location":"getting-started/first-source/#examining-the-cloudevents-player","text":"You can use the CloudEvents Player to send and receive CloudEvents. If you open the Service URL in your browser, the Create Event form appears. The Service URL is http://cloudevents-player.default.${LOADBALANCER_IP}.sslip.io , for example, http://cloudevents-player.default.127.0.0.1.sslip.io for kind . What do these fields mean? Field Description Event ID A unique ID. Click the loop icon to generate a new one. Event Type An event type. Event Source An event source. Specversion Demarcates which CloudEvents spec you're using (should always be 1.0). Message The data section of the CloudEvent, a payload which is carrying the data you care to be delivered. For more information on the CloudEvents Specification, check out the CloudEvents Spec .","title":"Examining the CloudEvents Player"},{"location":"getting-started/first-source/#sending-an-event","text":"Try sending an event using the CloudEvents Player interface: Fill in the form with whatever you data you want. Ensure your Event Source does not contain any spaces. Click SEND EVENT . Clicking the shows you the CloudEvent as the Broker sees it. Want to send events using the command line instead? As an alternative to the Web form, events can also be sent/viewed using the command line. To post an event: curl -i http://cloudevents-player.default. ${ LOADBALANCER_IP } .sslip.io \\ -H \"Content-Type: application/json\" \\ -H \"Ce-Id: 123456789\" \\ -H \"Ce-Specversion: 1.0\" \\ -H \"Ce-Type: some-type\" \\ -H \"Ce-Source: command-line\" \\ -d '{\"msg\":\"Hello CloudEvents!\"}' And to view events: curl http://cloudevents-player.default. ${ LOADBALANCER_IP } .sslip.io/messages The icon in the \"Status\" column implies that the event has been sent to our Broker... but where has the event gone? Well, right now, nowhere! A Broker is simply a receptacle for events. In order for your events to be sent anywhere, you must create a Trigger which listens for your events and places them somewhere. And, you're in luck; you'll create your first Trigger on the next page!","title":"Sending an event"},{"location":"getting-started/first-traffic-split/","text":"Basics of Traffic Splitting \u00b6 The last super power of Knative Serving we'll go over in this tutorial is traffic splitting . What are some common traffic splitting use-cases? Splitting traffic is useful for a number of very common modern infrastructure needs, such as blue/green deployments and canary deployments . Bringing these industry standards to bear on Kubernetes is as simple as a single CLI command on Knative or YAML tweak, let's see how! Creating a new Revision \u00b6 A new Revision gets created each and every time you make changes to the configuration of your Knative Service. When splitting traffic, Knative splits traffic between different Revisions of your Knative Service. What exactly is a Revision? You can think of a Revision snapshot-in-time of application code and configuration. Create a new Revision \u00b6 Instead of TARGET=World update the environment variable TARGET on your Knative Service hello to greet \"Knative\" instead. kn YAML Deploy the updated version of your Knative Service by running the command: kn service update hello \\ --env TARGET = Knative As before, kn prints out some helpful information to the CLI. Expected output Service 'hello' created to latest revision 'hello-00002' is available at URL: http://hello.default. ${ LOADBALANCER_IP } .sslip.io Edit your existing hello.yaml file to contain the following: --- apiVersion: serving.knative.dev/v1 kind: Service metadata: name: hello spec: template: spec: containers: - image: gcr.io/knative-samples/helloworld-go ports: - containerPort: 8080 env: - name: TARGET value: \"Knative\" Deploy the updated version of your Knative Service by running the command: kubectl apply -f hello.yaml Expected output service.serving.knative.dev/hello configured Note that since we are updating an existing Knative Service hello , the URL doesn't change, but our new Revision should have the new name hello-00002 . Access the new Revision \u00b6 To see the change, access the Knative Service again on your browser or use curl in your terminal: echo \"Accessing URL $( kn service describe hello -o url ) \" curl \" $( kn service describe hello -o url ) \" Expected output Hello Knative! Splitting Traffic \u00b6 You may at this point be wondering, \"where did 'Hello World!' go?\" Remember, Revisions are an immutable snapshot-in-time of application code and configuration, so your old hello-00001 Revision is still available to you. List your Revisions \u00b6 We can easily see a list of our existing Revisions with the kn or kubectl CLI. kn kubectl View a list of revisions by running the command: kn revisions list Expected output NAME SERVICE TRAFFIC TAGS GENERATION AGE CONDITIONS READY REASON hello-00002 hello 100 % 2 30s 3 OK / 4 True hello-00001 hello 1 5m 3 OK / 4 True View a list of Revisions by running the command: kubectl get revisions Expected output NAME CONFIG NAME K8S SERVICE NAME GENERATION READY REASON ACTUAL REPLICAS DESIRED REPLICAS hello-00001 hello 1 True 0 0 hello-00002 hello 2 True 0 0 When running the kn command, the column most relevant for our purposes is TRAFFIC . We can see that 100% of traffic is going to our latest Revision, hello-00002 , which is on the row with the highest GENERATION . 0% of traffic is going to the Revision we configured earlier, hello-00001 . When you create a new Revision of a Knative Service, Knative defaults to directing 100% of traffic to this latest Revision. We can change this default behavior by specifying how much traffic we want each of our Revisions to receive. Split traffic between Revisions \u00b6 Lets split traffic between our two Revisions: kn YAML Run the command: kn service update hello \\ --traffic hello-00001 = 50 \\ --traffic @latest = 50 Add the traffic section to the bottom of your existing hello.yaml file: apiVersion: serving.knative.dev/v1 kind: Service metadata: name: hello spec: template: spec: containers: - image: gcr.io/knative-samples/helloworld-go ports: - containerPort: 8080 env: - name: TARGET value: \"Knative\" traffic: - latestRevision: true percent: 50 - latestRevision: false percent: 50 revisionName: hello-00001 Apply the YAML by running the command: kubectl apply -f hello.yaml Info @latest will always point to our \"latest\" Revision which, at the moment, is hello-00002 . Verify the traffic split \u00b6 To verify that the traffic split has configured correctly, list the revisions again by running the command: kn revisions list Expected output NAME SERVICE TRAFFIC TAGS GENERATION AGE CONDITIONS READY REASON hello-00002 hello 50 % 2 10m 3 OK / 4 True hello-00001 hello 50 % 1 36m 3 OK / 4 True Access your Knative Service multiple times in your browser to see the different output being served by each Revision. Similarly, you can access the Service URL from the terminal multiple times to see the traffic being split between the Revisions. echo \"Accessing URL $( kn service describe hello -o url ) \" curl \" $( kn service describe hello -o url ) \" Expected output Hello Knative! Hello World! Hello Knative! Hello World!","title":"\u6d41\u91cf\u62c6\u5206"},{"location":"getting-started/first-traffic-split/#basics-of-traffic-splitting","text":"The last super power of Knative Serving we'll go over in this tutorial is traffic splitting . What are some common traffic splitting use-cases? Splitting traffic is useful for a number of very common modern infrastructure needs, such as blue/green deployments and canary deployments . Bringing these industry standards to bear on Kubernetes is as simple as a single CLI command on Knative or YAML tweak, let's see how!","title":"Basics of Traffic Splitting"},{"location":"getting-started/first-traffic-split/#creating-a-new-revision","text":"A new Revision gets created each and every time you make changes to the configuration of your Knative Service. When splitting traffic, Knative splits traffic between different Revisions of your Knative Service. What exactly is a Revision? You can think of a Revision snapshot-in-time of application code and configuration.","title":"Creating a new Revision"},{"location":"getting-started/first-traffic-split/#create-a-new-revision","text":"Instead of TARGET=World update the environment variable TARGET on your Knative Service hello to greet \"Knative\" instead. kn YAML Deploy the updated version of your Knative Service by running the command: kn service update hello \\ --env TARGET = Knative As before, kn prints out some helpful information to the CLI. Expected output Service 'hello' created to latest revision 'hello-00002' is available at URL: http://hello.default. ${ LOADBALANCER_IP } .sslip.io Edit your existing hello.yaml file to contain the following: --- apiVersion: serving.knative.dev/v1 kind: Service metadata: name: hello spec: template: spec: containers: - image: gcr.io/knative-samples/helloworld-go ports: - containerPort: 8080 env: - name: TARGET value: \"Knative\" Deploy the updated version of your Knative Service by running the command: kubectl apply -f hello.yaml Expected output service.serving.knative.dev/hello configured Note that since we are updating an existing Knative Service hello , the URL doesn't change, but our new Revision should have the new name hello-00002 .","title":"Create a new Revision"},{"location":"getting-started/first-traffic-split/#access-the-new-revision","text":"To see the change, access the Knative Service again on your browser or use curl in your terminal: echo \"Accessing URL $( kn service describe hello -o url ) \" curl \" $( kn service describe hello -o url ) \" Expected output Hello Knative!","title":"Access the new Revision"},{"location":"getting-started/first-traffic-split/#splitting-traffic","text":"You may at this point be wondering, \"where did 'Hello World!' go?\" Remember, Revisions are an immutable snapshot-in-time of application code and configuration, so your old hello-00001 Revision is still available to you.","title":"Splitting Traffic"},{"location":"getting-started/first-traffic-split/#list-your-revisions","text":"We can easily see a list of our existing Revisions with the kn or kubectl CLI. kn kubectl View a list of revisions by running the command: kn revisions list Expected output NAME SERVICE TRAFFIC TAGS GENERATION AGE CONDITIONS READY REASON hello-00002 hello 100 % 2 30s 3 OK / 4 True hello-00001 hello 1 5m 3 OK / 4 True View a list of Revisions by running the command: kubectl get revisions Expected output NAME CONFIG NAME K8S SERVICE NAME GENERATION READY REASON ACTUAL REPLICAS DESIRED REPLICAS hello-00001 hello 1 True 0 0 hello-00002 hello 2 True 0 0 When running the kn command, the column most relevant for our purposes is TRAFFIC . We can see that 100% of traffic is going to our latest Revision, hello-00002 , which is on the row with the highest GENERATION . 0% of traffic is going to the Revision we configured earlier, hello-00001 . When you create a new Revision of a Knative Service, Knative defaults to directing 100% of traffic to this latest Revision. We can change this default behavior by specifying how much traffic we want each of our Revisions to receive.","title":"List your Revisions"},{"location":"getting-started/first-traffic-split/#split-traffic-between-revisions","text":"Lets split traffic between our two Revisions: kn YAML Run the command: kn service update hello \\ --traffic hello-00001 = 50 \\ --traffic @latest = 50 Add the traffic section to the bottom of your existing hello.yaml file: apiVersion: serving.knative.dev/v1 kind: Service metadata: name: hello spec: template: spec: containers: - image: gcr.io/knative-samples/helloworld-go ports: - containerPort: 8080 env: - name: TARGET value: \"Knative\" traffic: - latestRevision: true percent: 50 - latestRevision: false percent: 50 revisionName: hello-00001 Apply the YAML by running the command: kubectl apply -f hello.yaml Info @latest will always point to our \"latest\" Revision which, at the moment, is hello-00002 .","title":"Split traffic between Revisions"},{"location":"getting-started/first-traffic-split/#verify-the-traffic-split","text":"To verify that the traffic split has configured correctly, list the revisions again by running the command: kn revisions list Expected output NAME SERVICE TRAFFIC TAGS GENERATION AGE CONDITIONS READY REASON hello-00002 hello 50 % 2 10m 3 OK / 4 True hello-00001 hello 50 % 1 36m 3 OK / 4 True Access your Knative Service multiple times in your browser to see the different output being served by each Revision. Similarly, you can access the Service URL from the terminal multiple times to see the traffic being split between the Revisions. echo \"Accessing URL $( kn service describe hello -o url ) \" curl \" $( kn service describe hello -o url ) \" Expected output Hello Knative! Hello World! Hello Knative! Hello World!","title":"Verify the traffic split"},{"location":"getting-started/first-trigger/","text":"Using Triggers and sinks \u00b6 In the last topic we used the CloudEvents Player as an event source to send events to the Broker. We now want the event to go from the Broker to an event sink. In this topic, we will use the CloudEvents Player as the sink as well as a source. This means we will be using the CloudEvents Player to both send and receive events. We will use a Trigger to listen for events in the Broker to send to the sink. Creating your first Trigger \u00b6 Create a Trigger that listens for CloudEvents from the event source and places them into the sink, which is also the CloudEvents Player app. kn YAML To create the Trigger, run the command: kn trigger create cloudevents-trigger --sink cloudevents-player --broker example-broker Expected output Trigger 'cloudevents-trigger' successfully created in namespace 'default' . Copy the following YAML into a file named ce-trigger.yaml : apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: cloudevents-trigger annotations: knative-eventing-injection: enabled spec: broker: example-broker subscriber: ref: apiVersion: serving.knative.dev/v1 kind: Service name: cloudevents-player Create the Trigger by running the command: kubectl apply -f ce-trigger.yaml Expected output trigger.eventing.knative.dev/cloudevents-trigger created What CloudEvents is my Trigger listening for? Because we didn't specify a --filter in our kn command, the Trigger is listening for any CloudEvents coming into the Broker. The expand the next note to see how to use Filters. Now, when we go back to the CloudEvents Player and send an event, we see that CloudEvents are both sent and received by the CloudEvents Player: You may need to refresh the page to see your changes. What if I want to filter on CloudEvent attributes? First, delete your existing Trigger: kn trigger delete cloudevents-trigger Now let's add a Trigger that listens for a certain CloudEvent Type kn trigger create cloudevents-player-filter --sink cloudevents-player --broker example-broker --filter type = some-type If you send a CloudEvent with type some-type , it is reflected in the CloudEvents Player UI. The Trigger ignores any other types. You can filter on any aspect of the CloudEvent you would like to. Some people call this \"Event-Driven Architecture\" which can be used to create your own \"Functions as a Service\" on Kubernetes","title":"\u4f7f\u7528 \u89e6\u53d1\u5668 and sinks"},{"location":"getting-started/first-trigger/#using-triggers-and-sinks","text":"In the last topic we used the CloudEvents Player as an event source to send events to the Broker. We now want the event to go from the Broker to an event sink. In this topic, we will use the CloudEvents Player as the sink as well as a source. This means we will be using the CloudEvents Player to both send and receive events. We will use a Trigger to listen for events in the Broker to send to the sink.","title":"Using Triggers and sinks"},{"location":"getting-started/first-trigger/#creating-your-first-trigger","text":"Create a Trigger that listens for CloudEvents from the event source and places them into the sink, which is also the CloudEvents Player app. kn YAML To create the Trigger, run the command: kn trigger create cloudevents-trigger --sink cloudevents-player --broker example-broker Expected output Trigger 'cloudevents-trigger' successfully created in namespace 'default' . Copy the following YAML into a file named ce-trigger.yaml : apiVersion: eventing.knative.dev/v1 kind: Trigger metadata: name: cloudevents-trigger annotations: knative-eventing-injection: enabled spec: broker: example-broker subscriber: ref: apiVersion: serving.knative.dev/v1 kind: Service name: cloudevents-player Create the Trigger by running the command: kubectl apply -f ce-trigger.yaml Expected output trigger.eventing.knative.dev/cloudevents-trigger created What CloudEvents is my Trigger listening for? Because we didn't specify a --filter in our kn command, the Trigger is listening for any CloudEvents coming into the Broker. The expand the next note to see how to use Filters. Now, when we go back to the CloudEvents Player and send an event, we see that CloudEvents are both sent and received by the CloudEvents Player: You may need to refresh the page to see your changes. What if I want to filter on CloudEvent attributes? First, delete your existing Trigger: kn trigger delete cloudevents-trigger Now let's add a Trigger that listens for a certain CloudEvent Type kn trigger create cloudevents-player-filter --sink cloudevents-player --broker example-broker --filter type = some-type If you send a CloudEvent with type some-type , it is reflected in the CloudEvents Player UI. The Trigger ignores any other types. You can filter on any aspect of the CloudEvent you would like to. Some people call this \"Event-Driven Architecture\" which can be used to create your own \"Functions as a Service\" on Kubernetes","title":"Creating your first Trigger"},{"location":"getting-started/getting-started-eventing/","text":"Introducing the Knative Eventing \u00b6 With Knative Serving, we have a powerful tool which can take our containerized code and deploy it with relative ease. With Knative Eventing, you gain a few new super powers that allow you to build Event-Driven Applications . What are Event Driven Applications? Event-driven applications are designed to detect events as they occur, and then deal with them using some event-handling procedure. Producing and consuming events with an \"event-handling procedure\" is precisely what Knative Eventing enables. Want to find out more about Event-Driven Architecture and Knative Eventing? Check out this CNCF Session aptly named \"Event-driven architecture with Knative events\" Knative Eventing examples \u00b6 Knative Eventing acts as the \"glue\" between the disparate parts of your architecture and allows you to easily communicate between those parts in a fault-tolerant way. Some examples include: Creating and responding to Kubernetes API events image/svg+xml .st0{fill:#FF0000;} .st1{fill:#FFFFFF;} .st2{fill:#282828;} Creating an image processing pipeline image/svg+xml .st0{fill:#FF0000;} .st1{fill:#FFFFFF;} .st2{fill:#282828;} Facilitating AI workloads at the edge in large-scale, drone-powered sustainable agriculture projects As you can see by the mentioned examples, Knative Eventing implementations can range from simplistic to extremely complex. For now, you'll start with simplistic and learn about the most basic components of Knative Eventing: Sources , Brokers , Triggers , and Sinks .","title":"Knative Eventing\u4ecb\u7ecd"},{"location":"getting-started/getting-started-eventing/#introducing-the-knative-eventing","text":"With Knative Serving, we have a powerful tool which can take our containerized code and deploy it with relative ease. With Knative Eventing, you gain a few new super powers that allow you to build Event-Driven Applications . What are Event Driven Applications? Event-driven applications are designed to detect events as they occur, and then deal with them using some event-handling procedure. Producing and consuming events with an \"event-handling procedure\" is precisely what Knative Eventing enables. Want to find out more about Event-Driven Architecture and Knative Eventing? Check out this CNCF Session aptly named \"Event-driven architecture with Knative events\"","title":"Introducing the Knative Eventing"},{"location":"getting-started/getting-started-eventing/#knative-eventing-examples","text":"Knative Eventing acts as the \"glue\" between the disparate parts of your architecture and allows you to easily communicate between those parts in a fault-tolerant way. Some examples include: Creating and responding to Kubernetes API events image/svg+xml .st0{fill:#FF0000;} .st1{fill:#FFFFFF;} .st2{fill:#282828;} Creating an image processing pipeline image/svg+xml .st0{fill:#FF0000;} .st1{fill:#FFFFFF;} .st2{fill:#282828;} Facilitating AI workloads at the edge in large-scale, drone-powered sustainable agriculture projects As you can see by the mentioned examples, Knative Eventing implementations can range from simplistic to extremely complex. For now, you'll start with simplistic and learn about the most basic components of Knative Eventing: Sources , Brokers , Triggers , and Sinks .","title":"Knative Eventing examples"},{"location":"getting-started/next-steps/","text":"Next Steps \u00b6 This topic provides a list of resources to help you continue your Knative journey. Knative samples \u00b6 Try out some of the following Knative samples: Converting a Kubernetes Service to a Knative Service Knative Serving code samples Knative Eventing and Event source code samples Explore the Knative docs \u00b6 See the following guides for documentation specific to your use case: Serving Guide Eventing Guide Knative books \u00b6 Books to help you understand Knative concepts and get additional examples: Knative in Action Want a free digital copy of Knative in Action? VMWare has generously donated access to a free copy of the Knative in Action eBook, get your copy here! Knative Cookbook Other Knative links \u00b6 Other links to help you get started with Knative: Knative YouTube Channel Knative.tips Are we missing something? \u00b6 We'd love to help you along the next step of your Knative journey. If we're missing something on this page that you feel should be here, please give us feedback !","title":"\u63a5\u4e0b\u6765?"},{"location":"getting-started/next-steps/#next-steps","text":"This topic provides a list of resources to help you continue your Knative journey.","title":"Next Steps"},{"location":"getting-started/next-steps/#knative-samples","text":"Try out some of the following Knative samples: Converting a Kubernetes Service to a Knative Service Knative Serving code samples Knative Eventing and Event source code samples","title":"Knative samples"},{"location":"getting-started/next-steps/#explore-the-knative-docs","text":"See the following guides for documentation specific to your use case: Serving Guide Eventing Guide","title":"Explore the Knative docs"},{"location":"getting-started/next-steps/#knative-books","text":"Books to help you understand Knative concepts and get additional examples: Knative in Action Want a free digital copy of Knative in Action? VMWare has generously donated access to a free copy of the Knative in Action eBook, get your copy here! Knative Cookbook","title":"Knative books"},{"location":"getting-started/next-steps/#other-knative-links","text":"Other links to help you get started with Knative: Knative YouTube Channel Knative.tips","title":"Other Knative links"},{"location":"getting-started/next-steps/#are-we-missing-something","text":"We'd love to help you along the next step of your Knative journey. If we're missing something on this page that you feel should be here, please give us feedback !","title":"Are we missing something?"},{"location":"getting-started/quickstart-install/","text":"\u4f7f\u7528quickstart\u5b89\u88c5 Knative \u00b6 \u672c\u4e3b\u9898\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528 Knative quickstart \u63d2\u4ef6\u5b89\u88c5 Knative Serving \u548c Eventing \u7684\u672c\u5730\u90e8\u7f72\uff08local deployment\uff09\u3002 \u8be5\u63d2\u4ef6\u5728\u672c\u5730 Kubernetes \u96c6\u7fa4\u4e0a\u5b89\u88c5\u9884\u914d\u7f6e\u7684 Knative \u90e8\u7f72\uff08deployment\uff09\u3002 \u8b66\u544a Knative quickstart \u73af\u5883\u4ec5\u4f9b\u5b9e\u9a8c\u4f7f\u7528\u3002 \u5173\u4e8e\u751f\u6210\u73af\u5883\u7684\u5b89\u88c5\uff0c\u8bf7\u53c2\u9605 \u57fa\u4e8eYAML\u7684\u5b89\u88c5 \u6216 Knative Operator\u5b89\u88c5 \u5f00\u59cb\u4e4b\u524d \u00b6 \u5728\u5f00\u59cb\u4f7f\u7528 Knative quickstart \u90e8\u7f72\u4e4b\u524d\uff0c\u60a8\u5fc5\u987b\u5b89\u88c5\uff1a kind (Kubernetes in Docker) or minikube \u4ee5 \u4f7f\u7528 Docker \u5bb9\u5668\u8282\u70b9\u8fd0\u884c\u672c\u5730 Kubernetes \u96c6\u7fa4 Kubernetes CLI ( kubectl ) \u4ee5\u5728Kubernetes\u96c6\u7fa4\u8fd0\u884c\u547d\u4ee4\u3002 \u4f60\u53ef\u4ee5\u4f7f\u7528 kubectl \u6765\u90e8\u7f72\u5e94\u7528\uff0c\u68c0\u67e5\u548c\u7ba1\u7406\u96c6\u7fa4\u8d44\u6e90\u4ee5\u53ca\u67e5\u770b\u65e5\u5fd7\u3002 Knative CLI ( kn ). \u6709\u5173\u8bf4\u660e\uff0c\u8bf7\u53c2\u9605\u4e0b\u4e00\u8282. \u60a8\u9700\u8981\u81f3\u5c11 3 \u4e2a CPU \u548c 3 GB \u7684 RAM \u624d\u80fd\u521b\u5efa\u96c6\u7fa4 Install the Knative CLI \u00b6 Knative CLI ( kn ) \u4e3a\u521b\u5efa Knative \u8d44\u6e90\uff08\u4f8b\u5982 Knative \u670d\u52a1\u548c\u4e8b\u4ef6\u6e90\uff09\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5feb\u901f\u7b80\u5355\u7684\u5de5\u5177\uff0c\u800c\u65e0\u9700\u76f4\u63a5\u521b\u5efa\u6216\u4fee\u6539 YAML \u6587\u4ef6\u3002 kn CLI \u8fd8\u7b80\u5316\u4e86\u5176\u4ed6\u590d\u6742\u6b65\u9aa4\u7684\u5b8c\u6210\uff0c\u4f8b\u5982\u81ea\u52a8\u4f38\u7f29\u548c\u6d41\u91cf\u62c6\u5206\u3002 \u7528 Homebrew \u7528\u4e8c\u8fdb\u5236\u6587\u4ef6 \u7528 Go \u7528docker\u955c\u50cf \u63d0\u793a \u56fd\u5185\u5b89\u88c5Homebrew\u7684\u65b9\u6cd5\uff0c\u53ef\u53c2\u8003\u77e5\u4e4e \u8fd9\u7bc7\u6587\u7ae0 \u6267\u884c\u4ee5\u4e0b\u64cd\u4f5c\u4e4b\u4e00\uff1a \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u7528 Homebrew \u5b89\u88c5 kn : brew install kn \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u5c06 kn \u66f4\u65b0\u5230\u6700\u65b0\u7248\u672c: brew upgrade kn \u4f7f\u7528Homebrew\u5347\u7ea7 kn \u65f6\u9047\u5230\u95ee\u9898? \u5982\u679c\u60a8\u5728\u4f7f\u7528 Homebrew \u5347\u7ea7 kn \u65f6\u9047\u5230\u95ee\u9898\uff0c\u53ef\u80fd\u662f\u7531\u4e8eCLI\u4ed3\u5e93\u7684master\u5206\u652f\u88ab\u91cd\u547d\u540d\u4e3amain\u4e86. \u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u89e3\u51b3\u6b64\u95ee\u9898\uff1a brew tap --repair brew update brew upgrade kn \u60a8\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8f7d\u5bf9\u5e94\u4f60\u7684\u64cd\u4f5c\u7cfb\u7edf\u7684 kn \u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6\u5e76\u5c06\u5176\u653e\u7f6e\u5728\u7cfb\u7edf\u8def\u5f84\u4e2d\u6765\u5b89\u88c5 kn \u3002\u8bf7\u6ce8\u610f\uff0c\u60a8\u5c06\u9700\u8981 kn v0.25 \u6216\u66f4\u9ad8\u7248\u672c\u3002 \u4ece kn \u53d1\u5e03\u9875\u9762 . \u4e0b\u8f7d\u5bf9\u5e94\u4f60\u7684\u64cd\u4f5c\u7cfb\u7edf\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u3002 \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5c06\u4e8c\u8fdb\u5236\u6587\u4ef6\u91cd\u547d\u540d\u4e3a kn \u5e76\u4f7f\u5176\u53ef\u6267\u884c\uff1a mv <path-to-binary-file> kn chmod +x kn \u5176\u4e2d <path-to-binary-file> \u662f\u4f60\u5728\u4e0a\u4e00\u6b65\u4e0b\u8f7d\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u7684\u5b58\u653e\u8def\u5f84, \u6bd4\u5982, kn-darwin-amd64 \u6216 kn-linux-amd64 . \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5c06\u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6\u79fb\u52a8\u5230 PATH \u4e0a\u7684\u76ee\u5f55\uff1a mv kn /usr/local/bin \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u9a8c\u8bc1\u63d2\u4ef6\u662f\u5426\u6b63\u5e38\u5de5\u4f5c\uff1a kn version Check out kn \u5ba2\u6237\u7aef\u4ed3\u5e93: git clone https://github.com/knative/client.git cd client/ \u6784\u5efa\u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6: hack/build.sh -f \u5c06 kn \u52a0\u5165\u4f60\u7684\u7cfb\u7edf\u8def\u5f84, \u5e76\u9a8c\u8bc1 kn \u547d\u4ee4\u662f\u5426\u6b63\u5e38\u5de5\u4f5c\u3002\u4f8b\u5982\uff1a kn version \u955c\u50cf\u94fe\u63a5: Latest release \u4f60\u53ef\u4ee5\u5728\u5bb9\u5668\u955c\u50cf\u4e2d\u8fd0\u884c kn \u547d\u4ee4\uff0c\u4f8b\u5982: docker run --rm -v \" $HOME /.kube/config:/root/.kube/config\" gcr.io/knative-releases/knative.dev/client/cmd/kn:latest service list Note \u4ece\u5bb9\u5668\u6620\u50cf\u8fd0\u884c kn \u4e0d\u4f1a\u5c06\u4e8c\u8fdb\u5236\u6587\u4ef6\u653e\u5728\u6c38\u4e45\u8def\u5f84\u4e0a\u3002\u6bcf\u6b21\u8981\u4f7f\u7528 kn \u65f6\u90fd\u5fc5\u987b\u91cd\u590d\u6b64\u8fc7\u7a0b\u3002 \u5b89\u88c5Knative quickstart \u63d2\u4ef6 \u00b6 \u7528 Homebrew Using a binary Using Go \u6267\u884c\u4ee5\u4e0b\u64cd\u4f5c\u4e4b\u4e00\uff1a \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u7528 Homebrew \u5b89\u88c5 quickstart : brew install knative-sandbox/kn-plugins/quickstart \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u5c06 quickstart \u66f4\u65b0\u5230\u6700\u65b0\u7248\u672c: brew upgrade knative-sandbox/kn-plugins/quickstart Download the binary for your system from the quickstart release page . Rename the file to remove the OS and architecture information. For example, rename kn-quickstart-amd64 to kn-quickstart . Make the plugin executable. For example, chmod +x kn-quickstart . Move the executable binary file to a directory on your PATH , for example, in /usr/local/bin . Verify that the plugin is working by running the command: kn quickstart --help Check out the kn-plugin-quickstart repository: git clone https://github.com/knative-sandbox/kn-plugin-quickstart.git cd kn-plugin-quickstart/ Build an executable binary: hack/build.sh Move the executable binary file to a directory on your PATH : mv kn-quickstart /usr/local/bin Verify that the plugin is working by running the command: kn quickstart --help Run the Knative quickstart plugin \u00b6 The quickstart plugin completes the following functions: Checks if you have the selected Kubernetes instance installed Creates a cluster called knative Installs Knative Serving with Kourier as the default networking layer, and sslip.io as the DNS Installs Knative Eventing and creates an in-memory Broker and Channel implementation To get a local deployment of Knative, run the quickstart plugin: Using kind Using minikube Install Knative and Kubernetes on a local Docker daemon by running: kn quickstart kind After the plugin is finished, verify you have a cluster called knative : kind get clusters Install Knative and Kubernetes in a minikube instance by running: Note The minikube cluster will be created with 6 GB of RAM. If you don't have enough memory, you can change to a different value not lower than 3 GB by running the command minikube config set memory 3078 before this command. kn quickstart minikube The output of the previous command asked you to run minikube tunnel. Run the following command to start the process in a secondary terminal window, then return to the primary window and press enter to continue: minikube tunnel --profile knative The tunnel must continue to run in a terminal window any time you are using your Knative quickstart environment. The tunnel command is required because it allows your cluster to access Knative ingress service as a LoadBalancer from your host computer. Note To terminate the tunnel process and clean up network routes, enter Ctrl-C . For more information about the minikube tunnel command, see the minikube documentation . After the plugin is finished, verify you have a cluster called knative : minikube profile list Next steps \u00b6 Now you've installed Knative, learn how to deploy your first Service in the next topic in this tutorial.","title":"\u901a\u8fc7quickstart\u5b89\u88c5Knative"},{"location":"getting-started/quickstart-install/#quickstart-knative","text":"\u672c\u4e3b\u9898\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528 Knative quickstart \u63d2\u4ef6\u5b89\u88c5 Knative Serving \u548c Eventing \u7684\u672c\u5730\u90e8\u7f72\uff08local deployment\uff09\u3002 \u8be5\u63d2\u4ef6\u5728\u672c\u5730 Kubernetes \u96c6\u7fa4\u4e0a\u5b89\u88c5\u9884\u914d\u7f6e\u7684 Knative \u90e8\u7f72\uff08deployment\uff09\u3002 \u8b66\u544a Knative quickstart \u73af\u5883\u4ec5\u4f9b\u5b9e\u9a8c\u4f7f\u7528\u3002 \u5173\u4e8e\u751f\u6210\u73af\u5883\u7684\u5b89\u88c5\uff0c\u8bf7\u53c2\u9605 \u57fa\u4e8eYAML\u7684\u5b89\u88c5 \u6216 Knative Operator\u5b89\u88c5","title":"\u4f7f\u7528quickstart\u5b89\u88c5 Knative"},{"location":"getting-started/quickstart-install/#_1","text":"\u5728\u5f00\u59cb\u4f7f\u7528 Knative quickstart \u90e8\u7f72\u4e4b\u524d\uff0c\u60a8\u5fc5\u987b\u5b89\u88c5\uff1a kind (Kubernetes in Docker) or minikube \u4ee5 \u4f7f\u7528 Docker \u5bb9\u5668\u8282\u70b9\u8fd0\u884c\u672c\u5730 Kubernetes \u96c6\u7fa4 Kubernetes CLI ( kubectl ) \u4ee5\u5728Kubernetes\u96c6\u7fa4\u8fd0\u884c\u547d\u4ee4\u3002 \u4f60\u53ef\u4ee5\u4f7f\u7528 kubectl \u6765\u90e8\u7f72\u5e94\u7528\uff0c\u68c0\u67e5\u548c\u7ba1\u7406\u96c6\u7fa4\u8d44\u6e90\u4ee5\u53ca\u67e5\u770b\u65e5\u5fd7\u3002 Knative CLI ( kn ). \u6709\u5173\u8bf4\u660e\uff0c\u8bf7\u53c2\u9605\u4e0b\u4e00\u8282. \u60a8\u9700\u8981\u81f3\u5c11 3 \u4e2a CPU \u548c 3 GB \u7684 RAM \u624d\u80fd\u521b\u5efa\u96c6\u7fa4","title":"\u5f00\u59cb\u4e4b\u524d"},{"location":"getting-started/quickstart-install/#install-the-knative-cli","text":"Knative CLI ( kn ) \u4e3a\u521b\u5efa Knative \u8d44\u6e90\uff08\u4f8b\u5982 Knative \u670d\u52a1\u548c\u4e8b\u4ef6\u6e90\uff09\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5feb\u901f\u7b80\u5355\u7684\u5de5\u5177\uff0c\u800c\u65e0\u9700\u76f4\u63a5\u521b\u5efa\u6216\u4fee\u6539 YAML \u6587\u4ef6\u3002 kn CLI \u8fd8\u7b80\u5316\u4e86\u5176\u4ed6\u590d\u6742\u6b65\u9aa4\u7684\u5b8c\u6210\uff0c\u4f8b\u5982\u81ea\u52a8\u4f38\u7f29\u548c\u6d41\u91cf\u62c6\u5206\u3002 \u7528 Homebrew \u7528\u4e8c\u8fdb\u5236\u6587\u4ef6 \u7528 Go \u7528docker\u955c\u50cf \u63d0\u793a \u56fd\u5185\u5b89\u88c5Homebrew\u7684\u65b9\u6cd5\uff0c\u53ef\u53c2\u8003\u77e5\u4e4e \u8fd9\u7bc7\u6587\u7ae0 \u6267\u884c\u4ee5\u4e0b\u64cd\u4f5c\u4e4b\u4e00\uff1a \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u7528 Homebrew \u5b89\u88c5 kn : brew install kn \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u5c06 kn \u66f4\u65b0\u5230\u6700\u65b0\u7248\u672c: brew upgrade kn \u4f7f\u7528Homebrew\u5347\u7ea7 kn \u65f6\u9047\u5230\u95ee\u9898? \u5982\u679c\u60a8\u5728\u4f7f\u7528 Homebrew \u5347\u7ea7 kn \u65f6\u9047\u5230\u95ee\u9898\uff0c\u53ef\u80fd\u662f\u7531\u4e8eCLI\u4ed3\u5e93\u7684master\u5206\u652f\u88ab\u91cd\u547d\u540d\u4e3amain\u4e86. \u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u89e3\u51b3\u6b64\u95ee\u9898\uff1a brew tap --repair brew update brew upgrade kn \u60a8\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8f7d\u5bf9\u5e94\u4f60\u7684\u64cd\u4f5c\u7cfb\u7edf\u7684 kn \u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6\u5e76\u5c06\u5176\u653e\u7f6e\u5728\u7cfb\u7edf\u8def\u5f84\u4e2d\u6765\u5b89\u88c5 kn \u3002\u8bf7\u6ce8\u610f\uff0c\u60a8\u5c06\u9700\u8981 kn v0.25 \u6216\u66f4\u9ad8\u7248\u672c\u3002 \u4ece kn \u53d1\u5e03\u9875\u9762 . \u4e0b\u8f7d\u5bf9\u5e94\u4f60\u7684\u64cd\u4f5c\u7cfb\u7edf\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u3002 \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5c06\u4e8c\u8fdb\u5236\u6587\u4ef6\u91cd\u547d\u540d\u4e3a kn \u5e76\u4f7f\u5176\u53ef\u6267\u884c\uff1a mv <path-to-binary-file> kn chmod +x kn \u5176\u4e2d <path-to-binary-file> \u662f\u4f60\u5728\u4e0a\u4e00\u6b65\u4e0b\u8f7d\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u7684\u5b58\u653e\u8def\u5f84, \u6bd4\u5982, kn-darwin-amd64 \u6216 kn-linux-amd64 . \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5c06\u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6\u79fb\u52a8\u5230 PATH \u4e0a\u7684\u76ee\u5f55\uff1a mv kn /usr/local/bin \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u9a8c\u8bc1\u63d2\u4ef6\u662f\u5426\u6b63\u5e38\u5de5\u4f5c\uff1a kn version Check out kn \u5ba2\u6237\u7aef\u4ed3\u5e93: git clone https://github.com/knative/client.git cd client/ \u6784\u5efa\u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6: hack/build.sh -f \u5c06 kn \u52a0\u5165\u4f60\u7684\u7cfb\u7edf\u8def\u5f84, \u5e76\u9a8c\u8bc1 kn \u547d\u4ee4\u662f\u5426\u6b63\u5e38\u5de5\u4f5c\u3002\u4f8b\u5982\uff1a kn version \u955c\u50cf\u94fe\u63a5: Latest release \u4f60\u53ef\u4ee5\u5728\u5bb9\u5668\u955c\u50cf\u4e2d\u8fd0\u884c kn \u547d\u4ee4\uff0c\u4f8b\u5982: docker run --rm -v \" $HOME /.kube/config:/root/.kube/config\" gcr.io/knative-releases/knative.dev/client/cmd/kn:latest service list Note \u4ece\u5bb9\u5668\u6620\u50cf\u8fd0\u884c kn \u4e0d\u4f1a\u5c06\u4e8c\u8fdb\u5236\u6587\u4ef6\u653e\u5728\u6c38\u4e45\u8def\u5f84\u4e0a\u3002\u6bcf\u6b21\u8981\u4f7f\u7528 kn \u65f6\u90fd\u5fc5\u987b\u91cd\u590d\u6b64\u8fc7\u7a0b\u3002","title":"Install the Knative CLI"},{"location":"getting-started/quickstart-install/#knative-quickstart","text":"\u7528 Homebrew Using a binary Using Go \u6267\u884c\u4ee5\u4e0b\u64cd\u4f5c\u4e4b\u4e00\uff1a \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u7528 Homebrew \u5b89\u88c5 quickstart : brew install knative-sandbox/kn-plugins/quickstart \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u5c06 quickstart \u66f4\u65b0\u5230\u6700\u65b0\u7248\u672c: brew upgrade knative-sandbox/kn-plugins/quickstart Download the binary for your system from the quickstart release page . Rename the file to remove the OS and architecture information. For example, rename kn-quickstart-amd64 to kn-quickstart . Make the plugin executable. For example, chmod +x kn-quickstart . Move the executable binary file to a directory on your PATH , for example, in /usr/local/bin . Verify that the plugin is working by running the command: kn quickstart --help Check out the kn-plugin-quickstart repository: git clone https://github.com/knative-sandbox/kn-plugin-quickstart.git cd kn-plugin-quickstart/ Build an executable binary: hack/build.sh Move the executable binary file to a directory on your PATH : mv kn-quickstart /usr/local/bin Verify that the plugin is working by running the command: kn quickstart --help","title":"\u5b89\u88c5Knative quickstart \u63d2\u4ef6"},{"location":"getting-started/quickstart-install/#run-the-knative-quickstart-plugin","text":"The quickstart plugin completes the following functions: Checks if you have the selected Kubernetes instance installed Creates a cluster called knative Installs Knative Serving with Kourier as the default networking layer, and sslip.io as the DNS Installs Knative Eventing and creates an in-memory Broker and Channel implementation To get a local deployment of Knative, run the quickstart plugin: Using kind Using minikube Install Knative and Kubernetes on a local Docker daemon by running: kn quickstart kind After the plugin is finished, verify you have a cluster called knative : kind get clusters Install Knative and Kubernetes in a minikube instance by running: Note The minikube cluster will be created with 6 GB of RAM. If you don't have enough memory, you can change to a different value not lower than 3 GB by running the command minikube config set memory 3078 before this command. kn quickstart minikube The output of the previous command asked you to run minikube tunnel. Run the following command to start the process in a secondary terminal window, then return to the primary window and press enter to continue: minikube tunnel --profile knative The tunnel must continue to run in a terminal window any time you are using your Knative quickstart environment. The tunnel command is required because it allows your cluster to access Knative ingress service as a LoadBalancer from your host computer. Note To terminate the tunnel process and clean up network routes, enter Ctrl-C . For more information about the minikube tunnel command, see the minikube documentation . After the plugin is finished, verify you have a cluster called knative : minikube profile list","title":"Run the Knative quickstart plugin"},{"location":"getting-started/quickstart-install/#next-steps","text":"Now you've installed Knative, learn how to deploy your first Service in the next topic in this tutorial.","title":"Next steps"},{"location":"install/","text":"\u5b89\u88c5Knative \u00b6 \u4f60\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u65b9\u6cd5\u5728\u96c6\u7fa4\u4e2d\u5b89\u88c5Serving\u7ec4\u4ef6\u548cEventing\u7ec4\u4ef6\uff1a \u4f7f\u7528 Knative Quickstart\u63d2\u4ef6 \u5b89\u88c5\u4e00\u4e2a\u9884\u914d\u7f6e\u7684Knative\u672c\u5730\u7248\u4f5c\u4e3a\u8bd5\u9a8c\u3002 \u4f7f\u7528\u57fa\u4e8e YAML \u7684\u65b9\u6cd5\uff0c\u8fd9\u53ef\u4ee5\u7528\u4e8e\u751f\u4ea7\u73af\u5883: \u901a\u8fc7YAML\u5b89\u88c5Knative Serving \u901a\u8fc7YAML\u5b89\u88c5Knative Eventing \u901a\u8fc7 Knative Operator \u6765\u5b89\u88c5\uff0c\u8fd9\u540c\u6837\u9002\u7528\u4e8e\u751f\u4ea7\u73af\u5883\u3002 \u67e5\u770b\u5176\u5b83\u5382\u5546\u63d0\u4f9b\u7684 Knative\u73af\u5883 . \u4f60\u4e5f\u53ef\u4ee5 \u5347\u7ea7\u5df2\u5b89\u88c5\u7684Knative . \u6ce8\u610f Knative \u5b89\u88c5\u8bf4\u660e\u5047\u8bbe\u60a8\u7684\u7cfb\u7edf\u4e3aMac \u6216 Linux\uff0c\u5e76\u4e14\u4f7f\u7528bash shell","title":"\u5173\u4e8e\u5b89\u88c5Knative"},{"location":"install/#knative","text":"\u4f60\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u65b9\u6cd5\u5728\u96c6\u7fa4\u4e2d\u5b89\u88c5Serving\u7ec4\u4ef6\u548cEventing\u7ec4\u4ef6\uff1a \u4f7f\u7528 Knative Quickstart\u63d2\u4ef6 \u5b89\u88c5\u4e00\u4e2a\u9884\u914d\u7f6e\u7684Knative\u672c\u5730\u7248\u4f5c\u4e3a\u8bd5\u9a8c\u3002 \u4f7f\u7528\u57fa\u4e8e YAML \u7684\u65b9\u6cd5\uff0c\u8fd9\u53ef\u4ee5\u7528\u4e8e\u751f\u4ea7\u73af\u5883: \u901a\u8fc7YAML\u5b89\u88c5Knative Serving \u901a\u8fc7YAML\u5b89\u88c5Knative Eventing \u901a\u8fc7 Knative Operator \u6765\u5b89\u88c5\uff0c\u8fd9\u540c\u6837\u9002\u7528\u4e8e\u751f\u4ea7\u73af\u5883\u3002 \u67e5\u770b\u5176\u5b83\u5382\u5546\u63d0\u4f9b\u7684 Knative\u73af\u5883 . \u4f60\u4e5f\u53ef\u4ee5 \u5347\u7ea7\u5df2\u5b89\u88c5\u7684Knative . \u6ce8\u610f Knative \u5b89\u88c5\u8bf4\u660e\u5047\u8bbe\u60a8\u7684\u7cfb\u7edf\u4e3aMac \u6216 Linux\uff0c\u5e76\u4e14\u4f7f\u7528bash shell","title":"\u5b89\u88c5Knative"},{"location":"install/installing-cert-manager/","text":"Installing cert-manager for TLS certificates \u00b6 Install the Cert-Manager tool to obtain TLS certificates that you can use for secure HTTPS connections in Knative. For more information about enabling HTTPS connections in Knative, see Configuring HTTPS with TLS certificates . You can use cert-manager to either manually obtain certificates, or to enable Knative for automatic certificate provisioning. Complete instructions about automatic certificate provisioning are provided in Enabling automatic TLS cert provisioning . Regardless of if your want to manually obtain certificates, or configure Knative for automatic provisioning, you can use the following steps to install cert-manager. Before you begin \u00b6 You must meet the following requirements to install cert-manager for Knative: Knative Serving must be installed. For details about installing the Serving component, see the Knative installation guide . You must configure your Knative cluster to use a custom domain . Knative currently supports cert-manager version 1.0.0 and higher. Downloading and installing cert-manager \u00b6 To download and install cert-manager, follow the Installation steps from the official cert-manager website. Completing the Knative configuration for TLS support \u00b6 Before you can use a TLS certificate for secure connections, you must finish configuring Knative: Manual : If you installed cert-manager to manually obtain certificates, continue to the following topic for instructions about creating a Kubernetes secret: Manually adding a TLS certificate Automatic : If you installed cert-manager to use for automatic certificate provisioning, continue to the following topic to enable that feature: Enabling automatic TLS certificate provisioning in Knative","title":"\u5b89\u88c5 cert-manager"},{"location":"install/installing-cert-manager/#installing-cert-manager-for-tls-certificates","text":"Install the Cert-Manager tool to obtain TLS certificates that you can use for secure HTTPS connections in Knative. For more information about enabling HTTPS connections in Knative, see Configuring HTTPS with TLS certificates . You can use cert-manager to either manually obtain certificates, or to enable Knative for automatic certificate provisioning. Complete instructions about automatic certificate provisioning are provided in Enabling automatic TLS cert provisioning . Regardless of if your want to manually obtain certificates, or configure Knative for automatic provisioning, you can use the following steps to install cert-manager.","title":"Installing cert-manager for TLS certificates"},{"location":"install/installing-cert-manager/#before-you-begin","text":"You must meet the following requirements to install cert-manager for Knative: Knative Serving must be installed. For details about installing the Serving component, see the Knative installation guide . You must configure your Knative cluster to use a custom domain . Knative currently supports cert-manager version 1.0.0 and higher.","title":"Before you begin"},{"location":"install/installing-cert-manager/#downloading-and-installing-cert-manager","text":"To download and install cert-manager, follow the Installation steps from the official cert-manager website.","title":"Downloading and installing cert-manager"},{"location":"install/installing-cert-manager/#completing-the-knative-configuration-for-tls-support","text":"Before you can use a TLS certificate for secure connections, you must finish configuring Knative: Manual : If you installed cert-manager to manually obtain certificates, continue to the following topic for instructions about creating a Kubernetes secret: Manually adding a TLS certificate Automatic : If you installed cert-manager to use for automatic certificate provisioning, continue to the following topic to enable that feature: Enabling automatic TLS certificate provisioning in Knative","title":"Completing the Knative configuration for TLS support"},{"location":"install/installing-istio/","text":"Installing Istio for Knative \u00b6 This guide walks you through manually installing and customizing Istio for use with Knative. If your cloud platform offers a managed Istio installation, we recommend installing Istio that way, unless you need to customize your installation. Before you begin \u00b6 You need: A Kubernetes cluster created. istioctl installed. Supported Istio versions \u00b6 The current known-to-be-stable version of Istio tested in conjunction with Knative is v1.12 . Versions in the 1.12 line are generally fine too. Installing Istio \u00b6 When you install Istio, there are a few options depending on your goals. For a basic Istio installation suitable for most Knative use cases, follow the Installing Istio without sidecar injection instructions. If you're familiar with Istio and know what kind of installation you want, read through the options and choose the installation that suits your needs. You can easily customize your Istio installation with istioctl . The following sections cover a few useful Istio configurations and their benefits. Choosing an Istio installation \u00b6 You can install Istio with or without a service mesh: Installing Istio without sidecar injection (Recommended default installation) Installing Istio with sidecar injection If you want to get up and running with Knative quickly, we recommend installing Istio without automatic sidecar injection. This install is also recommended for users who don't need the Istio service mesh, or who want to enable the service mesh by manually injecting the Istio sidecars . Installing Istio without sidecar injection \u00b6 Enter the following command to install Istio: To install Istio without sidecar injection: istioctl install -y Installing Istio with sidecar injection \u00b6 If you want to enable the Istio service mesh, you must enable automatic sidecar injection . The Istio service mesh provides a few benefits: Allows you to turn on mutual TLS , which secures service-to-service traffic within the cluster. Allows you to use the Istio authorization policy , controlling the access to each Knative service based on Istio service roles. For automatic sidecar injection, set autoInject: enabled in addition to the earlier operator configuration. global: proxy: autoInject: enabled Using Istio mTLS feature \u00b6 Since there are some networking communications between knative-serving namespace and the namespace where your services running on, you need additional preparations for mTLS enabled environment. Enable sidecar container on knative-serving system namespace. kubectl label namespace knative-serving istio-injection = enabled Set PeerAuthentication to PERMISSIVE on knative-serving system namespace by creating a YAML file using the following template: apiVersion: \"security.istio.io/v1beta1\" kind: \"PeerAuthentication\" metadata: name: \"default\" namespace: \"knative-serving\" spec: mtls: mode: PERMISSIVE Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. After you install the cluster local gateway, your service and deployment for the local gateway is named knative-local-gateway . Updating the config-istio configmap to use a non-default local gateway \u00b6 If you create a custom service and deployment for local gateway with a name other than knative-local-gateway , you need to update gateway configmap config-istio under the knative-serving namespace. Edit the config-istio configmap: kubectl edit configmap config-istio -n knative-serving Replace the local-gateway.knative-serving.knative-local-gateway field with the custom service. As an example, if you name both the service and deployment custom-local-gateway under the namespace istio-system , it should be updated to: custom-local-gateway.istio-system.svc.cluster.local As an example, if both the custom service and deployment are labeled with custom: custom-local-gateway , not the default istio: knative-local-gateway , you must update gateway instance knative-local-gateway in the knative-serving namespace: kubectl edit gateway knative-local-gateway -n knative-serving Replace the label selector with the label of your service: istio: knative-local-gateway For the service mentioned earlier, it should be updated to: custom: custom-local-gateway If there is a change in service ports (compared to that of knative-local-gateway ), update the port info in the gateway accordingly. Verifying your Istio install \u00b6 View the status of your Istio installation to make sure the install was successful. It might take a few seconds, so rerun the following command until all of the pods show a STATUS of Running or Completed : kubectl get pods --namespace istio-system Tip: You can append the --watch flag to the kubectl get commands to view the pod status in realtime. You use CTRL + C to exit watch mode. Configuring DNS \u00b6 Knative dispatches to different services based on their hostname, so it is recommended to have DNS properly configured. To do this, begin by looking up the external IP address that Istio received: $ kubectl get svc -nistio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-ingressgateway LoadBalancer 10.0.2.24 34.83.80.117 15020:32206/TCP,80:30742/TCP,443:30996/TCP 2m14s istio-pilot ClusterIP 10.0.3.27 <none> 15010/TCP,15011/TCP,8080/TCP,15014/TCP 2m14s This external IP can be used with your DNS provider with a wildcard A record. However, for a basic non-production set up, this external IP address can be used with sslip.io in the config-domain ConfigMap in knative-serving . You can edit this by using the following command: kubectl edit cm config-domain --namespace knative-serving Given this external IP, change the content to: apiVersion: v1 kind: ConfigMap metadata: name: config-domain namespace: knative-serving data: # sslip.io is a \"magic\" DNS provider, which resolves all DNS lookups for: # *.{ip}.sslip.io to {ip}. 34.83.80.117.sslip.io: \"\" Istio resources \u00b6 For the official Istio installation guide, see the Istio Kubernetes Getting Started Guide . For the full list of available configs when installing Istio with istioctl , see the Istio Installation Options reference . Clean up Istio \u00b6 See the Uninstall Istio . What's next \u00b6 View the Knative Serving documentation . Try some Knative Serving code samples .","title":"\u4e3a Knative \u5b89\u88c5Istio"},{"location":"install/installing-istio/#installing-istio-for-knative","text":"This guide walks you through manually installing and customizing Istio for use with Knative. If your cloud platform offers a managed Istio installation, we recommend installing Istio that way, unless you need to customize your installation.","title":"Installing Istio for Knative"},{"location":"install/installing-istio/#before-you-begin","text":"You need: A Kubernetes cluster created. istioctl installed.","title":"Before you begin"},{"location":"install/installing-istio/#supported-istio-versions","text":"The current known-to-be-stable version of Istio tested in conjunction with Knative is v1.12 . Versions in the 1.12 line are generally fine too.","title":"Supported Istio versions"},{"location":"install/installing-istio/#installing-istio","text":"When you install Istio, there are a few options depending on your goals. For a basic Istio installation suitable for most Knative use cases, follow the Installing Istio without sidecar injection instructions. If you're familiar with Istio and know what kind of installation you want, read through the options and choose the installation that suits your needs. You can easily customize your Istio installation with istioctl . The following sections cover a few useful Istio configurations and their benefits.","title":"Installing Istio"},{"location":"install/installing-istio/#choosing-an-istio-installation","text":"You can install Istio with or without a service mesh: Installing Istio without sidecar injection (Recommended default installation) Installing Istio with sidecar injection If you want to get up and running with Knative quickly, we recommend installing Istio without automatic sidecar injection. This install is also recommended for users who don't need the Istio service mesh, or who want to enable the service mesh by manually injecting the Istio sidecars .","title":"Choosing an Istio installation"},{"location":"install/installing-istio/#installing-istio-without-sidecar-injection","text":"Enter the following command to install Istio: To install Istio without sidecar injection: istioctl install -y","title":"Installing Istio without sidecar injection"},{"location":"install/installing-istio/#installing-istio-with-sidecar-injection","text":"If you want to enable the Istio service mesh, you must enable automatic sidecar injection . The Istio service mesh provides a few benefits: Allows you to turn on mutual TLS , which secures service-to-service traffic within the cluster. Allows you to use the Istio authorization policy , controlling the access to each Knative service based on Istio service roles. For automatic sidecar injection, set autoInject: enabled in addition to the earlier operator configuration. global: proxy: autoInject: enabled","title":"Installing Istio with sidecar injection"},{"location":"install/installing-istio/#using-istio-mtls-feature","text":"Since there are some networking communications between knative-serving namespace and the namespace where your services running on, you need additional preparations for mTLS enabled environment. Enable sidecar container on knative-serving system namespace. kubectl label namespace knative-serving istio-injection = enabled Set PeerAuthentication to PERMISSIVE on knative-serving system namespace by creating a YAML file using the following template: apiVersion: \"security.istio.io/v1beta1\" kind: \"PeerAuthentication\" metadata: name: \"default\" namespace: \"knative-serving\" spec: mtls: mode: PERMISSIVE Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. After you install the cluster local gateway, your service and deployment for the local gateway is named knative-local-gateway .","title":"Using Istio mTLS feature"},{"location":"install/installing-istio/#updating-the-config-istio-configmap-to-use-a-non-default-local-gateway","text":"If you create a custom service and deployment for local gateway with a name other than knative-local-gateway , you need to update gateway configmap config-istio under the knative-serving namespace. Edit the config-istio configmap: kubectl edit configmap config-istio -n knative-serving Replace the local-gateway.knative-serving.knative-local-gateway field with the custom service. As an example, if you name both the service and deployment custom-local-gateway under the namespace istio-system , it should be updated to: custom-local-gateway.istio-system.svc.cluster.local As an example, if both the custom service and deployment are labeled with custom: custom-local-gateway , not the default istio: knative-local-gateway , you must update gateway instance knative-local-gateway in the knative-serving namespace: kubectl edit gateway knative-local-gateway -n knative-serving Replace the label selector with the label of your service: istio: knative-local-gateway For the service mentioned earlier, it should be updated to: custom: custom-local-gateway If there is a change in service ports (compared to that of knative-local-gateway ), update the port info in the gateway accordingly.","title":"Updating the config-istio configmap to use a non-default local gateway"},{"location":"install/installing-istio/#verifying-your-istio-install","text":"View the status of your Istio installation to make sure the install was successful. It might take a few seconds, so rerun the following command until all of the pods show a STATUS of Running or Completed : kubectl get pods --namespace istio-system Tip: You can append the --watch flag to the kubectl get commands to view the pod status in realtime. You use CTRL + C to exit watch mode.","title":"Verifying your Istio install"},{"location":"install/installing-istio/#configuring-dns","text":"Knative dispatches to different services based on their hostname, so it is recommended to have DNS properly configured. To do this, begin by looking up the external IP address that Istio received: $ kubectl get svc -nistio-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE istio-ingressgateway LoadBalancer 10.0.2.24 34.83.80.117 15020:32206/TCP,80:30742/TCP,443:30996/TCP 2m14s istio-pilot ClusterIP 10.0.3.27 <none> 15010/TCP,15011/TCP,8080/TCP,15014/TCP 2m14s This external IP can be used with your DNS provider with a wildcard A record. However, for a basic non-production set up, this external IP address can be used with sslip.io in the config-domain ConfigMap in knative-serving . You can edit this by using the following command: kubectl edit cm config-domain --namespace knative-serving Given this external IP, change the content to: apiVersion: v1 kind: ConfigMap metadata: name: config-domain namespace: knative-serving data: # sslip.io is a \"magic\" DNS provider, which resolves all DNS lookups for: # *.{ip}.sslip.io to {ip}. 34.83.80.117.sslip.io: \"\"","title":"Configuring DNS"},{"location":"install/installing-istio/#istio-resources","text":"For the official Istio installation guide, see the Istio Kubernetes Getting Started Guide . For the full list of available configs when installing Istio with istioctl , see the Istio Installation Options reference .","title":"Istio resources"},{"location":"install/installing-istio/#clean-up-istio","text":"See the Uninstall Istio .","title":"Clean up Istio"},{"location":"install/installing-istio/#whats-next","text":"View the Knative Serving documentation . Try some Knative Serving code samples .","title":"What's next"},{"location":"install/knative-offerings/","text":"Knative Offerings \u00b6 Knative has a rich community with many vendors participating, and many of those vendors offer commercial Knative products. Please check with each of these vendors for what is or is not supported. Here is a list of commercial Knative products (alphabetically): Cloud Native Runtimes for VMware Tanzu : A serverless application runtime for Kubernetes that is based on Knative, and runs on a single Kubernetes cluster Gardener : Install Knative in Gardener's vanilla Kubernetes clusters to add an extra layer of serverless runtime. Google Cloud Run for Anthos : Extend Google Kubernetes Engine with a flexible serverless development platform. With Cloud Run for Anthos, you get the operational flexibility of Kubernetes with the developer experience of serverless, allowing you to deploy and manage Knative-based services on your own cluster, and trigger them with events from Google, 3rd-party sources, and your own applications. Google Cloud Run : A fully-managed Knative-based serverless platform. With no Kubernetes cluster to manage, Cloud Run lets you go from container to production in seconds. IBM Cloud Code Engine : A fully-managed serverless platform that runs all your containerized workloads, including http-driven application, batch jobs or event-driven functions. Nutanix Karbon : Extend Nutanix Karbon with serverless capabilities by installing Knative on a Karbon managed Kubernetes cluster. Red Hat Openshift Serverless : enables stateful, stateless, and serverless workloads to all run on a single multi-cloud container platform with automated operations. Developers can use a single platform for hosting their microservices, legacy, and serverless applications. TriggerMesh : On-premises and fully-managed Knative and event-driven integration platform. With support for AWS, Azure, Google, and many more cloud and enterprise event sources and brokers. Commercial Knative support and professional services available.","title":"Using a Knative-based offering"},{"location":"install/knative-offerings/#knative-offerings","text":"Knative has a rich community with many vendors participating, and many of those vendors offer commercial Knative products. Please check with each of these vendors for what is or is not supported. Here is a list of commercial Knative products (alphabetically): Cloud Native Runtimes for VMware Tanzu : A serverless application runtime for Kubernetes that is based on Knative, and runs on a single Kubernetes cluster Gardener : Install Knative in Gardener's vanilla Kubernetes clusters to add an extra layer of serverless runtime. Google Cloud Run for Anthos : Extend Google Kubernetes Engine with a flexible serverless development platform. With Cloud Run for Anthos, you get the operational flexibility of Kubernetes with the developer experience of serverless, allowing you to deploy and manage Knative-based services on your own cluster, and trigger them with events from Google, 3rd-party sources, and your own applications. Google Cloud Run : A fully-managed Knative-based serverless platform. With no Kubernetes cluster to manage, Cloud Run lets you go from container to production in seconds. IBM Cloud Code Engine : A fully-managed serverless platform that runs all your containerized workloads, including http-driven application, batch jobs or event-driven functions. Nutanix Karbon : Extend Nutanix Karbon with serverless capabilities by installing Knative on a Karbon managed Kubernetes cluster. Red Hat Openshift Serverless : enables stateful, stateless, and serverless workloads to all run on a single multi-cloud container platform with automated operations. Developers can use a single platform for hosting their microservices, legacy, and serverless applications. TriggerMesh : On-premises and fully-managed Knative and event-driven integration platform. With support for AWS, Azure, Google, and many more cloud and enterprise event sources and brokers. Commercial Knative support and professional services available.","title":"Knative Offerings"},{"location":"install/quickstart-install/","text":"\u4f7f\u7528quickstart\u5b89\u88c5 Knative \u00b6 \u672c\u4e3b\u9898\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528 Knative quickstart \u63d2\u4ef6\u5b89\u88c5 Knative Serving \u548c Eventing \u7684\u672c\u5730\u90e8\u7f72\uff08local deployment\uff09\u3002 \u8be5\u63d2\u4ef6\u5728\u672c\u5730 Kubernetes \u96c6\u7fa4\u4e0a\u5b89\u88c5\u9884\u914d\u7f6e\u7684 Knative \u90e8\u7f72\uff08deployment\uff09\u3002 \u8b66\u544a Knative quickstart \u73af\u5883\u4ec5\u4f9b\u5b9e\u9a8c\u4f7f\u7528\u3002 \u5173\u4e8e\u751f\u6210\u73af\u5883\u7684\u5b89\u88c5\uff0c\u8bf7\u53c2\u9605 \u57fa\u4e8eYAML\u7684\u5b89\u88c5 \u6216 Knative Operator\u5b89\u88c5 \u5f00\u59cb\u4e4b\u524d \u00b6 \u5728\u5f00\u59cb\u4f7f\u7528 Knative quickstart \u90e8\u7f72\u4e4b\u524d\uff0c\u60a8\u5fc5\u987b\u5b89\u88c5\uff1a kind (Kubernetes in Docker) or minikube \u4ee5 \u4f7f\u7528 Docker \u5bb9\u5668\u8282\u70b9\u8fd0\u884c\u672c\u5730 Kubernetes \u96c6\u7fa4 Kubernetes CLI ( kubectl ) \u4ee5\u5728Kubernetes\u96c6\u7fa4\u8fd0\u884c\u547d\u4ee4\u3002 \u4f60\u53ef\u4ee5\u4f7f\u7528 kubectl \u6765\u90e8\u7f72\u5e94\u7528\uff0c\u68c0\u67e5\u548c\u7ba1\u7406\u96c6\u7fa4\u8d44\u6e90\u4ee5\u53ca\u67e5\u770b\u65e5\u5fd7\u3002 Knative CLI ( kn ). \u6709\u5173\u8bf4\u660e\uff0c\u8bf7\u53c2\u9605\u4e0b\u4e00\u8282. \u60a8\u9700\u8981\u81f3\u5c11 3 \u4e2a CPU \u548c 3 GB \u7684 RAM \u624d\u80fd\u521b\u5efa\u96c6\u7fa4 Install the Knative CLI \u00b6 Knative CLI ( kn ) \u4e3a\u521b\u5efa Knative \u8d44\u6e90\uff08\u4f8b\u5982 Knative \u670d\u52a1\u548c\u4e8b\u4ef6\u6e90\uff09\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5feb\u901f\u7b80\u5355\u7684\u5de5\u5177\uff0c\u800c\u65e0\u9700\u76f4\u63a5\u521b\u5efa\u6216\u4fee\u6539 YAML \u6587\u4ef6\u3002 kn CLI \u8fd8\u7b80\u5316\u4e86\u5176\u4ed6\u590d\u6742\u6b65\u9aa4\u7684\u5b8c\u6210\uff0c\u4f8b\u5982\u81ea\u52a8\u4f38\u7f29\u548c\u6d41\u91cf\u62c6\u5206\u3002 \u7528 Homebrew \u7528\u4e8c\u8fdb\u5236\u6587\u4ef6 \u7528 Go \u7528docker\u955c\u50cf \u63d0\u793a \u56fd\u5185\u5b89\u88c5Homebrew\u7684\u65b9\u6cd5\uff0c\u53ef\u53c2\u8003\u77e5\u4e4e \u8fd9\u7bc7\u6587\u7ae0 \u6267\u884c\u4ee5\u4e0b\u64cd\u4f5c\u4e4b\u4e00\uff1a \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u7528 Homebrew \u5b89\u88c5 kn : brew install kn \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u5c06 kn \u66f4\u65b0\u5230\u6700\u65b0\u7248\u672c: brew upgrade kn \u4f7f\u7528Homebrew\u5347\u7ea7 kn \u65f6\u9047\u5230\u95ee\u9898? \u5982\u679c\u60a8\u5728\u4f7f\u7528 Homebrew \u5347\u7ea7 kn \u65f6\u9047\u5230\u95ee\u9898\uff0c\u53ef\u80fd\u662f\u7531\u4e8eCLI\u4ed3\u5e93\u7684master\u5206\u652f\u88ab\u91cd\u547d\u540d\u4e3amain\u4e86. \u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u89e3\u51b3\u6b64\u95ee\u9898\uff1a brew tap --repair brew update brew upgrade kn \u60a8\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8f7d\u5bf9\u5e94\u4f60\u7684\u64cd\u4f5c\u7cfb\u7edf\u7684 kn \u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6\u5e76\u5c06\u5176\u653e\u7f6e\u5728\u7cfb\u7edf\u8def\u5f84\u4e2d\u6765\u5b89\u88c5 kn \u3002\u8bf7\u6ce8\u610f\uff0c\u60a8\u5c06\u9700\u8981 kn v0.25 \u6216\u66f4\u9ad8\u7248\u672c\u3002 \u4ece kn \u53d1\u5e03\u9875\u9762 . \u4e0b\u8f7d\u5bf9\u5e94\u4f60\u7684\u64cd\u4f5c\u7cfb\u7edf\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u3002 \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5c06\u4e8c\u8fdb\u5236\u6587\u4ef6\u91cd\u547d\u540d\u4e3a kn \u5e76\u4f7f\u5176\u53ef\u6267\u884c\uff1a mv <path-to-binary-file> kn chmod +x kn \u5176\u4e2d <path-to-binary-file> \u662f\u4f60\u5728\u4e0a\u4e00\u6b65\u4e0b\u8f7d\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u7684\u5b58\u653e\u8def\u5f84, \u6bd4\u5982, kn-darwin-amd64 \u6216 kn-linux-amd64 . \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5c06\u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6\u79fb\u52a8\u5230 PATH \u4e0a\u7684\u76ee\u5f55\uff1a mv kn /usr/local/bin \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u9a8c\u8bc1\u63d2\u4ef6\u662f\u5426\u6b63\u5e38\u5de5\u4f5c\uff1a kn version Check out kn \u5ba2\u6237\u7aef\u4ed3\u5e93: git clone https://github.com/knative/client.git cd client/ \u6784\u5efa\u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6: hack/build.sh -f \u5c06 kn \u52a0\u5165\u4f60\u7684\u7cfb\u7edf\u8def\u5f84, \u5e76\u9a8c\u8bc1 kn \u547d\u4ee4\u662f\u5426\u6b63\u5e38\u5de5\u4f5c\u3002\u4f8b\u5982\uff1a kn version \u955c\u50cf\u94fe\u63a5: Latest release \u4f60\u53ef\u4ee5\u5728\u5bb9\u5668\u955c\u50cf\u4e2d\u8fd0\u884c kn \u547d\u4ee4\uff0c\u4f8b\u5982: docker run --rm -v \" $HOME /.kube/config:/root/.kube/config\" gcr.io/knative-releases/knative.dev/client/cmd/kn:latest service list Note \u4ece\u5bb9\u5668\u6620\u50cf\u8fd0\u884c kn \u4e0d\u4f1a\u5c06\u4e8c\u8fdb\u5236\u6587\u4ef6\u653e\u5728\u6c38\u4e45\u8def\u5f84\u4e0a\u3002\u6bcf\u6b21\u8981\u4f7f\u7528 kn \u65f6\u90fd\u5fc5\u987b\u91cd\u590d\u6b64\u8fc7\u7a0b\u3002 \u5b89\u88c5Knative quickstart \u63d2\u4ef6 \u00b6 \u7528 Homebrew Using a binary Using Go \u6267\u884c\u4ee5\u4e0b\u64cd\u4f5c\u4e4b\u4e00\uff1a \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u7528 Homebrew \u5b89\u88c5 quickstart : brew install knative-sandbox/kn-plugins/quickstart \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u5c06 quickstart \u66f4\u65b0\u5230\u6700\u65b0\u7248\u672c: brew upgrade knative-sandbox/kn-plugins/quickstart Download the binary for your system from the quickstart release page . Rename the file to remove the OS and architecture information. For example, rename kn-quickstart-amd64 to kn-quickstart . Make the plugin executable. For example, chmod +x kn-quickstart . Move the executable binary file to a directory on your PATH , for example, in /usr/local/bin . Verify that the plugin is working by running the command: kn quickstart --help Check out the kn-plugin-quickstart repository: git clone https://github.com/knative-sandbox/kn-plugin-quickstart.git cd kn-plugin-quickstart/ Build an executable binary: hack/build.sh Move the executable binary file to a directory on your PATH : mv kn-quickstart /usr/local/bin Verify that the plugin is working by running the command: kn quickstart --help Run the Knative quickstart plugin \u00b6 The quickstart plugin completes the following functions: Checks if you have the selected Kubernetes instance installed Creates a cluster called knative Installs Knative Serving with Kourier as the default networking layer, and sslip.io as the DNS Installs Knative Eventing and creates an in-memory Broker and Channel implementation To get a local deployment of Knative, run the quickstart plugin: Using kind Using minikube Install Knative and Kubernetes on a local Docker daemon by running: kn quickstart kind After the plugin is finished, verify you have a cluster called knative : kind get clusters Install Knative and Kubernetes in a minikube instance by running: Note The minikube cluster will be created with 6 GB of RAM. If you don't have enough memory, you can change to a different value not lower than 3 GB by running the command minikube config set memory 3078 before this command. kn quickstart minikube The output of the previous command asked you to run minikube tunnel. Run the following command to start the process in a secondary terminal window, then return to the primary window and press enter to continue: minikube tunnel --profile knative The tunnel must continue to run in a terminal window any time you are using your Knative quickstart environment. The tunnel command is required because it allows your cluster to access Knative ingress service as a LoadBalancer from your host computer. Note To terminate the tunnel process and clean up network routes, enter Ctrl-C . For more information about the minikube tunnel command, see the minikube documentation . After the plugin is finished, verify you have a cluster called knative : minikube profile list Next steps \u00b6 Learn how to deploy your first Service in the Knative tutorial . Try out Knative code samples . See the Knative Serving and Knative Eventing guides.","title":"\u901a\u8fc7quickstart\u5b89\u88c5Knative"},{"location":"install/quickstart-install/#quickstart-knative","text":"\u672c\u4e3b\u9898\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528 Knative quickstart \u63d2\u4ef6\u5b89\u88c5 Knative Serving \u548c Eventing \u7684\u672c\u5730\u90e8\u7f72\uff08local deployment\uff09\u3002 \u8be5\u63d2\u4ef6\u5728\u672c\u5730 Kubernetes \u96c6\u7fa4\u4e0a\u5b89\u88c5\u9884\u914d\u7f6e\u7684 Knative \u90e8\u7f72\uff08deployment\uff09\u3002 \u8b66\u544a Knative quickstart \u73af\u5883\u4ec5\u4f9b\u5b9e\u9a8c\u4f7f\u7528\u3002 \u5173\u4e8e\u751f\u6210\u73af\u5883\u7684\u5b89\u88c5\uff0c\u8bf7\u53c2\u9605 \u57fa\u4e8eYAML\u7684\u5b89\u88c5 \u6216 Knative Operator\u5b89\u88c5","title":"\u4f7f\u7528quickstart\u5b89\u88c5 Knative"},{"location":"install/quickstart-install/#_1","text":"\u5728\u5f00\u59cb\u4f7f\u7528 Knative quickstart \u90e8\u7f72\u4e4b\u524d\uff0c\u60a8\u5fc5\u987b\u5b89\u88c5\uff1a kind (Kubernetes in Docker) or minikube \u4ee5 \u4f7f\u7528 Docker \u5bb9\u5668\u8282\u70b9\u8fd0\u884c\u672c\u5730 Kubernetes \u96c6\u7fa4 Kubernetes CLI ( kubectl ) \u4ee5\u5728Kubernetes\u96c6\u7fa4\u8fd0\u884c\u547d\u4ee4\u3002 \u4f60\u53ef\u4ee5\u4f7f\u7528 kubectl \u6765\u90e8\u7f72\u5e94\u7528\uff0c\u68c0\u67e5\u548c\u7ba1\u7406\u96c6\u7fa4\u8d44\u6e90\u4ee5\u53ca\u67e5\u770b\u65e5\u5fd7\u3002 Knative CLI ( kn ). \u6709\u5173\u8bf4\u660e\uff0c\u8bf7\u53c2\u9605\u4e0b\u4e00\u8282. \u60a8\u9700\u8981\u81f3\u5c11 3 \u4e2a CPU \u548c 3 GB \u7684 RAM \u624d\u80fd\u521b\u5efa\u96c6\u7fa4","title":"\u5f00\u59cb\u4e4b\u524d"},{"location":"install/quickstart-install/#install-the-knative-cli","text":"Knative CLI ( kn ) \u4e3a\u521b\u5efa Knative \u8d44\u6e90\uff08\u4f8b\u5982 Knative \u670d\u52a1\u548c\u4e8b\u4ef6\u6e90\uff09\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5feb\u901f\u7b80\u5355\u7684\u5de5\u5177\uff0c\u800c\u65e0\u9700\u76f4\u63a5\u521b\u5efa\u6216\u4fee\u6539 YAML \u6587\u4ef6\u3002 kn CLI \u8fd8\u7b80\u5316\u4e86\u5176\u4ed6\u590d\u6742\u6b65\u9aa4\u7684\u5b8c\u6210\uff0c\u4f8b\u5982\u81ea\u52a8\u4f38\u7f29\u548c\u6d41\u91cf\u62c6\u5206\u3002 \u7528 Homebrew \u7528\u4e8c\u8fdb\u5236\u6587\u4ef6 \u7528 Go \u7528docker\u955c\u50cf \u63d0\u793a \u56fd\u5185\u5b89\u88c5Homebrew\u7684\u65b9\u6cd5\uff0c\u53ef\u53c2\u8003\u77e5\u4e4e \u8fd9\u7bc7\u6587\u7ae0 \u6267\u884c\u4ee5\u4e0b\u64cd\u4f5c\u4e4b\u4e00\uff1a \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u7528 Homebrew \u5b89\u88c5 kn : brew install kn \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u5c06 kn \u66f4\u65b0\u5230\u6700\u65b0\u7248\u672c: brew upgrade kn \u4f7f\u7528Homebrew\u5347\u7ea7 kn \u65f6\u9047\u5230\u95ee\u9898? \u5982\u679c\u60a8\u5728\u4f7f\u7528 Homebrew \u5347\u7ea7 kn \u65f6\u9047\u5230\u95ee\u9898\uff0c\u53ef\u80fd\u662f\u7531\u4e8eCLI\u4ed3\u5e93\u7684master\u5206\u652f\u88ab\u91cd\u547d\u540d\u4e3amain\u4e86. \u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u89e3\u51b3\u6b64\u95ee\u9898\uff1a brew tap --repair brew update brew upgrade kn \u60a8\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8f7d\u5bf9\u5e94\u4f60\u7684\u64cd\u4f5c\u7cfb\u7edf\u7684 kn \u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6\u5e76\u5c06\u5176\u653e\u7f6e\u5728\u7cfb\u7edf\u8def\u5f84\u4e2d\u6765\u5b89\u88c5 kn \u3002\u8bf7\u6ce8\u610f\uff0c\u60a8\u5c06\u9700\u8981 kn v0.25 \u6216\u66f4\u9ad8\u7248\u672c\u3002 \u4ece kn \u53d1\u5e03\u9875\u9762 . \u4e0b\u8f7d\u5bf9\u5e94\u4f60\u7684\u64cd\u4f5c\u7cfb\u7edf\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u3002 \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5c06\u4e8c\u8fdb\u5236\u6587\u4ef6\u91cd\u547d\u540d\u4e3a kn \u5e76\u4f7f\u5176\u53ef\u6267\u884c\uff1a mv <path-to-binary-file> kn chmod +x kn \u5176\u4e2d <path-to-binary-file> \u662f\u4f60\u5728\u4e0a\u4e00\u6b65\u4e0b\u8f7d\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u7684\u5b58\u653e\u8def\u5f84, \u6bd4\u5982, kn-darwin-amd64 \u6216 kn-linux-amd64 . \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5c06\u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6\u79fb\u52a8\u5230 PATH \u4e0a\u7684\u76ee\u5f55\uff1a mv kn /usr/local/bin \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u9a8c\u8bc1\u63d2\u4ef6\u662f\u5426\u6b63\u5e38\u5de5\u4f5c\uff1a kn version Check out kn \u5ba2\u6237\u7aef\u4ed3\u5e93: git clone https://github.com/knative/client.git cd client/ \u6784\u5efa\u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6: hack/build.sh -f \u5c06 kn \u52a0\u5165\u4f60\u7684\u7cfb\u7edf\u8def\u5f84, \u5e76\u9a8c\u8bc1 kn \u547d\u4ee4\u662f\u5426\u6b63\u5e38\u5de5\u4f5c\u3002\u4f8b\u5982\uff1a kn version \u955c\u50cf\u94fe\u63a5: Latest release \u4f60\u53ef\u4ee5\u5728\u5bb9\u5668\u955c\u50cf\u4e2d\u8fd0\u884c kn \u547d\u4ee4\uff0c\u4f8b\u5982: docker run --rm -v \" $HOME /.kube/config:/root/.kube/config\" gcr.io/knative-releases/knative.dev/client/cmd/kn:latest service list Note \u4ece\u5bb9\u5668\u6620\u50cf\u8fd0\u884c kn \u4e0d\u4f1a\u5c06\u4e8c\u8fdb\u5236\u6587\u4ef6\u653e\u5728\u6c38\u4e45\u8def\u5f84\u4e0a\u3002\u6bcf\u6b21\u8981\u4f7f\u7528 kn \u65f6\u90fd\u5fc5\u987b\u91cd\u590d\u6b64\u8fc7\u7a0b\u3002","title":"Install the Knative CLI"},{"location":"install/quickstart-install/#knative-quickstart","text":"\u7528 Homebrew Using a binary Using Go \u6267\u884c\u4ee5\u4e0b\u64cd\u4f5c\u4e4b\u4e00\uff1a \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u7528 Homebrew \u5b89\u88c5 quickstart : brew install knative-sandbox/kn-plugins/quickstart \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u5c06 quickstart \u66f4\u65b0\u5230\u6700\u65b0\u7248\u672c: brew upgrade knative-sandbox/kn-plugins/quickstart Download the binary for your system from the quickstart release page . Rename the file to remove the OS and architecture information. For example, rename kn-quickstart-amd64 to kn-quickstart . Make the plugin executable. For example, chmod +x kn-quickstart . Move the executable binary file to a directory on your PATH , for example, in /usr/local/bin . Verify that the plugin is working by running the command: kn quickstart --help Check out the kn-plugin-quickstart repository: git clone https://github.com/knative-sandbox/kn-plugin-quickstart.git cd kn-plugin-quickstart/ Build an executable binary: hack/build.sh Move the executable binary file to a directory on your PATH : mv kn-quickstart /usr/local/bin Verify that the plugin is working by running the command: kn quickstart --help","title":"\u5b89\u88c5Knative quickstart \u63d2\u4ef6"},{"location":"install/quickstart-install/#run-the-knative-quickstart-plugin","text":"The quickstart plugin completes the following functions: Checks if you have the selected Kubernetes instance installed Creates a cluster called knative Installs Knative Serving with Kourier as the default networking layer, and sslip.io as the DNS Installs Knative Eventing and creates an in-memory Broker and Channel implementation To get a local deployment of Knative, run the quickstart plugin: Using kind Using minikube Install Knative and Kubernetes on a local Docker daemon by running: kn quickstart kind After the plugin is finished, verify you have a cluster called knative : kind get clusters Install Knative and Kubernetes in a minikube instance by running: Note The minikube cluster will be created with 6 GB of RAM. If you don't have enough memory, you can change to a different value not lower than 3 GB by running the command minikube config set memory 3078 before this command. kn quickstart minikube The output of the previous command asked you to run minikube tunnel. Run the following command to start the process in a secondary terminal window, then return to the primary window and press enter to continue: minikube tunnel --profile knative The tunnel must continue to run in a terminal window any time you are using your Knative quickstart environment. The tunnel command is required because it allows your cluster to access Knative ingress service as a LoadBalancer from your host computer. Note To terminate the tunnel process and clean up network routes, enter Ctrl-C . For more information about the minikube tunnel command, see the minikube documentation . After the plugin is finished, verify you have a cluster called knative : minikube profile list","title":"Run the Knative quickstart plugin"},{"location":"install/quickstart-install/#next-steps","text":"Learn how to deploy your first Service in the Knative tutorial . Try out Knative code samples . See the Knative Serving and Knative Eventing guides.","title":"Next steps"},{"location":"install/uninstall/","text":"Uninstalling Knative \u00b6 To uninstall an Operator-based Knative installation, see the following Uninstall an Operator-based Knative Installation procedure. To uninstall a YAML-based Knative installation, see the following Uninstall a YAML-based Knative Installation procedure. Uninstalling a YAML-based Knative installation \u00b6 To uninstall a YAML-based Knative installation: Uninstalling optional Serving extensions \u00b6 Uninstall any Serving extensions you have installed by performing the steps in the following relevant tab: HPA autoscaling TLS with cert-manager TLS via HTTP01 Knative also supports the use of the Kubernetes Horizontal Pod Autoscaler (HPA) for driving autoscaling decisions. The following command will uninstall the components needed to support HPA-class autoscaling: kubectl delete -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-hpa.yaml Uninstall the component that integrates Knative with cert-manager: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-certmanager/latest/release.yaml Optional: if you no longer need cert-manager, uninstall it by following the steps in the cert-manager documentation . Uninstall the net-http01 controller by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-http01/latest/release.yaml Uninstalling a networking layer \u00b6 Follow the relevant procedure to uninstall the networking layer you installed: Contour Istio Kourier The following commands uninstall Contour and enable its Knative integration. Uninstall the Knative Contour controller by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-contour/latest/net-contour.yaml Uninstall Contour: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-contour/latest/contour.yaml The following commands uninstall Istio and enable its Knative integration. Uninstall the Knative Istio controller by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-istio/latest/net-istio.yaml Optional: if you no longer need Istio, uninstall it by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-istio/latest/istio.yaml Uninstall the Knative Kourier controller by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-kourier/latest/kourier.yaml Uninstalling the Serving component \u00b6 Uninstall the Serving core components by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-core.yaml Uninstall the required custom resources by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-crds.yaml Uninstalling optional Eventing extensions \u00b6 Uninstall any Eventing extensions you have installed by following the relevant procedure: Apache Kafka Sink Sugar Controller GitHub Source Apache Kafka Source GCP Sources Apache CouchDB Source VMware Sources and Bindings Uninstall the Kafka Sink data plane: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-sink.yaml Uninstall the Kafka controller: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Uninstall the Eventing Sugar Controller by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-sugar-controller.yaml Uninstall a single-tenant GitHub source by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-github/latest/github.yaml Uninstall a multi-tenant GitHub source by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-github/latest/mt-github.yaml Uninstall the Apache Kafka source by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-kafka/latest/source.yaml Uninstall the GCP sources by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/knative-gcp/latest/cloud-run-events.yaml Uninstall the Apache CouchDB source by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-couchdb/latest/couchdb.yaml Uninstall the VMware sources and bindings by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/sources-for-knative/latest/release.yaml Uninstalling an optional Broker (Eventing) layer \u00b6 Uninstall a Broker (Eventing) layer, if you installed one: Apache Kafka Broker MT-Channel-based Uninstall the Kafka Broker data plane by running the following command: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-broker.yaml Uninstall the Kafka controller by running the following command: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Uninstall the broker by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing/latest/mt-channel-broker.yaml Uninstalling optional channel (messaging) layers \u00b6 Uninstall each channel layer you have installed: Apache Kafka Channel Google Cloud Pub/Sub Channel In-Memory (standalone) NATS Channel Uninstall the Apache Kafka Channel by running: curl -L \"https://storage.googleapis.com/knative-nightly/eventing-kafka/latest/channel-consolidated.yaml\" \\ | sed 's/REPLACE_WITH_CLUSTER_URL/my-cluster-kafka-bootstrap.kafka:9092/' \\ | kubectl delete -f - Uninstall the Google Cloud Pub/Sub Channel by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/knative-gcp/latest/cloud-run-events.yaml Uninstall the in-memory channel implementation by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing/latest/in-memory-channel.yaml Uninstall the NATS Streaming channel by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-natss/latest/eventing-natss.yaml Uninstall NATS Streaming for Kubernetes. For more information, see the eventing-natss repository in GitHub. Uninstalling the Eventing component \u00b6 Uninstall the Eventing core components by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-core.yaml Uninstall the required custom resources by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-crds.yaml Uninstall an Operator-based Knative installation \u00b6 To uninstall an Operator-based Knative installation, follow these procedures: Removing the Knative Serving component \u00b6 Remove the Knative Serving CR: kubectl delete KnativeServing knative-serving -n knative-serving Removing Knative Eventing component \u00b6 Remove the Knative Eventing CR: kubectl delete KnativeEventing knative-eventing -n knative-eventing Knative operator prevents unsafe removal of Knative resources. Even if the Knative Serving and Knative Eventing CRs are successfully removed, all the CRDs in Knative are still kept in the cluster. All your resources relying on Knative CRDs can still work. Removing the Knative Operator: \u00b6 If you have installed Knative using the Release page, remove the operator using the following command: kubectl delete -f https://storage.googleapis.com/knative-nightly/operator/latest/operator.yaml If you have installed Knative from source, uninstall it using the following command while in the root directory for the source: ko delete -f config/","title":"\u5378\u8f7dKnative"},{"location":"install/uninstall/#uninstalling-knative","text":"To uninstall an Operator-based Knative installation, see the following Uninstall an Operator-based Knative Installation procedure. To uninstall a YAML-based Knative installation, see the following Uninstall a YAML-based Knative Installation procedure.","title":"Uninstalling Knative"},{"location":"install/uninstall/#uninstalling-a-yaml-based-knative-installation","text":"To uninstall a YAML-based Knative installation:","title":"Uninstalling a YAML-based Knative installation"},{"location":"install/uninstall/#uninstalling-optional-serving-extensions","text":"Uninstall any Serving extensions you have installed by performing the steps in the following relevant tab: HPA autoscaling TLS with cert-manager TLS via HTTP01 Knative also supports the use of the Kubernetes Horizontal Pod Autoscaler (HPA) for driving autoscaling decisions. The following command will uninstall the components needed to support HPA-class autoscaling: kubectl delete -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-hpa.yaml Uninstall the component that integrates Knative with cert-manager: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-certmanager/latest/release.yaml Optional: if you no longer need cert-manager, uninstall it by following the steps in the cert-manager documentation . Uninstall the net-http01 controller by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-http01/latest/release.yaml","title":"Uninstalling optional Serving extensions"},{"location":"install/uninstall/#uninstalling-a-networking-layer","text":"Follow the relevant procedure to uninstall the networking layer you installed: Contour Istio Kourier The following commands uninstall Contour and enable its Knative integration. Uninstall the Knative Contour controller by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-contour/latest/net-contour.yaml Uninstall Contour: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-contour/latest/contour.yaml The following commands uninstall Istio and enable its Knative integration. Uninstall the Knative Istio controller by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-istio/latest/net-istio.yaml Optional: if you no longer need Istio, uninstall it by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-istio/latest/istio.yaml Uninstall the Knative Kourier controller by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/net-kourier/latest/kourier.yaml","title":"Uninstalling a networking layer"},{"location":"install/uninstall/#uninstalling-the-serving-component","text":"Uninstall the Serving core components by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-core.yaml Uninstall the required custom resources by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-crds.yaml","title":"Uninstalling the Serving component"},{"location":"install/uninstall/#uninstalling-optional-eventing-extensions","text":"Uninstall any Eventing extensions you have installed by following the relevant procedure: Apache Kafka Sink Sugar Controller GitHub Source Apache Kafka Source GCP Sources Apache CouchDB Source VMware Sources and Bindings Uninstall the Kafka Sink data plane: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-sink.yaml Uninstall the Kafka controller: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Uninstall the Eventing Sugar Controller by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-sugar-controller.yaml Uninstall a single-tenant GitHub source by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-github/latest/github.yaml Uninstall a multi-tenant GitHub source by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-github/latest/mt-github.yaml Uninstall the Apache Kafka source by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-kafka/latest/source.yaml Uninstall the GCP sources by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/knative-gcp/latest/cloud-run-events.yaml Uninstall the Apache CouchDB source by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-couchdb/latest/couchdb.yaml Uninstall the VMware sources and bindings by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/sources-for-knative/latest/release.yaml","title":"Uninstalling optional Eventing extensions"},{"location":"install/uninstall/#uninstalling-an-optional-broker-eventing-layer","text":"Uninstall a Broker (Eventing) layer, if you installed one: Apache Kafka Broker MT-Channel-based Uninstall the Kafka Broker data plane by running the following command: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-broker.yaml Uninstall the Kafka controller by running the following command: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Uninstall the broker by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing/latest/mt-channel-broker.yaml","title":"Uninstalling an optional Broker (Eventing) layer"},{"location":"install/uninstall/#uninstalling-optional-channel-messaging-layers","text":"Uninstall each channel layer you have installed: Apache Kafka Channel Google Cloud Pub/Sub Channel In-Memory (standalone) NATS Channel Uninstall the Apache Kafka Channel by running: curl -L \"https://storage.googleapis.com/knative-nightly/eventing-kafka/latest/channel-consolidated.yaml\" \\ | sed 's/REPLACE_WITH_CLUSTER_URL/my-cluster-kafka-bootstrap.kafka:9092/' \\ | kubectl delete -f - Uninstall the Google Cloud Pub/Sub Channel by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/knative-gcp/latest/cloud-run-events.yaml Uninstall the in-memory channel implementation by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing/latest/in-memory-channel.yaml Uninstall the NATS Streaming channel by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing-natss/latest/eventing-natss.yaml Uninstall NATS Streaming for Kubernetes. For more information, see the eventing-natss repository in GitHub.","title":"Uninstalling optional channel (messaging) layers"},{"location":"install/uninstall/#uninstalling-the-eventing-component","text":"Uninstall the Eventing core components by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-core.yaml Uninstall the required custom resources by running: kubectl delete -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-crds.yaml","title":"Uninstalling the Eventing component"},{"location":"install/uninstall/#uninstall-an-operator-based-knative-installation","text":"To uninstall an Operator-based Knative installation, follow these procedures:","title":"Uninstall an Operator-based Knative installation"},{"location":"install/uninstall/#removing-the-knative-serving-component","text":"Remove the Knative Serving CR: kubectl delete KnativeServing knative-serving -n knative-serving","title":"Removing the Knative Serving component"},{"location":"install/uninstall/#removing-knative-eventing-component","text":"Remove the Knative Eventing CR: kubectl delete KnativeEventing knative-eventing -n knative-eventing Knative operator prevents unsafe removal of Knative resources. Even if the Knative Serving and Knative Eventing CRs are successfully removed, all the CRDs in Knative are still kept in the cluster. All your resources relying on Knative CRDs can still work.","title":"Removing Knative Eventing component"},{"location":"install/uninstall/#removing-the-knative-operator","text":"If you have installed Knative using the Release page, remove the operator using the following command: kubectl delete -f https://storage.googleapis.com/knative-nightly/operator/latest/operator.yaml If you have installed Knative from source, uninstall it using the following command while in the root directory for the source: ko delete -f config/","title":"Removing the Knative Operator:"},{"location":"install/client/","text":"CLI tools \u00b6 The following CLI tools are supported for use with Knative. kubectl \u00b6 You can use kubectl to apply the YAML files required to install Knative components, and also to create Knative resources, such as services and event sources using YAML. See Install and Set Up kubectl . kn \u00b6 kn provides a quick and easy interface for creating Knative resources such as services and event sources, without the need to create or modify YAML files directly. kn also simplifies completion of otherwise complex procedures such as autoscaling and traffic splitting. Note kn cannot be used to install Knative components such as Serving or Eventing. See Installing kn . Connecting CLI tools to your cluster \u00b6 After you have installed kubectl or kn , these tools will search for the kubeconfig file of your cluster in the default location of $HOME/.kube/config , and will use this file to connect to the cluster. A kubeconfig file is usually automatically created when you create a Kubernetes cluster. You can also set the environment variable $KUBECONFIG , and point it to the kubeconfig file. Using the kn CLI, you can specify the following options to connect to the cluster: --kubeconfig : use this option to point to the kubeconfig file. This is equivalent to setting the $KUBECONFIG environment variable. --context : use this option to specify the name of a context from the existing kubeconfig file. Use one of the contexts from the output of kubectl config get-contexts . You can also specify a config file in the following ways: Setting the environment variable $KUBECONFIG , and point it to the kubeconfig file. Using the kn CLI --config option, for example, kn service list --config path/to/config.yaml . The default config is at ~/.config/kn/config.yaml . For more information about kubeconfig files, see Organizing Cluster Access Using kubeconfig Files . Using kubeconfig files with your platform \u00b6 Instructions for using kubeconfig files are available for the following platforms: Amazon EKS Google GKE IBM IKS Red Hat OpenShift Cloud Platform Starting minikube writes this file automatically, or provides an appropriate context in an existing configuration file.","title":"CLI \u5de5\u5177\u6982\u8ff0"},{"location":"install/client/#cli-tools","text":"The following CLI tools are supported for use with Knative.","title":"CLI tools"},{"location":"install/client/#kubectl","text":"You can use kubectl to apply the YAML files required to install Knative components, and also to create Knative resources, such as services and event sources using YAML. See Install and Set Up kubectl .","title":"kubectl"},{"location":"install/client/#kn","text":"kn provides a quick and easy interface for creating Knative resources such as services and event sources, without the need to create or modify YAML files directly. kn also simplifies completion of otherwise complex procedures such as autoscaling and traffic splitting. Note kn cannot be used to install Knative components such as Serving or Eventing. See Installing kn .","title":"kn"},{"location":"install/client/#connecting-cli-tools-to-your-cluster","text":"After you have installed kubectl or kn , these tools will search for the kubeconfig file of your cluster in the default location of $HOME/.kube/config , and will use this file to connect to the cluster. A kubeconfig file is usually automatically created when you create a Kubernetes cluster. You can also set the environment variable $KUBECONFIG , and point it to the kubeconfig file. Using the kn CLI, you can specify the following options to connect to the cluster: --kubeconfig : use this option to point to the kubeconfig file. This is equivalent to setting the $KUBECONFIG environment variable. --context : use this option to specify the name of a context from the existing kubeconfig file. Use one of the contexts from the output of kubectl config get-contexts . You can also specify a config file in the following ways: Setting the environment variable $KUBECONFIG , and point it to the kubeconfig file. Using the kn CLI --config option, for example, kn service list --config path/to/config.yaml . The default config is at ~/.config/kn/config.yaml . For more information about kubeconfig files, see Organizing Cluster Access Using kubeconfig Files .","title":"Connecting CLI tools to your cluster"},{"location":"install/client/#using-kubeconfig-files-with-your-platform","text":"Instructions for using kubeconfig files are available for the following platforms: Amazon EKS Google GKE IBM IKS Red Hat OpenShift Cloud Platform Starting minikube writes this file automatically, or provides an appropriate context in an existing configuration file.","title":"Using kubeconfig files with your platform"},{"location":"install/client/configure-kn/","text":"Customizing kn \u00b6 You can customize your kn CLI setup by creating a config.yaml configuration file. You can provide this configuration by using the --config flag, otherwise the configuration is picked up from a default location. The default configuration location conforms to the XDG Base Directory Specification , and is different for Unix systems and Windows systems. If the XDG_CONFIG_HOME environment variable is set, the default configuration location that kn looks for is $XDG_CONFIG_HOME/kn . If the XDG_CONFIG_HOME environment variable is not set, kn looks for the configuration in the home directory of the user at $HOME/.config/kn/config.yaml . For Windows systems, the default kn configuration location is %APPDATA%\\kn . Example configuration file \u00b6 plugins : path-lookup : true directory : ~/.config/kn/plugins eventing : sink-mappings : - prefix : svc group : core version : v1 resource : services Where path-lookup specifies whether kn should look for plugins in the PATH environment variable. This is a boolean configuration option (default: true ). Note: the path-lookup option has been deprecated and will be removed in a future version where path lookup will be enabled unconditionally. directory specifies the directory where kn will look for plugins. The default path depends on the operating system, as described earlier. This can be any directory that is visible to the user (default: $base_dir/plugins , where $base_dir is the directory where this configuration file is stored). sink-mappings defines the Kubernetes Addressable resource that is used when you use the --sink flag with a kn CLI command. prefix : The prefix you want to use to describe your sink. Service, svc , channel , and broker are predefined prefixes in kn . group : The API group of the Kubernetes resource. version : The version of the Kubernetes resource. resource : The lowercased, plural name of the Kubernetes resource type. For example, services or brokers .","title":"\u81ea\u5b9a\u4e49 kn"},{"location":"install/client/configure-kn/#customizing-kn","text":"You can customize your kn CLI setup by creating a config.yaml configuration file. You can provide this configuration by using the --config flag, otherwise the configuration is picked up from a default location. The default configuration location conforms to the XDG Base Directory Specification , and is different for Unix systems and Windows systems. If the XDG_CONFIG_HOME environment variable is set, the default configuration location that kn looks for is $XDG_CONFIG_HOME/kn . If the XDG_CONFIG_HOME environment variable is not set, kn looks for the configuration in the home directory of the user at $HOME/.config/kn/config.yaml . For Windows systems, the default kn configuration location is %APPDATA%\\kn .","title":"Customizing kn"},{"location":"install/client/configure-kn/#example-configuration-file","text":"plugins : path-lookup : true directory : ~/.config/kn/plugins eventing : sink-mappings : - prefix : svc group : core version : v1 resource : services Where path-lookup specifies whether kn should look for plugins in the PATH environment variable. This is a boolean configuration option (default: true ). Note: the path-lookup option has been deprecated and will be removed in a future version where path lookup will be enabled unconditionally. directory specifies the directory where kn will look for plugins. The default path depends on the operating system, as described earlier. This can be any directory that is visible to the user (default: $base_dir/plugins , where $base_dir is the directory where this configuration file is stored). sink-mappings defines the Kubernetes Addressable resource that is used when you use the --sink flag with a kn CLI command. prefix : The prefix you want to use to describe your sink. Service, svc , channel , and broker are predefined prefixes in kn . group : The API group of the Kubernetes resource. version : The version of the Kubernetes resource. resource : The lowercased, plural name of the Kubernetes resource type. For example, services or brokers .","title":"Example configuration file"},{"location":"install/client/install-kn/","text":"Installing kn \u00b6 This guide provides details about how you can install the Knative kn CLI. Install the kn CLI \u00b6 Knative CLI ( kn ) \u4e3a\u521b\u5efa Knative \u8d44\u6e90\uff08\u4f8b\u5982 Knative \u670d\u52a1\u548c\u4e8b\u4ef6\u6e90\uff09\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5feb\u901f\u7b80\u5355\u7684\u5de5\u5177\uff0c\u800c\u65e0\u9700\u76f4\u63a5\u521b\u5efa\u6216\u4fee\u6539 YAML \u6587\u4ef6\u3002 kn CLI \u8fd8\u7b80\u5316\u4e86\u5176\u4ed6\u590d\u6742\u6b65\u9aa4\u7684\u5b8c\u6210\uff0c\u4f8b\u5982\u81ea\u52a8\u4f38\u7f29\u548c\u6d41\u91cf\u62c6\u5206\u3002 \u7528 Homebrew \u7528\u4e8c\u8fdb\u5236\u6587\u4ef6 \u7528 Go \u7528docker\u955c\u50cf \u63d0\u793a \u56fd\u5185\u5b89\u88c5Homebrew\u7684\u65b9\u6cd5\uff0c\u53ef\u53c2\u8003\u77e5\u4e4e \u8fd9\u7bc7\u6587\u7ae0 \u6267\u884c\u4ee5\u4e0b\u64cd\u4f5c\u4e4b\u4e00\uff1a \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u7528 Homebrew \u5b89\u88c5 kn : brew install kn \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u5c06 kn \u66f4\u65b0\u5230\u6700\u65b0\u7248\u672c: brew upgrade kn \u4f7f\u7528Homebrew\u5347\u7ea7 kn \u65f6\u9047\u5230\u95ee\u9898? \u5982\u679c\u60a8\u5728\u4f7f\u7528 Homebrew \u5347\u7ea7 kn \u65f6\u9047\u5230\u95ee\u9898\uff0c\u53ef\u80fd\u662f\u7531\u4e8eCLI\u4ed3\u5e93\u7684master\u5206\u652f\u88ab\u91cd\u547d\u540d\u4e3amain\u4e86. \u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u89e3\u51b3\u6b64\u95ee\u9898\uff1a brew tap --repair brew update brew upgrade kn \u60a8\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8f7d\u5bf9\u5e94\u4f60\u7684\u64cd\u4f5c\u7cfb\u7edf\u7684 kn \u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6\u5e76\u5c06\u5176\u653e\u7f6e\u5728\u7cfb\u7edf\u8def\u5f84\u4e2d\u6765\u5b89\u88c5 kn \u3002\u8bf7\u6ce8\u610f\uff0c\u60a8\u5c06\u9700\u8981 kn v0.25 \u6216\u66f4\u9ad8\u7248\u672c\u3002 \u4ece kn \u53d1\u5e03\u9875\u9762 . \u4e0b\u8f7d\u5bf9\u5e94\u4f60\u7684\u64cd\u4f5c\u7cfb\u7edf\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u3002 \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5c06\u4e8c\u8fdb\u5236\u6587\u4ef6\u91cd\u547d\u540d\u4e3a kn \u5e76\u4f7f\u5176\u53ef\u6267\u884c\uff1a mv <path-to-binary-file> kn chmod +x kn \u5176\u4e2d <path-to-binary-file> \u662f\u4f60\u5728\u4e0a\u4e00\u6b65\u4e0b\u8f7d\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u7684\u5b58\u653e\u8def\u5f84, \u6bd4\u5982, kn-darwin-amd64 \u6216 kn-linux-amd64 . \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5c06\u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6\u79fb\u52a8\u5230 PATH \u4e0a\u7684\u76ee\u5f55\uff1a mv kn /usr/local/bin \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u9a8c\u8bc1\u63d2\u4ef6\u662f\u5426\u6b63\u5e38\u5de5\u4f5c\uff1a kn version Check out kn \u5ba2\u6237\u7aef\u4ed3\u5e93: git clone https://github.com/knative/client.git cd client/ \u6784\u5efa\u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6: hack/build.sh -f \u5c06 kn \u52a0\u5165\u4f60\u7684\u7cfb\u7edf\u8def\u5f84, \u5e76\u9a8c\u8bc1 kn \u547d\u4ee4\u662f\u5426\u6b63\u5e38\u5de5\u4f5c\u3002\u4f8b\u5982\uff1a kn version \u955c\u50cf\u94fe\u63a5: Latest release \u4f60\u53ef\u4ee5\u5728\u5bb9\u5668\u955c\u50cf\u4e2d\u8fd0\u884c kn \u547d\u4ee4\uff0c\u4f8b\u5982: docker run --rm -v \" $HOME /.kube/config:/root/.kube/config\" gcr.io/knative-releases/knative.dev/client/cmd/kn:latest service list Note \u4ece\u5bb9\u5668\u6620\u50cf\u8fd0\u884c kn \u4e0d\u4f1a\u5c06\u4e8c\u8fdb\u5236\u6587\u4ef6\u653e\u5728\u6c38\u4e45\u8def\u5f84\u4e0a\u3002\u6bcf\u6b21\u8981\u4f7f\u7528 kn \u65f6\u90fd\u5fc5\u987b\u91cd\u590d\u6b64\u8fc7\u7a0b\u3002 Install kn using the nightly-built binary \u00b6 Warning Nightly container images include features which may not be included in the latest Knative release and are not considered to be stable. Nightly-built executable binaries are available for users who want to install the latest pre-release build of kn . Links to the latest nightly-built executable binaries are available here: macOS Linux Windows Using kn with Tekton \u00b6 See the Tekton documentation .","title":"\u5b89\u88c5 kn"},{"location":"install/client/install-kn/#installing-kn","text":"This guide provides details about how you can install the Knative kn CLI.","title":"Installing kn"},{"location":"install/client/install-kn/#install-the-kn-cli","text":"Knative CLI ( kn ) \u4e3a\u521b\u5efa Knative \u8d44\u6e90\uff08\u4f8b\u5982 Knative \u670d\u52a1\u548c\u4e8b\u4ef6\u6e90\uff09\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5feb\u901f\u7b80\u5355\u7684\u5de5\u5177\uff0c\u800c\u65e0\u9700\u76f4\u63a5\u521b\u5efa\u6216\u4fee\u6539 YAML \u6587\u4ef6\u3002 kn CLI \u8fd8\u7b80\u5316\u4e86\u5176\u4ed6\u590d\u6742\u6b65\u9aa4\u7684\u5b8c\u6210\uff0c\u4f8b\u5982\u81ea\u52a8\u4f38\u7f29\u548c\u6d41\u91cf\u62c6\u5206\u3002 \u7528 Homebrew \u7528\u4e8c\u8fdb\u5236\u6587\u4ef6 \u7528 Go \u7528docker\u955c\u50cf \u63d0\u793a \u56fd\u5185\u5b89\u88c5Homebrew\u7684\u65b9\u6cd5\uff0c\u53ef\u53c2\u8003\u77e5\u4e4e \u8fd9\u7bc7\u6587\u7ae0 \u6267\u884c\u4ee5\u4e0b\u64cd\u4f5c\u4e4b\u4e00\uff1a \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u7528 Homebrew \u5b89\u88c5 kn : brew install kn \u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff0c\u5c06 kn \u66f4\u65b0\u5230\u6700\u65b0\u7248\u672c: brew upgrade kn \u4f7f\u7528Homebrew\u5347\u7ea7 kn \u65f6\u9047\u5230\u95ee\u9898? \u5982\u679c\u60a8\u5728\u4f7f\u7528 Homebrew \u5347\u7ea7 kn \u65f6\u9047\u5230\u95ee\u9898\uff0c\u53ef\u80fd\u662f\u7531\u4e8eCLI\u4ed3\u5e93\u7684master\u5206\u652f\u88ab\u91cd\u547d\u540d\u4e3amain\u4e86. \u53ef\u4ee5\u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u89e3\u51b3\u6b64\u95ee\u9898\uff1a brew tap --repair brew update brew upgrade kn \u60a8\u53ef\u4ee5\u901a\u8fc7\u4e0b\u8f7d\u5bf9\u5e94\u4f60\u7684\u64cd\u4f5c\u7cfb\u7edf\u7684 kn \u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6\u5e76\u5c06\u5176\u653e\u7f6e\u5728\u7cfb\u7edf\u8def\u5f84\u4e2d\u6765\u5b89\u88c5 kn \u3002\u8bf7\u6ce8\u610f\uff0c\u60a8\u5c06\u9700\u8981 kn v0.25 \u6216\u66f4\u9ad8\u7248\u672c\u3002 \u4ece kn \u53d1\u5e03\u9875\u9762 . \u4e0b\u8f7d\u5bf9\u5e94\u4f60\u7684\u64cd\u4f5c\u7cfb\u7edf\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u3002 \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5c06\u4e8c\u8fdb\u5236\u6587\u4ef6\u91cd\u547d\u540d\u4e3a kn \u5e76\u4f7f\u5176\u53ef\u6267\u884c\uff1a mv <path-to-binary-file> kn chmod +x kn \u5176\u4e2d <path-to-binary-file> \u662f\u4f60\u5728\u4e0a\u4e00\u6b65\u4e0b\u8f7d\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u7684\u5b58\u653e\u8def\u5f84, \u6bd4\u5982, kn-darwin-amd64 \u6216 kn-linux-amd64 . \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u5c06\u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6\u79fb\u52a8\u5230 PATH \u4e0a\u7684\u76ee\u5f55\uff1a mv kn /usr/local/bin \u901a\u8fc7\u8fd0\u884c\u4ee5\u4e0b\u547d\u4ee4\u9a8c\u8bc1\u63d2\u4ef6\u662f\u5426\u6b63\u5e38\u5de5\u4f5c\uff1a kn version Check out kn \u5ba2\u6237\u7aef\u4ed3\u5e93: git clone https://github.com/knative/client.git cd client/ \u6784\u5efa\u53ef\u6267\u884c\u4e8c\u8fdb\u5236\u6587\u4ef6: hack/build.sh -f \u5c06 kn \u52a0\u5165\u4f60\u7684\u7cfb\u7edf\u8def\u5f84, \u5e76\u9a8c\u8bc1 kn \u547d\u4ee4\u662f\u5426\u6b63\u5e38\u5de5\u4f5c\u3002\u4f8b\u5982\uff1a kn version \u955c\u50cf\u94fe\u63a5: Latest release \u4f60\u53ef\u4ee5\u5728\u5bb9\u5668\u955c\u50cf\u4e2d\u8fd0\u884c kn \u547d\u4ee4\uff0c\u4f8b\u5982: docker run --rm -v \" $HOME /.kube/config:/root/.kube/config\" gcr.io/knative-releases/knative.dev/client/cmd/kn:latest service list Note \u4ece\u5bb9\u5668\u6620\u50cf\u8fd0\u884c kn \u4e0d\u4f1a\u5c06\u4e8c\u8fdb\u5236\u6587\u4ef6\u653e\u5728\u6c38\u4e45\u8def\u5f84\u4e0a\u3002\u6bcf\u6b21\u8981\u4f7f\u7528 kn \u65f6\u90fd\u5fc5\u987b\u91cd\u590d\u6b64\u8fc7\u7a0b\u3002","title":"Install the kn CLI"},{"location":"install/client/install-kn/#install-kn-using-the-nightly-built-binary","text":"Warning Nightly container images include features which may not be included in the latest Knative release and are not considered to be stable. Nightly-built executable binaries are available for users who want to install the latest pre-release build of kn . Links to the latest nightly-built executable binaries are available here: macOS Linux Windows","title":"Install kn using the nightly-built binary"},{"location":"install/client/install-kn/#using-kn-with-tekton","text":"See the Tekton documentation .","title":"Using kn with Tekton"},{"location":"install/client/kn-plugins/","text":"kn plugins \u00b6 The kn CLI supports the use of plugins. Plugins enable you to extend the functionality of your kn installation by adding custom commands and other shared commands that are not part of the core distribution of kn . Warning The plugins must be named with the prefix kn- to be detected by kn . For example, kn-func will be detected but func won't be detected. How to install a plugin \u00b6 Manual installation \u00b6 You can manually install all plugins. To manually install a plugin: Download the current release of the plugin from GitHub. See the list of Knative plugins you can download. Rename the file to remove the OS and architecture information. For example, rename kn-admin-darwin-amd64 to kn-admin . Make the plugin executable. For example, chmod +x kn-admin . Move the file to a directory on your PATH . For example, /usr/local/bin . Homebrew \u00b6 You can install some plugins can be installed using the Knative plugins Homebrew Tap . For example, you can install the kn-admin plugin by running brew install knative-sandbox/kn-plugins/admin . List of Knative plugins \u00b6 Plugin Description Available via Homebrew? kn-plugin-admin kn plugin for managing a Kubernetes based Knative installation Y kn-plugin-diag kn plugin for diagnosing issues by exposing detailed information for different layers of Knative objects N kn-plugin-event kn plugin for sending events to Knative sinks Y kn-plugin-func kn plugin for functions Y kn-plugin-migration kn plugin for migrating Knative Services from one cluster to another N kn-plugin-operator kn plugin for managing Knative with Knative Operator N kn-plugin-quickstart kn plugin for developers to install a quickstart Knative cluster for experimentation purposes Y kn-plugin-service-log kn plugin for showing the standard output of Knative Services N kn-plugin-source-kafka kn plugin for managing Kafka event sources Y kn-plugin-source-kamelet kn plugin for managing Kamelets and KameletBindings Y","title":"kn \u63d2\u4ef6"},{"location":"install/client/kn-plugins/#kn-plugins","text":"The kn CLI supports the use of plugins. Plugins enable you to extend the functionality of your kn installation by adding custom commands and other shared commands that are not part of the core distribution of kn . Warning The plugins must be named with the prefix kn- to be detected by kn . For example, kn-func will be detected but func won't be detected.","title":"kn plugins"},{"location":"install/client/kn-plugins/#how-to-install-a-plugin","text":"","title":"How to install a plugin"},{"location":"install/client/kn-plugins/#manual-installation","text":"You can manually install all plugins. To manually install a plugin: Download the current release of the plugin from GitHub. See the list of Knative plugins you can download. Rename the file to remove the OS and architecture information. For example, rename kn-admin-darwin-amd64 to kn-admin . Make the plugin executable. For example, chmod +x kn-admin . Move the file to a directory on your PATH . For example, /usr/local/bin .","title":"Manual installation"},{"location":"install/client/kn-plugins/#homebrew","text":"You can install some plugins can be installed using the Knative plugins Homebrew Tap . For example, you can install the kn-admin plugin by running brew install knative-sandbox/kn-plugins/admin .","title":"Homebrew"},{"location":"install/client/kn-plugins/#list-of-knative-plugins","text":"Plugin Description Available via Homebrew? kn-plugin-admin kn plugin for managing a Kubernetes based Knative installation Y kn-plugin-diag kn plugin for diagnosing issues by exposing detailed information for different layers of Knative objects N kn-plugin-event kn plugin for sending events to Knative sinks Y kn-plugin-func kn plugin for functions Y kn-plugin-migration kn plugin for migrating Knative Services from one cluster to another N kn-plugin-operator kn plugin for managing Knative with Knative Operator N kn-plugin-quickstart kn plugin for developers to install a quickstart Knative cluster for experimentation purposes Y kn-plugin-service-log kn plugin for showing the standard output of Knative Services N kn-plugin-source-kafka kn plugin for managing Kafka event sources Y kn-plugin-source-kamelet kn plugin for managing Kamelets and KameletBindings Y","title":"List of Knative plugins"},{"location":"install/operator/configuring-eventing-cr/","text":"Configuring the Eventing Operator custom resource \u00b6 You can configure the Knative Eventing operator by modifying settings in the KnativeEventing custom resource (CR). Installing a specific version of Eventing \u00b6 Cluster administrators can install a specific version of Knative Eventing by using the spec.version field. For example, if you want to install Knative Eventing v0.19.0, you can apply the following KnativeEventing CR: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : version : 0.19.0 If spec.version is not specified, the Knative Operator will install the latest available version of Knative Eventing. If users specify an invalid or unavailable version, the Knative Operator will do nothing. The Knative Operator always includes the latest 3 minor release versions. If Knative Eventing is already managed by the Operator, updating the spec.version field in the KnativeEventing CR enables upgrading or downgrading the Knative Eventing version, without requiring modifications to the Operator. Note that the Knative Operator only permits upgrades or downgrades by one minor release version at a time. For example, if the current Knative Eventing deployment is version 0.18.x, you must upgrade to 0.19.x before upgrading to 0.20.x. Setting a default channel \u00b6 If you are using different channel implementations, like the KafkaChannel, or you want a specific configuration of the InMemoryChannel to be the default configuration, you can change the default behavior by updating the default-ch-webhook ConfigMap. You can do this by modifying the KnativeEventing CR: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : config : default-ch-webhook : default-ch-config : | clusterDefault: apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 10 replicationFactor: 1 namespaceDefaults: my-namespace: apiVersion: messaging.knative.dev/v1 kind: InMemoryChannel spec: delivery: backoffDelay: PT0.5S backoffPolicy: exponential retry: 5 Note The clusterDefault setting determines the global, cluster-wide default channel type. You can configure channel defaults for individual namespaces by using the namespaceDefaults setting. Setting the default channel for the broker \u00b6 If you are using a channel-based broker, you can change the default channel type for the broker from InMemoryChannel to KafkaChannel, by updating the config-br-default-channel ConfigMap. You can do this by modifying the KnativeEventing CR: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : config : config-br-default-channel : channel-template-spec : | apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 6 replicationFactor: 1 Private repository and private secrets \u00b6 The Knative Eventing Operator CR is configured the same way as the Knative Serving Operator CR. See the documentation on Private repository and private secret . Knative Eventing also specifies only one container within each Deployment resource. However, the container does not use the same name as its parent Deployment, which means that the container name in Knative Eventing is not the same unique identifier as it is in Knative Serving. List of containers within each Deployment resource: Component Deployment name Container name Core eventing eventing-controller eventing-controller Core eventing eventing-webhook eventing-webhook Eventing Broker broker-controller eventing-controller In-Memory Channel imc-controller controller In-Memory Channel imc-dispatcher dispatcher The default field can still be used to replace the images in a predefined format. However, if the container name is not a unique identifier, for example eventing-controller , you must use the override field to replace it, by specifying deployment/container as the unique key. Some images are defined by using the environment variable in Knative Eventing. They can be replaced by taking advantage of the override field. Download images in a predefined format without secrets \u00b6 This example shows how you can define custom image links that can be defined in the KnativeEventing CR using the simplified format docker.io/knative-images/${NAME}:{CUSTOM-TAG} . In the following example: The custom tag latest is used for all images. All image links are accessible without using secrets. Images are defined in the accepted format docker.io/knative-images/${NAME}:{CUSTOM-TAG} . Push images to the following image tags: Deployment Container Docker image eventing-controller eventing-controller docker.io/knative-images/eventing-controller:latest eventing-webhook docker.io/knative-images/eventing-webhook:latest broker-controller eventing-controller docker.io/knative-images/broker-eventing-controller:latest controller docker.io/knative-images/controller:latest dispatcher docker.io/knative-images/dispatcher:latest Define your the KnativeEventing CR with following content: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : default : docker.io/knative-images/${NAME}:latest override : broker-controller/eventing-controller : docker.io/knative-images-repo1/broker-eventing-controller:latest - `${NAME}` maps to the container name in each `Deployment` resource. - `default` is used to define the image format for all containers, except the container `eventing-controller` in the deployment `broker-controller`. To replace the image for this container, use the `override` field to specify individually, by using `broker-controller/eventing-controller` as the key. Download images from different repositories without secrets \u00b6 If your custom image links are not defined in a uniform format, you will need to individually include each link in the KnativeEventing CR. For example, to define the following list of images: Deployment Container Docker Image eventing-controller eventing-controller docker.io/knative-images/eventing-controller:latest eventing-webhook docker.io/knative-images/eventing-webhook:latest controller docker.io/knative-images/controller:latest dispatcher docker.io/knative-images/dispatcher:latest broker-controller eventing-controller docker.io/knative-images/broker-eventing-controller:latest The KnativeEventing CR must be modified to include the full list. For example: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : override : eventing-controller/eventing-controller : docker.io/knative-images-repo1/eventing-controller:latest eventing-webhook/eventing-webhook : docker.io/knative-images-repo2/eventing-webhook:latest imc-controller/controller : docker.io/knative-images-repo3/imc-controller:latest imc-dispatcher/dispatcher : docker.io/knative-images-repo4/imc-dispatcher:latest broker-controller/eventing-controller : docker.io/knative-images-repo5/broker-eventing-controller:latest If you want to replace the image defined by the environment variable, you must modify the KnativeEventing CR. For example, if you want to replace the image defined by the environment variable DISPATCHER_IMAGE , in the container controller , of the deployment imc-controller , and the target image is docker.io/knative-images-repo5/DISPATCHER_IMAGE:latest , the KnativeEventing CR would be as follows: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : override : eventing-controller/eventing-controller : docker.io/knative-images-repo1/eventing-controller:latest eventing-webhook/eventing-webhook : docker.io/knative-images-repo2/eventing-webhook:latest imc-controller/controller : docker.io/knative-images-repo3/imc-controller:latest imc-dispatcher/dispatcher : docker.io/knative-images-repo4/imc-dispatcher:latest broker-controller/eventing-controller : docker.io/knative-images-repo5/broker-eventing-controller:latest DISPATCHER_IMAGE : docker.io/knative-images-repo5/DISPATCHER_IMAGE:latest Download images with secrets \u00b6 If your image repository requires private secrets for access, you must append the imagePullSecrets attribute to the KnativeEventing CR. This example uses a secret named regcred . Refer to the Kubernetes documentation to create your own private secrets. After you create the secret, edit the KnativeEventing CR: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : ... imagePullSecrets : - name : regcred The field imagePullSecrets requires a list of secrets. You can add multiple secrets to access the images: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : ... imagePullSecrets : - name : regcred - name : regcred-2 ... Configuring the default broker class \u00b6 Knative Eventing allows you to define a default broker class when the user does not specify one. The Operator provides two broker classes by default: ChannelBasedBroker and MTChannelBasedBroker. The field defaultBrokerClass indicates which class to use; if empty, the ChannelBasedBroker is used. The following example CR specifies MTChannelBasedBroker as the default: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : defaultBrokerClass : MTChannelBasedBroker System resource settings \u00b6 The KnativeEventing CR allows you to configure system resources for Knative system containers. Requests and limits can be configured for the following containers: eventing-controller eventing-webhook imc-controller imc-dispatcher mt-broker-ingress mt-broker-ingress mt-broker-controller To override resource settings for a specific container, you must create an entry in the spec.resources list with the container name and the Kubernetes resource settings . Info If multiple deployments share the same container name, the configuration in spec.resources for that certain container will apply to all the deployments. Visit Override System Resources based on the deployment to specify the resources for a container within a specific deployment. For example, the following KnativeEventing CR configures the eventing-webhook container to request 0.3 CPU and 100MB of RAM, and sets hard limits of 1 CPU, 250MB RAM, and 4GB of local storage: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : resources : - container : eventing-webhook requests : cpu : 300m memory : 100Mi limits : cpu : 1000m memory : 250Mi Override system deployments \u00b6 If you would like to override some configurations for a specific deployment, you can override the configuration by using spec.deployments in the CR. Currently resources , replicas , labels , annotations and nodeSelector are supported. Override the resources \u00b6 The KnativeEventing custom resource is able to configure system resources for the Knative system containers based on the deployment. Requests and limits can be configured for all the available containers within the deployment, like eventing-controller , eventing-webhook , imc-controller , etc. For example, the following KnativeEventing resource configures the container eventing-controller in the deployment eventing-controller to request 0.3 CPU and 100MB of RAM, and sets hard limits of 1 CPU and 250MB RAM: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : deployments : - name : eventing-controller resources : - container : eventing-controller requests : cpu : 300m memory : 100Mi limits : cpu : 1000m memory : 250Mi Override the nodeSelector \u00b6 The KnativeEventing resource is able to override the nodeSelector for the Knative Eventing deployment resources. For example, if you would like to add the following tolerations nodeSelector : disktype : hdd to the deployment eventing-controller , you need to change your KnativeEventing CR as below: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : deployments : - name : eventing-controller nodeSelector : disktype : hdd Override the tolerations \u00b6 The KnativeEventing resource is able to override tolerations for the Knative Eventing deployment resources. For example, if you would like to add the following tolerations tolerations : - key : \"key1\" operator : \"Equal\" value : \"value1\" effect : \"NoSchedule\" to the deployment eventing-controller , you need to change your KnativeEventing CR as below: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : deployments : - name : eventing-controller tolerations : - key : \"key1\" operator : \"Equal\" value : \"value1\" effect : \"NoSchedule\" Override the affinity \u00b6 The KnativeEventing resource is able to override the affinity, including nodeAffinity, podAffinity, and podAntiAffinity, for the Knative Eventing deployment resources. For example, if you would like to add the following nodeAffinity affinity : nodeAffinity : preferredDuringSchedulingIgnoredDuringExecution : - weight : 1 preference : matchExpressions : - key : disktype operator : In values : - ssd to the deployment activator , you need to change your KnativeEventing CR as below: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : deployments : - name : activator affinity : nodeAffinity : preferredDuringSchedulingIgnoredDuringExecution : - weight : 1 preference : matchExpressions : - key : disktype operator : In values : - ssd","title":"\u914d\u7f6e Knative Eventing CRDs"},{"location":"install/operator/configuring-eventing-cr/#configuring-the-eventing-operator-custom-resource","text":"You can configure the Knative Eventing operator by modifying settings in the KnativeEventing custom resource (CR).","title":"Configuring the Eventing Operator custom resource"},{"location":"install/operator/configuring-eventing-cr/#installing-a-specific-version-of-eventing","text":"Cluster administrators can install a specific version of Knative Eventing by using the spec.version field. For example, if you want to install Knative Eventing v0.19.0, you can apply the following KnativeEventing CR: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : version : 0.19.0 If spec.version is not specified, the Knative Operator will install the latest available version of Knative Eventing. If users specify an invalid or unavailable version, the Knative Operator will do nothing. The Knative Operator always includes the latest 3 minor release versions. If Knative Eventing is already managed by the Operator, updating the spec.version field in the KnativeEventing CR enables upgrading or downgrading the Knative Eventing version, without requiring modifications to the Operator. Note that the Knative Operator only permits upgrades or downgrades by one minor release version at a time. For example, if the current Knative Eventing deployment is version 0.18.x, you must upgrade to 0.19.x before upgrading to 0.20.x.","title":"Installing a specific version of Eventing"},{"location":"install/operator/configuring-eventing-cr/#setting-a-default-channel","text":"If you are using different channel implementations, like the KafkaChannel, or you want a specific configuration of the InMemoryChannel to be the default configuration, you can change the default behavior by updating the default-ch-webhook ConfigMap. You can do this by modifying the KnativeEventing CR: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : config : default-ch-webhook : default-ch-config : | clusterDefault: apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 10 replicationFactor: 1 namespaceDefaults: my-namespace: apiVersion: messaging.knative.dev/v1 kind: InMemoryChannel spec: delivery: backoffDelay: PT0.5S backoffPolicy: exponential retry: 5 Note The clusterDefault setting determines the global, cluster-wide default channel type. You can configure channel defaults for individual namespaces by using the namespaceDefaults setting.","title":"Setting a default channel"},{"location":"install/operator/configuring-eventing-cr/#setting-the-default-channel-for-the-broker","text":"If you are using a channel-based broker, you can change the default channel type for the broker from InMemoryChannel to KafkaChannel, by updating the config-br-default-channel ConfigMap. You can do this by modifying the KnativeEventing CR: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : config : config-br-default-channel : channel-template-spec : | apiVersion: messaging.knative.dev/v1beta1 kind: KafkaChannel spec: numPartitions: 6 replicationFactor: 1","title":"Setting the default channel for the broker"},{"location":"install/operator/configuring-eventing-cr/#private-repository-and-private-secrets","text":"The Knative Eventing Operator CR is configured the same way as the Knative Serving Operator CR. See the documentation on Private repository and private secret . Knative Eventing also specifies only one container within each Deployment resource. However, the container does not use the same name as its parent Deployment, which means that the container name in Knative Eventing is not the same unique identifier as it is in Knative Serving. List of containers within each Deployment resource: Component Deployment name Container name Core eventing eventing-controller eventing-controller Core eventing eventing-webhook eventing-webhook Eventing Broker broker-controller eventing-controller In-Memory Channel imc-controller controller In-Memory Channel imc-dispatcher dispatcher The default field can still be used to replace the images in a predefined format. However, if the container name is not a unique identifier, for example eventing-controller , you must use the override field to replace it, by specifying deployment/container as the unique key. Some images are defined by using the environment variable in Knative Eventing. They can be replaced by taking advantage of the override field.","title":"Private repository and private secrets"},{"location":"install/operator/configuring-eventing-cr/#download-images-in-a-predefined-format-without-secrets","text":"This example shows how you can define custom image links that can be defined in the KnativeEventing CR using the simplified format docker.io/knative-images/${NAME}:{CUSTOM-TAG} . In the following example: The custom tag latest is used for all images. All image links are accessible without using secrets. Images are defined in the accepted format docker.io/knative-images/${NAME}:{CUSTOM-TAG} . Push images to the following image tags: Deployment Container Docker image eventing-controller eventing-controller docker.io/knative-images/eventing-controller:latest eventing-webhook docker.io/knative-images/eventing-webhook:latest broker-controller eventing-controller docker.io/knative-images/broker-eventing-controller:latest controller docker.io/knative-images/controller:latest dispatcher docker.io/knative-images/dispatcher:latest Define your the KnativeEventing CR with following content: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : default : docker.io/knative-images/${NAME}:latest override : broker-controller/eventing-controller : docker.io/knative-images-repo1/broker-eventing-controller:latest - `${NAME}` maps to the container name in each `Deployment` resource. - `default` is used to define the image format for all containers, except the container `eventing-controller` in the deployment `broker-controller`. To replace the image for this container, use the `override` field to specify individually, by using `broker-controller/eventing-controller` as the key.","title":"Download images in a predefined format without secrets"},{"location":"install/operator/configuring-eventing-cr/#download-images-from-different-repositories-without-secrets","text":"If your custom image links are not defined in a uniform format, you will need to individually include each link in the KnativeEventing CR. For example, to define the following list of images: Deployment Container Docker Image eventing-controller eventing-controller docker.io/knative-images/eventing-controller:latest eventing-webhook docker.io/knative-images/eventing-webhook:latest controller docker.io/knative-images/controller:latest dispatcher docker.io/knative-images/dispatcher:latest broker-controller eventing-controller docker.io/knative-images/broker-eventing-controller:latest The KnativeEventing CR must be modified to include the full list. For example: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : override : eventing-controller/eventing-controller : docker.io/knative-images-repo1/eventing-controller:latest eventing-webhook/eventing-webhook : docker.io/knative-images-repo2/eventing-webhook:latest imc-controller/controller : docker.io/knative-images-repo3/imc-controller:latest imc-dispatcher/dispatcher : docker.io/knative-images-repo4/imc-dispatcher:latest broker-controller/eventing-controller : docker.io/knative-images-repo5/broker-eventing-controller:latest If you want to replace the image defined by the environment variable, you must modify the KnativeEventing CR. For example, if you want to replace the image defined by the environment variable DISPATCHER_IMAGE , in the container controller , of the deployment imc-controller , and the target image is docker.io/knative-images-repo5/DISPATCHER_IMAGE:latest , the KnativeEventing CR would be as follows: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : override : eventing-controller/eventing-controller : docker.io/knative-images-repo1/eventing-controller:latest eventing-webhook/eventing-webhook : docker.io/knative-images-repo2/eventing-webhook:latest imc-controller/controller : docker.io/knative-images-repo3/imc-controller:latest imc-dispatcher/dispatcher : docker.io/knative-images-repo4/imc-dispatcher:latest broker-controller/eventing-controller : docker.io/knative-images-repo5/broker-eventing-controller:latest DISPATCHER_IMAGE : docker.io/knative-images-repo5/DISPATCHER_IMAGE:latest","title":"Download images from different repositories without secrets"},{"location":"install/operator/configuring-eventing-cr/#download-images-with-secrets","text":"If your image repository requires private secrets for access, you must append the imagePullSecrets attribute to the KnativeEventing CR. This example uses a secret named regcred . Refer to the Kubernetes documentation to create your own private secrets. After you create the secret, edit the KnativeEventing CR: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : ... imagePullSecrets : - name : regcred The field imagePullSecrets requires a list of secrets. You can add multiple secrets to access the images: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : registry : ... imagePullSecrets : - name : regcred - name : regcred-2 ...","title":"Download images with secrets"},{"location":"install/operator/configuring-eventing-cr/#configuring-the-default-broker-class","text":"Knative Eventing allows you to define a default broker class when the user does not specify one. The Operator provides two broker classes by default: ChannelBasedBroker and MTChannelBasedBroker. The field defaultBrokerClass indicates which class to use; if empty, the ChannelBasedBroker is used. The following example CR specifies MTChannelBasedBroker as the default: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : defaultBrokerClass : MTChannelBasedBroker","title":"Configuring the default broker class"},{"location":"install/operator/configuring-eventing-cr/#system-resource-settings","text":"The KnativeEventing CR allows you to configure system resources for Knative system containers. Requests and limits can be configured for the following containers: eventing-controller eventing-webhook imc-controller imc-dispatcher mt-broker-ingress mt-broker-ingress mt-broker-controller To override resource settings for a specific container, you must create an entry in the spec.resources list with the container name and the Kubernetes resource settings . Info If multiple deployments share the same container name, the configuration in spec.resources for that certain container will apply to all the deployments. Visit Override System Resources based on the deployment to specify the resources for a container within a specific deployment. For example, the following KnativeEventing CR configures the eventing-webhook container to request 0.3 CPU and 100MB of RAM, and sets hard limits of 1 CPU, 250MB RAM, and 4GB of local storage: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : resources : - container : eventing-webhook requests : cpu : 300m memory : 100Mi limits : cpu : 1000m memory : 250Mi","title":"System resource settings"},{"location":"install/operator/configuring-eventing-cr/#override-system-deployments","text":"If you would like to override some configurations for a specific deployment, you can override the configuration by using spec.deployments in the CR. Currently resources , replicas , labels , annotations and nodeSelector are supported.","title":"Override system deployments"},{"location":"install/operator/configuring-eventing-cr/#override-the-resources","text":"The KnativeEventing custom resource is able to configure system resources for the Knative system containers based on the deployment. Requests and limits can be configured for all the available containers within the deployment, like eventing-controller , eventing-webhook , imc-controller , etc. For example, the following KnativeEventing resource configures the container eventing-controller in the deployment eventing-controller to request 0.3 CPU and 100MB of RAM, and sets hard limits of 1 CPU and 250MB RAM: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : deployments : - name : eventing-controller resources : - container : eventing-controller requests : cpu : 300m memory : 100Mi limits : cpu : 1000m memory : 250Mi","title":"Override the resources"},{"location":"install/operator/configuring-eventing-cr/#override-the-nodeselector","text":"The KnativeEventing resource is able to override the nodeSelector for the Knative Eventing deployment resources. For example, if you would like to add the following tolerations nodeSelector : disktype : hdd to the deployment eventing-controller , you need to change your KnativeEventing CR as below: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : deployments : - name : eventing-controller nodeSelector : disktype : hdd","title":"Override the nodeSelector"},{"location":"install/operator/configuring-eventing-cr/#override-the-tolerations","text":"The KnativeEventing resource is able to override tolerations for the Knative Eventing deployment resources. For example, if you would like to add the following tolerations tolerations : - key : \"key1\" operator : \"Equal\" value : \"value1\" effect : \"NoSchedule\" to the deployment eventing-controller , you need to change your KnativeEventing CR as below: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : deployments : - name : eventing-controller tolerations : - key : \"key1\" operator : \"Equal\" value : \"value1\" effect : \"NoSchedule\"","title":"Override the tolerations"},{"location":"install/operator/configuring-eventing-cr/#override-the-affinity","text":"The KnativeEventing resource is able to override the affinity, including nodeAffinity, podAffinity, and podAntiAffinity, for the Knative Eventing deployment resources. For example, if you would like to add the following nodeAffinity affinity : nodeAffinity : preferredDuringSchedulingIgnoredDuringExecution : - weight : 1 preference : matchExpressions : - key : disktype operator : In values : - ssd to the deployment activator , you need to change your KnativeEventing CR as below: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : deployments : - name : activator affinity : nodeAffinity : preferredDuringSchedulingIgnoredDuringExecution : - weight : 1 preference : matchExpressions : - key : disktype operator : In values : - ssd","title":"Override the affinity"},{"location":"install/operator/configuring-serving-cr/","text":"Configuring the Knative Serving Operator custom resource \u00b6 The Knative Serving Operator can be configured with the following options: Version configuration Private repository and private secret SSL certificate for controller Replace the default istio-ingressgateway service Replace the knative-ingress-gateway gateway Cluster local gateway High availability System resource settings Override system deployments Version configuration \u00b6 Cluster administrators can install a specific version of Knative Serving by using the spec.version field. For example, if you want to install Knative Serving v0.23.0, you can apply the following KnativeServing custom resource: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : version : 0.23.0 If spec.version is not specified, the Knative Operator installs the latest available version of Knative Serving. If users specify an invalid or unavailable version, the Knative Operator will do nothing. The Knative Operator always includes the latest 3 minor release versions. For example, if the current version of the Knative Operator is v0.24.0, the earliest version of Knative Serving available through the Operator is v0.22.0. If Knative Serving is already managed by the Operator, updating the spec.version field in the KnativeServing resource enables upgrading or downgrading the Knative Serving version, without needing to change the Operator. Important The Knative Operator only permits upgrades or downgrades by one minor release version at a time. For example, if the current Knative Serving deployment is version v0.22.0, you must upgrade to v0.23.0 before upgrading to v0.24.0. Private repository and private secrets \u00b6 You can use the spec.registry section of the operator CR to change the image references to point to a private registry or specify imagePullSecrets : default : this field defines a image reference template for all Knative images. The format is example-registry.io/custom/path/${NAME}:{CUSTOM-TAG} . If you use the same tag for all your images, the only difference is the image name. ${NAME} is a pre-defined variable in the operator corresponding to the container name. If you name the images in your private repo to align with the container names ( activator , autoscaler , controller , webhook , autoscaler-hpa , net-istio-controller , and queue-proxy ), the default argument should be sufficient. override : a map from container name to the full registry location. This section is only needed when the registry images do not match the common naming format. For containers whose name matches a key, the value is used in preference to the image name calculated by default . If a container's name does not match a key in override , the template in default is used. imagePullSecrets : a list of Secret names used when pulling Knative container images. The Secrets must be created in the same namespace as the Knative Serving Deployments. See deploying images from a private container registry for configuration details. Download images in a predefined format without secrets: \u00b6 This example shows how you can define custom image links that can be defined in the CR using the simplified format docker.io/knative-images/${NAME}:{CUSTOM-TAG} . In the following example: the custom tag v0.13.0 is used for all images all image links are accessible without using secrets images are pushed as docker.io/knative-images/${NAME}:{CUSTOM-TAG} First, you need to make sure your images pushed to the following image tags: Container Docker Image activator docker.io/knative-images/activator:v0.13.0 autoscaler docker.io/knative-images/autoscaler:v0.13.0 controller docker.io/knative-images/controller:v0.13.0 webhook docker.io/knative-images/webhook:v0.13.0 autoscaler-hpa docker.io/knative-images/autoscaler-hpa:v0.13.0 net-istio-controller docker.io/knative-images/net-istio-controller:v0.13.0 queue-proxy docker.io/knative-images/queue-proxy:v0.13.0 Then, you need to define your operator CR with following content: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : registry : default : docker.io/knative-images/${NAME}:v0.13.0 Download images individually without secrets \u00b6 If your custom image links are not defined in a uniform format by default, you will need to individually include each link in the CR. For example, to given the following images: Container Docker Image activator docker.io/knative-images-repo1/activator:v0.13.0 autoscaler docker.io/knative-images-repo2/autoscaler:v0.13.0 controller docker.io/knative-images-repo3/controller:v0.13.0 webhook docker.io/knative-images-repo4/webhook:v0.13.0 autoscaler-hpa docker.io/knative-images-repo5/autoscaler-hpa:v0.13.0 net-istio-controller docker.io/knative-images-repo6/prefix-net-istio-controller:v0.13.0 net-istio-webhook docker.io/knative-images-repo6/net-istio-webhooko:v0.13.0 queue-proxy docker.io/knative-images-repo7/queue-proxy-suffix:v0.13.0 The Operator CR should be modified to include the full list: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : registry : override : activator : docker.io/knative-images-repo1/activator:v0.13.0 autoscaler : docker.io/knative-images-repo2/autoscaler:v0.13.0 controller : docker.io/knative-images-repo3/controller:v0.13.0 webhook : docker.io/knative-images-repo4/webhook:v0.13.0 autoscaler-hpa : docker.io/knative-images-repo5/autoscaler-hpa:v0.13.0 net-istio-controller/controller : docker.io/knative-images-repo6/prefix-net-istio-controller:v0.13.0 net-istio-webhook/webhook : docker.io/knative-images-repo6/net-istio-webhook:v0.13.0 queue-proxy : docker.io/knative-images-repo7/queue-proxy-suffix:v0.13.0 Note If the container name is not unique across all Deployments, DaemonSets and Jobs, you can prefix the container name with the parent container name and a slash. For example, istio-webhook/webhook . Download images with secrets \u00b6 If your image repository requires private secrets for access, include the imagePullSecrets attribute. This example uses a secret named regcred . You must create your own private secrets if these are required: From existing docker credentials From command line for docker credentials Create your own secret After you create this secret, edit the Operator CR by appending the following content: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : registry : ... imagePullSecrets : - name : regcred The field imagePullSecrets expects a list of secrets. You can add multiple secrets to access the images as follows: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : registry : ... imagePullSecrets : - name : regcred - name : regcred-2 ... SSL certificate for controller \u00b6 To enable tag to digest resolution , the Knative Serving controller needs to access the container registry. To allow the controller to trust a self-signed registry cert, you can use the Operator to specify the certificate using a ConfigMap or Secret. Specify the following fields in spec.controller-custom-certs to select a custom registry certificate: name : the name of the ConfigMap or Secret. type : either the string \"ConfigMap\" or \"Secret\". If you create a ConfigMap named testCert containing the certificate, change your CR: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : controller-custom-certs : name : testCert type : ConfigMap Replace the default istio-ingressgateway-service \u00b6 To set up a custom ingress gateway, follow Step 1: Create Gateway Service and Deployment Instance . Step 2: Update the Knative gateway \u00b6 Update spec.ingress.istio.knative-ingress-gateway to select the labels of the new ingress gateway: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : ingress : istio : enabled : true knative-ingress-gateway : selector : istio : ingressgateway Step 3: Update Gateway ConfigMap \u00b6 Additionally, you will need to update the Istio ConfigMap: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : ingress : istio : enabled : true knative-ingress-gateway : selector : istio : ingressgateway config : istio : gateway.knative-serving.knative-ingress-gateway : \"custom-ingressgateway.custom-ns.svc.cluster.local\" The key in spec.config.istio is in the format of gateway.<gateway_namespace>.<gateway_name> . Replace the knative-ingress-gateway gateway \u00b6 To create the ingress gateway, follow Step 1: Create the Gateway . Step 2: Update Gateway ConfigMap \u00b6 You will need to update the Istio ConfigMap: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : config : istio : gateway.custom-ns.knative-custom-gateway : \"istio-ingressgateway.istio-system.svc.cluster.local\" The key in spec.config.istio is in the format of gateway.<gateway_namespace>.<gateway_name> . Configuration of cluster local gateway \u00b6 Update spec.ingress.istio.knative-local-gateway to select the labels of the new cluster-local ingress gateway: Default local gateway name: \u00b6 Go through the installing Istio guide to use local cluster gateway, if you use the default gateway called knative-local-gateway . Non-default local gateway name: \u00b6 If you create custom local gateway with a name other than knative-local-gateway , update config.istio and the knative-local-gateway selector: This example shows a service and deployment knative-local-gateway in the namespace istio-system , with the label custom: custom-local-gw : apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : ingress : istio : enabled : true knative-local-gateway : selector : custom : custom-local-gateway config : istio : local-gateway.knative-serving.knative-local-gateway : \"custom-local-gateway.istio-system.svc.cluster.local\" High availability \u00b6 By default, Knative Serving runs a single instance of each deployment. The spec.high-availability field allows you to configure the number of replicas for all deployments managed by the operator. The following configuration specifies a replica count of 3 for the deployments: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : high-availability : replicas : 3 The replicas field also configures the HorizontalPodAutoscaler resources based on the spec.high-availability . Let's say the operator includes the following HorizontalPodAutoscaler: apiVersion : autoscaling/v2beta2 kind : HorizontalPodAutoscaler metadata : ... spec : minReplicas : 3 maxReplicas : 5 If you configure replicas: 2 , which is less than minReplicas , the operator transforms minReplicas to 1 . If you configure replicas: 6 , which is more than maxReplicas , the operator transforms maxReplicas to maxReplicas + (replicas - minReplicas) which is 8 . System Resource Settings \u00b6 The operator custom resource allows you to configure system resources for the Knative system containers. Requests and limits can be configured for the following containers: activator , autoscaler , controller , webhook , autoscaler-hpa , net-istio-controller and queue-proxy . Info If multiple deployments share the same container name, the configuration in spec.resources for that certain container will apply to all the deployments. To override resource settings for a specific container, create an entry in the spec.resources list with the container name and the Kubernetes resource settings . For example, the following KnativeServing resource configures the activator to request 0.3 CPU and 100MB of RAM, and sets hard limits of 1 CPU, 250MB RAM, and 4GB of local storage: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : resources : - container : activator requests : cpu : 300m memory : 100Mi limits : cpu : 1000m memory : 250Mi ephemeral-storage : 4Gi If you would like to add another container autoscaler with the same configuration, you need to change your CR as follows: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : resources : - container : activator requests : cpu : 300m memory : 100Mi limits : cpu : 1000m memory : 250Mi ephemeral-storage : 4Gi - container : autoscaler requests : cpu : 300m memory : 100Mi limits : cpu : 1000m memory : 250Mi ephemeral-storage : 4Gi Override system deployments \u00b6 If you would like to override some configurations for a specific deployment, you can override the configuration by using spec.deployments in CR. Currently replicas , labels , annotations and nodeSelector are supported. Override replicas, labels and annotations \u00b6 The following KnativeServing resource overrides the webhook deployment to have 3 Replicas, the label mylabel: foo , and the annotation myannotataions: bar , while other system deployments have 2 Replicas by using spec.high-availability . apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : high-availability : replicas : 2 deployments : - name : webhook replicas : 3 labels : mylabel : foo annotations : myannotataions : bar Note The KnativeServing resource label and annotation settings override the webhook's labels and annotations for both Deployments and Pods. Override the nodeSelector \u00b6 The following KnativeServing resource overrides the webhook deployment to use the disktype: hdd nodeSelector: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : deployments : - name : webhook nodeSelector : disktype : hdd Override the tolerations \u00b6 The KnativeServing resource is able to override tolerations for the Knative Serving deployment resources. For example, if you would like to add the following tolerations tolerations : - key : \"key1\" operator : \"Equal\" value : \"value1\" effect : \"NoSchedule\" to the deployment activator , you need to change your KnativeServing CR as below: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : deployments : - name : activator tolerations : - key : \"key1\" operator : \"Equal\" value : \"value1\" effect : \"NoSchedule\" Override the affinity \u00b6 The KnativeServing resource is able to override the affinity, including nodeAffinity, podAffinity, and podAntiAffinity, for the Knative Serving deployment resources. For example, if you would like to add the following nodeAffinity affinity : nodeAffinity : preferredDuringSchedulingIgnoredDuringExecution : - weight : 1 preference : matchExpressions : - key : disktype operator : In values : - ssd to the deployment activator , you need to change your KnativeServing CR as below: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : deployments : - name : activator affinity : nodeAffinity : preferredDuringSchedulingIgnoredDuringExecution : - weight : 1 preference : matchExpressions : - key : disktype operator : In values : - ssd","title":"\u914d\u7f6e Knative Serving CRDs"},{"location":"install/operator/configuring-serving-cr/#configuring-the-knative-serving-operator-custom-resource","text":"The Knative Serving Operator can be configured with the following options: Version configuration Private repository and private secret SSL certificate for controller Replace the default istio-ingressgateway service Replace the knative-ingress-gateway gateway Cluster local gateway High availability System resource settings Override system deployments","title":"Configuring the Knative Serving Operator custom resource"},{"location":"install/operator/configuring-serving-cr/#version-configuration","text":"Cluster administrators can install a specific version of Knative Serving by using the spec.version field. For example, if you want to install Knative Serving v0.23.0, you can apply the following KnativeServing custom resource: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : version : 0.23.0 If spec.version is not specified, the Knative Operator installs the latest available version of Knative Serving. If users specify an invalid or unavailable version, the Knative Operator will do nothing. The Knative Operator always includes the latest 3 minor release versions. For example, if the current version of the Knative Operator is v0.24.0, the earliest version of Knative Serving available through the Operator is v0.22.0. If Knative Serving is already managed by the Operator, updating the spec.version field in the KnativeServing resource enables upgrading or downgrading the Knative Serving version, without needing to change the Operator. Important The Knative Operator only permits upgrades or downgrades by one minor release version at a time. For example, if the current Knative Serving deployment is version v0.22.0, you must upgrade to v0.23.0 before upgrading to v0.24.0.","title":"Version configuration"},{"location":"install/operator/configuring-serving-cr/#private-repository-and-private-secrets","text":"You can use the spec.registry section of the operator CR to change the image references to point to a private registry or specify imagePullSecrets : default : this field defines a image reference template for all Knative images. The format is example-registry.io/custom/path/${NAME}:{CUSTOM-TAG} . If you use the same tag for all your images, the only difference is the image name. ${NAME} is a pre-defined variable in the operator corresponding to the container name. If you name the images in your private repo to align with the container names ( activator , autoscaler , controller , webhook , autoscaler-hpa , net-istio-controller , and queue-proxy ), the default argument should be sufficient. override : a map from container name to the full registry location. This section is only needed when the registry images do not match the common naming format. For containers whose name matches a key, the value is used in preference to the image name calculated by default . If a container's name does not match a key in override , the template in default is used. imagePullSecrets : a list of Secret names used when pulling Knative container images. The Secrets must be created in the same namespace as the Knative Serving Deployments. See deploying images from a private container registry for configuration details.","title":"Private repository and private secrets"},{"location":"install/operator/configuring-serving-cr/#download-images-in-a-predefined-format-without-secrets","text":"This example shows how you can define custom image links that can be defined in the CR using the simplified format docker.io/knative-images/${NAME}:{CUSTOM-TAG} . In the following example: the custom tag v0.13.0 is used for all images all image links are accessible without using secrets images are pushed as docker.io/knative-images/${NAME}:{CUSTOM-TAG} First, you need to make sure your images pushed to the following image tags: Container Docker Image activator docker.io/knative-images/activator:v0.13.0 autoscaler docker.io/knative-images/autoscaler:v0.13.0 controller docker.io/knative-images/controller:v0.13.0 webhook docker.io/knative-images/webhook:v0.13.0 autoscaler-hpa docker.io/knative-images/autoscaler-hpa:v0.13.0 net-istio-controller docker.io/knative-images/net-istio-controller:v0.13.0 queue-proxy docker.io/knative-images/queue-proxy:v0.13.0 Then, you need to define your operator CR with following content: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : registry : default : docker.io/knative-images/${NAME}:v0.13.0","title":"Download images in a predefined format without secrets:"},{"location":"install/operator/configuring-serving-cr/#download-images-individually-without-secrets","text":"If your custom image links are not defined in a uniform format by default, you will need to individually include each link in the CR. For example, to given the following images: Container Docker Image activator docker.io/knative-images-repo1/activator:v0.13.0 autoscaler docker.io/knative-images-repo2/autoscaler:v0.13.0 controller docker.io/knative-images-repo3/controller:v0.13.0 webhook docker.io/knative-images-repo4/webhook:v0.13.0 autoscaler-hpa docker.io/knative-images-repo5/autoscaler-hpa:v0.13.0 net-istio-controller docker.io/knative-images-repo6/prefix-net-istio-controller:v0.13.0 net-istio-webhook docker.io/knative-images-repo6/net-istio-webhooko:v0.13.0 queue-proxy docker.io/knative-images-repo7/queue-proxy-suffix:v0.13.0 The Operator CR should be modified to include the full list: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : registry : override : activator : docker.io/knative-images-repo1/activator:v0.13.0 autoscaler : docker.io/knative-images-repo2/autoscaler:v0.13.0 controller : docker.io/knative-images-repo3/controller:v0.13.0 webhook : docker.io/knative-images-repo4/webhook:v0.13.0 autoscaler-hpa : docker.io/knative-images-repo5/autoscaler-hpa:v0.13.0 net-istio-controller/controller : docker.io/knative-images-repo6/prefix-net-istio-controller:v0.13.0 net-istio-webhook/webhook : docker.io/knative-images-repo6/net-istio-webhook:v0.13.0 queue-proxy : docker.io/knative-images-repo7/queue-proxy-suffix:v0.13.0 Note If the container name is not unique across all Deployments, DaemonSets and Jobs, you can prefix the container name with the parent container name and a slash. For example, istio-webhook/webhook .","title":"Download images individually without secrets"},{"location":"install/operator/configuring-serving-cr/#download-images-with-secrets","text":"If your image repository requires private secrets for access, include the imagePullSecrets attribute. This example uses a secret named regcred . You must create your own private secrets if these are required: From existing docker credentials From command line for docker credentials Create your own secret After you create this secret, edit the Operator CR by appending the following content: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : registry : ... imagePullSecrets : - name : regcred The field imagePullSecrets expects a list of secrets. You can add multiple secrets to access the images as follows: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : registry : ... imagePullSecrets : - name : regcred - name : regcred-2 ...","title":"Download images with secrets"},{"location":"install/operator/configuring-serving-cr/#ssl-certificate-for-controller","text":"To enable tag to digest resolution , the Knative Serving controller needs to access the container registry. To allow the controller to trust a self-signed registry cert, you can use the Operator to specify the certificate using a ConfigMap or Secret. Specify the following fields in spec.controller-custom-certs to select a custom registry certificate: name : the name of the ConfigMap or Secret. type : either the string \"ConfigMap\" or \"Secret\". If you create a ConfigMap named testCert containing the certificate, change your CR: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : controller-custom-certs : name : testCert type : ConfigMap","title":"SSL certificate for controller"},{"location":"install/operator/configuring-serving-cr/#replace-the-default-istio-ingressgateway-service","text":"To set up a custom ingress gateway, follow Step 1: Create Gateway Service and Deployment Instance .","title":"Replace the default istio-ingressgateway-service"},{"location":"install/operator/configuring-serving-cr/#step-2-update-the-knative-gateway","text":"Update spec.ingress.istio.knative-ingress-gateway to select the labels of the new ingress gateway: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : ingress : istio : enabled : true knative-ingress-gateway : selector : istio : ingressgateway","title":"Step 2: Update the Knative gateway"},{"location":"install/operator/configuring-serving-cr/#step-3-update-gateway-configmap","text":"Additionally, you will need to update the Istio ConfigMap: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : ingress : istio : enabled : true knative-ingress-gateway : selector : istio : ingressgateway config : istio : gateway.knative-serving.knative-ingress-gateway : \"custom-ingressgateway.custom-ns.svc.cluster.local\" The key in spec.config.istio is in the format of gateway.<gateway_namespace>.<gateway_name> .","title":"Step 3: Update Gateway ConfigMap"},{"location":"install/operator/configuring-serving-cr/#replace-the-knative-ingress-gateway-gateway","text":"To create the ingress gateway, follow Step 1: Create the Gateway .","title":"Replace the knative-ingress-gateway gateway"},{"location":"install/operator/configuring-serving-cr/#step-2-update-gateway-configmap","text":"You will need to update the Istio ConfigMap: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : config : istio : gateway.custom-ns.knative-custom-gateway : \"istio-ingressgateway.istio-system.svc.cluster.local\" The key in spec.config.istio is in the format of gateway.<gateway_namespace>.<gateway_name> .","title":"Step 2: Update Gateway ConfigMap"},{"location":"install/operator/configuring-serving-cr/#configuration-of-cluster-local-gateway","text":"Update spec.ingress.istio.knative-local-gateway to select the labels of the new cluster-local ingress gateway:","title":"Configuration of cluster local gateway"},{"location":"install/operator/configuring-serving-cr/#default-local-gateway-name","text":"Go through the installing Istio guide to use local cluster gateway, if you use the default gateway called knative-local-gateway .","title":"Default local gateway name:"},{"location":"install/operator/configuring-serving-cr/#non-default-local-gateway-name","text":"If you create custom local gateway with a name other than knative-local-gateway , update config.istio and the knative-local-gateway selector: This example shows a service and deployment knative-local-gateway in the namespace istio-system , with the label custom: custom-local-gw : apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : ingress : istio : enabled : true knative-local-gateway : selector : custom : custom-local-gateway config : istio : local-gateway.knative-serving.knative-local-gateway : \"custom-local-gateway.istio-system.svc.cluster.local\"","title":"Non-default local gateway name:"},{"location":"install/operator/configuring-serving-cr/#high-availability","text":"By default, Knative Serving runs a single instance of each deployment. The spec.high-availability field allows you to configure the number of replicas for all deployments managed by the operator. The following configuration specifies a replica count of 3 for the deployments: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : high-availability : replicas : 3 The replicas field also configures the HorizontalPodAutoscaler resources based on the spec.high-availability . Let's say the operator includes the following HorizontalPodAutoscaler: apiVersion : autoscaling/v2beta2 kind : HorizontalPodAutoscaler metadata : ... spec : minReplicas : 3 maxReplicas : 5 If you configure replicas: 2 , which is less than minReplicas , the operator transforms minReplicas to 1 . If you configure replicas: 6 , which is more than maxReplicas , the operator transforms maxReplicas to maxReplicas + (replicas - minReplicas) which is 8 .","title":"High availability"},{"location":"install/operator/configuring-serving-cr/#system-resource-settings","text":"The operator custom resource allows you to configure system resources for the Knative system containers. Requests and limits can be configured for the following containers: activator , autoscaler , controller , webhook , autoscaler-hpa , net-istio-controller and queue-proxy . Info If multiple deployments share the same container name, the configuration in spec.resources for that certain container will apply to all the deployments. To override resource settings for a specific container, create an entry in the spec.resources list with the container name and the Kubernetes resource settings . For example, the following KnativeServing resource configures the activator to request 0.3 CPU and 100MB of RAM, and sets hard limits of 1 CPU, 250MB RAM, and 4GB of local storage: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : resources : - container : activator requests : cpu : 300m memory : 100Mi limits : cpu : 1000m memory : 250Mi ephemeral-storage : 4Gi If you would like to add another container autoscaler with the same configuration, you need to change your CR as follows: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : resources : - container : activator requests : cpu : 300m memory : 100Mi limits : cpu : 1000m memory : 250Mi ephemeral-storage : 4Gi - container : autoscaler requests : cpu : 300m memory : 100Mi limits : cpu : 1000m memory : 250Mi ephemeral-storage : 4Gi","title":"System Resource Settings"},{"location":"install/operator/configuring-serving-cr/#override-system-deployments","text":"If you would like to override some configurations for a specific deployment, you can override the configuration by using spec.deployments in CR. Currently replicas , labels , annotations and nodeSelector are supported.","title":"Override system deployments"},{"location":"install/operator/configuring-serving-cr/#override-replicas-labels-and-annotations","text":"The following KnativeServing resource overrides the webhook deployment to have 3 Replicas, the label mylabel: foo , and the annotation myannotataions: bar , while other system deployments have 2 Replicas by using spec.high-availability . apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : high-availability : replicas : 2 deployments : - name : webhook replicas : 3 labels : mylabel : foo annotations : myannotataions : bar Note The KnativeServing resource label and annotation settings override the webhook's labels and annotations for both Deployments and Pods.","title":"Override replicas, labels and annotations"},{"location":"install/operator/configuring-serving-cr/#override-the-nodeselector","text":"The following KnativeServing resource overrides the webhook deployment to use the disktype: hdd nodeSelector: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : deployments : - name : webhook nodeSelector : disktype : hdd","title":"Override the nodeSelector"},{"location":"install/operator/configuring-serving-cr/#override-the-tolerations","text":"The KnativeServing resource is able to override tolerations for the Knative Serving deployment resources. For example, if you would like to add the following tolerations tolerations : - key : \"key1\" operator : \"Equal\" value : \"value1\" effect : \"NoSchedule\" to the deployment activator , you need to change your KnativeServing CR as below: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : deployments : - name : activator tolerations : - key : \"key1\" operator : \"Equal\" value : \"value1\" effect : \"NoSchedule\"","title":"Override the tolerations"},{"location":"install/operator/configuring-serving-cr/#override-the-affinity","text":"The KnativeServing resource is able to override the affinity, including nodeAffinity, podAffinity, and podAntiAffinity, for the Knative Serving deployment resources. For example, if you would like to add the following nodeAffinity affinity : nodeAffinity : preferredDuringSchedulingIgnoredDuringExecution : - weight : 1 preference : matchExpressions : - key : disktype operator : In values : - ssd to the deployment activator , you need to change your KnativeServing CR as below: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : deployments : - name : activator affinity : nodeAffinity : preferredDuringSchedulingIgnoredDuringExecution : - weight : 1 preference : matchExpressions : - key : disktype operator : In values : - ssd","title":"Override the affinity"},{"location":"install/operator/configuring-with-operator/","text":"Configuring Knative by using the Operator \u00b6 The Operator manages the configuration of a Knative installation, including propagating values from the KnativeServing and KnativeEventing custom resources to system ConfigMaps . Any updates to ConfigMaps which are applied manually are overwritten by the Operator. However, modifying the Knative custom resources allows you to set values for these ConfigMaps. Knative has multiple ConfigMaps that are named with the prefix config- . All Knative ConfigMaps are created in the same namespace as the custom resource that they apply to. For example, if the KnativeServing custom resource is created in the knative-serving namespace, all Knative Serving ConfigMaps are also created in this namespace. The spec.config in the Knative custom resources have one <name> entry for each ConfigMap, named config-<name> , with a value which is be used for the ConfigMap data . Examples \u00b6 You can specify that the KnativeServing custom resource uses the config-domain ConfigMap as follows: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : config : domain : example.org : | selector: app: prod example.com : \"\" You can apply values to multiple ConfigMaps. This example sets stable-window to 60s in the config-autoscaler ConfigMap, as well as specifying the config-domain ConfigMap: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : config : domain : example.org : | selector: app: prod example.com : \"\" autoscaler : stable-window : \"60s\"","title":"\u901a\u8fc7Operator\u914d\u7f6eKnative"},{"location":"install/operator/configuring-with-operator/#configuring-knative-by-using-the-operator","text":"The Operator manages the configuration of a Knative installation, including propagating values from the KnativeServing and KnativeEventing custom resources to system ConfigMaps . Any updates to ConfigMaps which are applied manually are overwritten by the Operator. However, modifying the Knative custom resources allows you to set values for these ConfigMaps. Knative has multiple ConfigMaps that are named with the prefix config- . All Knative ConfigMaps are created in the same namespace as the custom resource that they apply to. For example, if the KnativeServing custom resource is created in the knative-serving namespace, all Knative Serving ConfigMaps are also created in this namespace. The spec.config in the Knative custom resources have one <name> entry for each ConfigMap, named config-<name> , with a value which is be used for the ConfigMap data .","title":"Configuring Knative by using the Operator"},{"location":"install/operator/configuring-with-operator/#examples","text":"You can specify that the KnativeServing custom resource uses the config-domain ConfigMap as follows: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : config : domain : example.org : | selector: app: prod example.com : \"\" You can apply values to multiple ConfigMaps. This example sets stable-window to 60s in the config-autoscaler ConfigMap, as well as specifying the config-domain ConfigMap: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : config : domain : example.org : | selector: app: prod example.com : \"\" autoscaler : stable-window : \"60s\"","title":"Examples"},{"location":"install/operator/knative-with-operators/","text":"Installing Knative using the Operator \u00b6 Knative provides a Kubernetes Operator to install, configure and manage Knative. You can install the Serving component, Eventing component, or both on your cluster. Prerequisites \u00b6 Before installing Knative, you must meet the following prerequisites: For prototyping purposes , Knative works on most local deployments of Kubernetes. For example, you can use a local, one-node cluster that has 2 CPUs and 4 GB of memory. Tip You can install a local distribution of Knative for development purposes using the Knative Quickstart plugin For production purposes , it is recommended that: If you have only one node in your cluster, you need at least 6 CPUs, 6 GB of memory, and 30 GB of disk storage. If you have multiple nodes in your cluster, for each node you need at least 2 CPUs, 4 GB of memory, and 20 GB of disk storage. You have a cluster that uses Kubernetes v1.22 or newer. You have installed the kubectl CLI . Your Kubernetes cluster must have access to the internet, because Kubernetes needs to be able to fetch images. To pull from a private registry, see Deploying images from a private container registry . Caution The system requirements provided are recommendations only. The requirements for your installation might vary, depending on whether you use optional components, such as a networking layer. Install the Knative Operator \u00b6 Before you install the Knative Serving and Eventing components, first install the Knative Operator. Install the latest Knative Operator release \u00b6 To install the latest stable Operator release, run the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/operator/latest/operator.yaml You can find information about the released versions of the Knative Operator on the releases page . Verify your Knative Operator installation \u00b6 Because the Operator is installed to the default namespace, ensure you set the current namespace to default by running the command: kubectl config set-context --current --namespace = default Check the Operator deployment status by running the command: kubectl get deployment knative-operator If the Operator is installed correctly, the deployment shows a Ready status: NAME READY UP-TO-DATE AVAILABLE AGE knative-operator 1 /1 1 1 19h Track the log \u00b6 To track the log of the Operator, run the command: kubectl logs -f deploy/knative-operator Installing the Knative Serving component \u00b6 To install Knative Serving you must create a custom resource (CR), add a networking layer to the CR, and configure DNS. Create the Knative Serving custom resource \u00b6 Install the current version (default) Install another version of Knative Serving Install customized Knative Serving To create the custom resource for the latest available Knative Serving in the Operator: Copy the following YAML into a file: apiVersion : v1 kind : Namespace metadata : name : knative-serving --- apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving Note When you don't specify a version by using spec.version field, the Operator defaults to the latest available version. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. You can install a new release of Knative Serving without upgrading the Operator. To install a new version of Knative Serving: Create a YAML file containing the following: apiVersion : v1 kind : Namespace metadata : name : knative-serving --- apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : version : \"<new-version>\" manifests : - URL : https://github.com/knative/serving/releases/download/v${VERSION}/serving-core.yaml - URL : https://github.com/knative/serving/releases/download/v${VERSION}/serving-hpa.yaml - URL : https://github.com/knative/serving/releases/download/v${VERSION}/serving-post-install-jobs.yaml - URL : https://github.com/knative/net-istio/releases/download/v${VERSION}/net-istio.yaml Where <new-version> is the Knative version you want to use, for example 1.0 . This field is used to set the version of Knative Serving and to automatically replace the tag ${VERSION} . Attention The field spec.manifests is used to specify one or multiple URL links of the Knative Serving component. The ordering of the URLs is critical. Put the manifest you want to apply first on the top. You must add a valid URL of the Knative network ingress plugin. You can use net-istio . Knative Serving component is tightly-coupled with a network ingress plugin in the Operator. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. The Operator provides you with the flexibility to install Knative Serving customized to your own requirements. As long as the manifests of customized Knative Serving are accessible to the Operator, you can install them. There are two modes available for you to install customized manifests: overwrite mode and append mode . With overwrite mode, you must define all manifests needed for Knative Serving to install because the Operator will no longer install any default manifests. With append mode, you only need to define your customized manifests. The customized manifests are installed after default manifests are applied. Overwrite mode: You can use overwrite mode when you want to customize all Knative Serving manifests. For example, if you want to install Knative Serving and istio ingress and you want customize both components, you can create the following YAML file: apiVersion : v1 kind : Namespace metadata : name : knative-serving --- apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : version : $spec_version manifests : - URL : https://my-serving/serving.yaml - URL : https://my-net-istio/net-istio.yaml This example installs the customized Knative Serving at version $spec_version which is available at https://my-serving/serving.yaml , and the customized ingress plugin net-istio which is available at https://my-net-istio/net-istio.yaml . Attention You can make the customized Knative Serving available in one or multiple links, as the spec.manifests supports a list of links. The ordering of the URLs is critical. Put the manifest you want to apply first on the top. We strongly recommend you to specify the version and the valid links to the customized Knative Serving, by leveraging both spec_version and spec.manifests . Do not skip either field. Append mode: You can use append mode to add your customized manifests into the default manifests. For example, if you only want to customize a few resources but you still want to install the default Knative Serving, you can create the following YAML file: apiVersion : v1 kind : Namespace metadata : name : knative-serving --- apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : version : $spec_version additionalManifests : - URL : https://my-serving/serving-custom.yaml This example installs the default Knative Serving, and installs your customized resources available at https://my-serving/serving-custom.yaml . Knative Operator installs the default manifests of Knative Serving at the version $spec_version , and then installs your customized manifests based on them. Install the networking layer \u00b6 Knative Operator can configure the Knative Serving component with different network layer options. Istio is the default network layer if the ingress is not specified in the Knative Serving CR. If you choose to use the default Istio network layer, you must install Istio on your cluster. Because of this, you might find it easier to configure Kourier as your networking layer. Click on each of the following tabs to see how you can configure Knative Serving with different ingresses: Kourier (Choose this if you are not sure) Istio (default) Contour The following steps install Kourier and enable its Knative integration: To configure Knative Serving to use Kourier, add spec.ingress.kourier and spec.config.network to your Serving CR YAML file as follows: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : # ... ingress : kourier : enabled : true config : network : ingress-class : \"kourier.ingress.networking.knative.dev\" Apply the YAML file for your Serving CR by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of your Serving CR file. Fetch the External IP or CNAME by running the command: kubectl --namespace knative-serving get service kourier Save this for configuring DNS later. The following steps install Istio to enable its Knative integration: Install Istio . If you installed Istio under a namespace other than the default istio-system : Add spec.config.istio to your Serving CR YAML file as follows: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : # ... config : istio : local-gateway.<local-gateway-namespace>.knative-local-gateway : \"knative-local-gateway.<istio-namespace>.svc.cluster.local\" Where: <local-gateway-namespace> is the local gateway namespace, which is the same as Knative Serving namespace knative-serving . <istio-namespace> is the namespace where Istio is installed. Apply the YAML file for your Serving CR by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of your Serving CR file. Fetch the External IP or CNAME by running the command: kubectl get svc istio-ingressgateway -n <istio-namespace> Save this for configuring DNS later. The following steps install Contour and enable its Knative integration: Install a properly configured Contour: kubectl apply --filename https://storage.googleapis.com/knative-nightly/net-contour/latest/contour.yaml To configure Knative Serving to use Contour, add spec.ingress.contour spec.config.network to your Serving CR YAML file as follows: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : # ... ingress : contour : enabled : true config : network : ingress-class : \"contour.ingress.networking.knative.dev\" Apply the YAML file for your Serving CR by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of your Serving CR file. Fetch the External IP or CNAME by running the command: kubectl --namespace contour-external get service envoy Save this for configuring DNS later. Verify the Knative Serving deployment \u00b6 Monitor the Knative deployments: kubectl get deployment -n knative-serving If Knative Serving has been successfully deployed, all deployments of the Knative Serving will show READY status. Here is a sample output: NAME READY UP-TO-DATE AVAILABLE AGE activator 1 /1 1 1 18s autoscaler 1 /1 1 1 18s autoscaler-hpa 1 /1 1 1 14s controller 1 /1 1 1 18s domain-mapping 1 /1 1 1 12s domainmapping-webhook 1 /1 1 1 12s webhook 1 /1 1 1 17s Check the status of Knative Serving Custom Resource: kubectl get KnativeServing knative-serving -n knative-serving If Knative Serving is successfully installed, you should see: NAME VERSION READY REASON knative-serving <version number> True Configure DNS \u00b6 You can configure DNS to prevent the need to run curl commands with a host header. The following tabs expand to show instructions for configuring DNS. Follow the procedure for the DNS of your choice: Magic DNS (sslip.io) Real DNS Temporary DNS Knative provides a Kubernetes Job called default-domain that configures Knative Serving to use sslip.io as the default DNS suffix. kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-default-domain.yaml Warning This will only work if the cluster LoadBalancer Service exposes an IPv4 address or hostname, so it will not work with IPv6 clusters or local setups like minikube unless minikube tunnel is running. In these cases, see the \"Real DNS\" or \"Temporary DNS\" tabs. To configure DNS for Knative, take the External IP or CNAME from setting up networking, and configure it with your DNS provider as follows: If the networking layer produced an External IP address, then configure a wildcard A record for the domain: # Here knative.example.com is the domain suffix for your cluster *.knative.example.com == A 35.233.41.212 If the networking layer produced a CNAME, then configure a CNAME record for the domain: # Here knative.example.com is the domain suffix for your cluster *.knative.example.com == CNAME a317a278525d111e89f272a164fd35fb-1510370581.eu-central-1.elb.amazonaws.com Once your DNS provider has been configured, add spec.config.domain into your existing Serving CR, and apply it: # Replace knative.example.com with your domain suffix apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : ... config : domain : \"knative.example.com\" : \"\" ... If you are using curl to access the sample applications, or your own Knative app, and are unable to use the \"Magic DNS (sslip.io)\" or \"Real DNS\" methods, there is a temporary approach. This is useful for those who wish to evaluate Knative without altering their DNS configuration, as per the \"Real DNS\" method, or cannot use the \"Magic DNS\" method due to using, for example, minikube locally or IPv6 clusters. To access your application using curl using this method: After starting your application, get the URL of your application: kubectl get ksvc The output should be similar to: NAME URL LATESTCREATED LATESTREADY READY REASON helloworld-go http://helloworld-go.default.example.com helloworld-go-vqjlf helloworld-go-vqjlf True Instruct curl to connect to the External IP or CNAME defined by the networking layer mentioned in section 3, and use the -H \"Host:\" command-line option to specify the Knative application's host name. For example, if the networking layer defines your External IP and port to be http://192.168.39.228:32198 and you wish to access the helloworld-go application mentioned earlier, use: curl -H \"Host: helloworld-go.default.example.com\" http://192.168.39.228:32198 In the case of the provided helloworld-go sample application, using the default configuration, the output is: Hello Go Sample v1! Refer to the \"Real DNS\" method for a permanent solution. Installing the Knative Eventing component \u00b6 To install Knative Eventing you must apply the custom resource (CR). Optionally, you can install the Knative Eventing component with different event sources. Create the Knative Eventing custom resource \u00b6 Install the current version (default) Install another version of Knative Eventing Install customized Knative Eventing To create the custom resource for the latest available Knative Eventing in the Operator: Copy the following YAML into a file: apiVersion : v1 kind : Namespace metadata : name : knative-eventing --- apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing Note When you do not specify a version by using spec.version field, the Operator defaults to the latest available version. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. You can install a new release of Knative Eventing without upgrading the Operator. To install a new version of Knative Eventing: Create a YAML file containing the following: apiVersion : v1 kind : Namespace metadata : name : knative-eventing --- apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : version : \"<new-version>\" manifests : - URL : https://github.com/knative/eventing/releases/download/v${VERSION}/eventing.yaml - URL : https://github.com/knative/eventing/releases/download/v${VERSION}/eventing-post-install-jobs.yaml Where <new-version> is the Knative version you want to use, for example 1.0 . This field is used to set the version of Knative Eventing and to automatically replace the tag ${VERSION} . Attention The field spec.manifests is used to specify one or multiple URL links of the Knative Serving component. The ordering of the URLs is critical. Put the manifest you want to apply first on the top. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. The Operator provides you with the flexibility to install Knative Eventing customized to your own requirements. As long as the manifests of customized Knative Eventing are accessible to the Operator, you can install them. There are two modes available for you to install customized manifests: overwrite mode and append mode . With overwrite mode, you must define all manifests needed for Knative Eventing to install because the Operator will no longer install any default manifests. With append mode, you only need to define your customized manifests. The customized manifests are installed after default manifests are applied. Overwrite mode: Use overwrite mode when you want to customize all Knative Eventing manifests to be installed. For example, if you want to install a customized Knative Eventing only, you can create and apply the following Eventing CR: apiVersion : v1 kind : Namespace metadata : name : knative-eventing --- apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : version : $spec_version manifests : - URL : https://my-eventing/eventing.yaml This example installs the customized Knative Eventing at version $spec_version which is available at https://my-eventing/eventing.yaml . Attention You can make the customized Knative Eventing available in one or multiple links, as the spec.manifests supports a list of links. The ordering of the URLs is critical. Put the manifest you want to apply first on the top. We strongly recommend you to specify the version and the valid links to the customized Knative Eventing, by leveraging both spec.version and spec.manifests . Do not skip either field. Append mode: You can use append mode to add your customized manifests into the default manifests. For example, if you only want to customize a few resources but you still want to install the default Knative Eventing, you can create and apply the following Eventing CR: apiVersion : v1 kind : Namespace metadata : name : knative-eventing --- apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : version : $spec_version additionalManifests : - URL : https://my-eventing/eventing-custom.yaml This example installs the default Knative Eventing, and installs your customized resources available at https://my-eventing/eventing-custom.yaml . Knative Operator installs the default manifests of Knative Eventing at the version $spec_version , and then installs your customized manifests based on them. Installing Knative Eventing with event sources \u00b6 Knative Operator can configure the Knative Eventing component with different event sources. Click on each of the following tabs to see how you can configure Knative Eventing with different event sources: Ceph Apache CouchDB GitHub GitLab Apache Kafka NATS Streaming Prometheus RabbitMQ Redis To configure Knative Eventing to install Ceph as the event source: Add spec.source.ceph to your Eventing CR YAML file as follows: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : # ... source : ceph : enabled : true Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To configure Knative Eventing to install Apache CouchDB as the event source: Add spec.source.couchdb to your Eventing CR YAML file as follows: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : # ... source : couchdb : enabled : true Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To configure Knative Eventing to install GitHub as the event source: Add spec.source.github to your Eventing CR YAML file as follows: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : # ... source : github : enabled : true Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To configure Knative Eventing to install GitLab as the event source: Add spec.source.gitlab to your Eventing CR YAML file as follows: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : # ... source : gitlab : enabled : true Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To configure Knative Eventing to install Kafka as the event source: Add spec.source.kafka to your Eventing CR YAML file as follows: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : # ... source : kafka : enabled : true Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To configure Knative Eventing to install NATS Streaming as the event source: Add spec.source.natss to your Eventing CR YAML file as follows: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : # ... source : natss : enabled : true Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To configure Knative Eventing to install Prometheus as the event source: Add spec.source.prometheus to your Eventing CR YAML file as follows: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : # ... source : prometheus : enabled : true Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To configure Knative Eventing to install RabbitMQ as the event source, Add spec.source.rabbitmq to your Eventing CR YAML file as follows: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : # ... source : rabbitmq : enabled : true Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To configure Knative Eventing to install Redis as the event source: Add spec.source.redis to your Eventing CR YAML file as follows: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : # ... source : redis : enabled : true Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Verify the Knative Eventing deployment \u00b6 Monitor the Knative deployments: kubectl get deployment -n knative-eventing If Knative Eventing has been successfully deployed, all deployments of the Knative Eventing will show READY status. Here is a sample output: NAME READY UP-TO-DATE AVAILABLE AGE eventing-controller 1 /1 1 1 43s eventing-webhook 1 /1 1 1 42s imc-controller 1 /1 1 1 39s imc-dispatcher 1 /1 1 1 38s mt-broker-controller 1 /1 1 1 36s mt-broker-filter 1 /1 1 1 37s mt-broker-ingress 1 /1 1 1 37s pingsource-mt-adapter 0 /0 0 0 43s sugar-controller 1 /1 1 1 36s Check the status of Knative Eventing Custom Resource: kubectl get KnativeEventing knative-eventing -n knative-eventing If Knative Eventing is successfully installed, you should see: NAME VERSION READY REASON knative-eventing <version number> True Uninstalling Knative \u00b6 Knative Operator prevents unsafe removal of Knative resources. Even if the Knative Serving and Knative Eventing CRs are successfully removed, all the CRDs in Knative are still kept in the cluster. All your resources relying on Knative CRDs can still work. Removing the Knative Serving component \u00b6 To remove the Knative Serving CR run the command: kubectl delete KnativeServing knative-serving -n knative-serving Removing Knative Eventing component \u00b6 To remove the Knative Eventing CR run the command: kubectl delete KnativeEventing knative-eventing -n knative-eventing Removing the Knative Operator: \u00b6 If you have installed Knative using the release page, remove the operator by running the command: kubectl delete -f https://storage.googleapis.com/knative-nightly/operator/latest/operator.yaml If you have installed Knative from source, uninstall it using the following command while in the root directory for the source: ko delete -f config/ What's next \u00b6 Configure Knative Serving using Operator Configure Knative Eventing using Operator","title":"\u901a\u8fc7Operator\u5b89\u88c5Knative"},{"location":"install/operator/knative-with-operators/#installing-knative-using-the-operator","text":"Knative provides a Kubernetes Operator to install, configure and manage Knative. You can install the Serving component, Eventing component, or both on your cluster.","title":"Installing Knative using the Operator"},{"location":"install/operator/knative-with-operators/#prerequisites","text":"Before installing Knative, you must meet the following prerequisites: For prototyping purposes , Knative works on most local deployments of Kubernetes. For example, you can use a local, one-node cluster that has 2 CPUs and 4 GB of memory. Tip You can install a local distribution of Knative for development purposes using the Knative Quickstart plugin For production purposes , it is recommended that: If you have only one node in your cluster, you need at least 6 CPUs, 6 GB of memory, and 30 GB of disk storage. If you have multiple nodes in your cluster, for each node you need at least 2 CPUs, 4 GB of memory, and 20 GB of disk storage. You have a cluster that uses Kubernetes v1.22 or newer. You have installed the kubectl CLI . Your Kubernetes cluster must have access to the internet, because Kubernetes needs to be able to fetch images. To pull from a private registry, see Deploying images from a private container registry . Caution The system requirements provided are recommendations only. The requirements for your installation might vary, depending on whether you use optional components, such as a networking layer.","title":"Prerequisites"},{"location":"install/operator/knative-with-operators/#install-the-knative-operator","text":"Before you install the Knative Serving and Eventing components, first install the Knative Operator.","title":"Install the Knative Operator"},{"location":"install/operator/knative-with-operators/#install-the-latest-knative-operator-release","text":"To install the latest stable Operator release, run the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/operator/latest/operator.yaml You can find information about the released versions of the Knative Operator on the releases page .","title":"Install the latest Knative Operator release"},{"location":"install/operator/knative-with-operators/#verify-your-knative-operator-installation","text":"Because the Operator is installed to the default namespace, ensure you set the current namespace to default by running the command: kubectl config set-context --current --namespace = default Check the Operator deployment status by running the command: kubectl get deployment knative-operator If the Operator is installed correctly, the deployment shows a Ready status: NAME READY UP-TO-DATE AVAILABLE AGE knative-operator 1 /1 1 1 19h","title":"Verify your Knative Operator installation"},{"location":"install/operator/knative-with-operators/#track-the-log","text":"To track the log of the Operator, run the command: kubectl logs -f deploy/knative-operator","title":"Track the log"},{"location":"install/operator/knative-with-operators/#installing-the-knative-serving-component","text":"To install Knative Serving you must create a custom resource (CR), add a networking layer to the CR, and configure DNS.","title":"Installing the Knative Serving component"},{"location":"install/operator/knative-with-operators/#create-the-knative-serving-custom-resource","text":"Install the current version (default) Install another version of Knative Serving Install customized Knative Serving To create the custom resource for the latest available Knative Serving in the Operator: Copy the following YAML into a file: apiVersion : v1 kind : Namespace metadata : name : knative-serving --- apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving Note When you don't specify a version by using spec.version field, the Operator defaults to the latest available version. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. You can install a new release of Knative Serving without upgrading the Operator. To install a new version of Knative Serving: Create a YAML file containing the following: apiVersion : v1 kind : Namespace metadata : name : knative-serving --- apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : version : \"<new-version>\" manifests : - URL : https://github.com/knative/serving/releases/download/v${VERSION}/serving-core.yaml - URL : https://github.com/knative/serving/releases/download/v${VERSION}/serving-hpa.yaml - URL : https://github.com/knative/serving/releases/download/v${VERSION}/serving-post-install-jobs.yaml - URL : https://github.com/knative/net-istio/releases/download/v${VERSION}/net-istio.yaml Where <new-version> is the Knative version you want to use, for example 1.0 . This field is used to set the version of Knative Serving and to automatically replace the tag ${VERSION} . Attention The field spec.manifests is used to specify one or multiple URL links of the Knative Serving component. The ordering of the URLs is critical. Put the manifest you want to apply first on the top. You must add a valid URL of the Knative network ingress plugin. You can use net-istio . Knative Serving component is tightly-coupled with a network ingress plugin in the Operator. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. The Operator provides you with the flexibility to install Knative Serving customized to your own requirements. As long as the manifests of customized Knative Serving are accessible to the Operator, you can install them. There are two modes available for you to install customized manifests: overwrite mode and append mode . With overwrite mode, you must define all manifests needed for Knative Serving to install because the Operator will no longer install any default manifests. With append mode, you only need to define your customized manifests. The customized manifests are installed after default manifests are applied. Overwrite mode: You can use overwrite mode when you want to customize all Knative Serving manifests. For example, if you want to install Knative Serving and istio ingress and you want customize both components, you can create the following YAML file: apiVersion : v1 kind : Namespace metadata : name : knative-serving --- apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : version : $spec_version manifests : - URL : https://my-serving/serving.yaml - URL : https://my-net-istio/net-istio.yaml This example installs the customized Knative Serving at version $spec_version which is available at https://my-serving/serving.yaml , and the customized ingress plugin net-istio which is available at https://my-net-istio/net-istio.yaml . Attention You can make the customized Knative Serving available in one or multiple links, as the spec.manifests supports a list of links. The ordering of the URLs is critical. Put the manifest you want to apply first on the top. We strongly recommend you to specify the version and the valid links to the customized Knative Serving, by leveraging both spec_version and spec.manifests . Do not skip either field. Append mode: You can use append mode to add your customized manifests into the default manifests. For example, if you only want to customize a few resources but you still want to install the default Knative Serving, you can create the following YAML file: apiVersion : v1 kind : Namespace metadata : name : knative-serving --- apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : version : $spec_version additionalManifests : - URL : https://my-serving/serving-custom.yaml This example installs the default Knative Serving, and installs your customized resources available at https://my-serving/serving-custom.yaml . Knative Operator installs the default manifests of Knative Serving at the version $spec_version , and then installs your customized manifests based on them.","title":"Create the Knative Serving custom resource"},{"location":"install/operator/knative-with-operators/#install-the-networking-layer","text":"Knative Operator can configure the Knative Serving component with different network layer options. Istio is the default network layer if the ingress is not specified in the Knative Serving CR. If you choose to use the default Istio network layer, you must install Istio on your cluster. Because of this, you might find it easier to configure Kourier as your networking layer. Click on each of the following tabs to see how you can configure Knative Serving with different ingresses: Kourier (Choose this if you are not sure) Istio (default) Contour The following steps install Kourier and enable its Knative integration: To configure Knative Serving to use Kourier, add spec.ingress.kourier and spec.config.network to your Serving CR YAML file as follows: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : # ... ingress : kourier : enabled : true config : network : ingress-class : \"kourier.ingress.networking.knative.dev\" Apply the YAML file for your Serving CR by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of your Serving CR file. Fetch the External IP or CNAME by running the command: kubectl --namespace knative-serving get service kourier Save this for configuring DNS later. The following steps install Istio to enable its Knative integration: Install Istio . If you installed Istio under a namespace other than the default istio-system : Add spec.config.istio to your Serving CR YAML file as follows: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : # ... config : istio : local-gateway.<local-gateway-namespace>.knative-local-gateway : \"knative-local-gateway.<istio-namespace>.svc.cluster.local\" Where: <local-gateway-namespace> is the local gateway namespace, which is the same as Knative Serving namespace knative-serving . <istio-namespace> is the namespace where Istio is installed. Apply the YAML file for your Serving CR by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of your Serving CR file. Fetch the External IP or CNAME by running the command: kubectl get svc istio-ingressgateway -n <istio-namespace> Save this for configuring DNS later. The following steps install Contour and enable its Knative integration: Install a properly configured Contour: kubectl apply --filename https://storage.googleapis.com/knative-nightly/net-contour/latest/contour.yaml To configure Knative Serving to use Contour, add spec.ingress.contour spec.config.network to your Serving CR YAML file as follows: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : # ... ingress : contour : enabled : true config : network : ingress-class : \"contour.ingress.networking.knative.dev\" Apply the YAML file for your Serving CR by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of your Serving CR file. Fetch the External IP or CNAME by running the command: kubectl --namespace contour-external get service envoy Save this for configuring DNS later.","title":"Install the networking layer"},{"location":"install/operator/knative-with-operators/#verify-the-knative-serving-deployment","text":"Monitor the Knative deployments: kubectl get deployment -n knative-serving If Knative Serving has been successfully deployed, all deployments of the Knative Serving will show READY status. Here is a sample output: NAME READY UP-TO-DATE AVAILABLE AGE activator 1 /1 1 1 18s autoscaler 1 /1 1 1 18s autoscaler-hpa 1 /1 1 1 14s controller 1 /1 1 1 18s domain-mapping 1 /1 1 1 12s domainmapping-webhook 1 /1 1 1 12s webhook 1 /1 1 1 17s Check the status of Knative Serving Custom Resource: kubectl get KnativeServing knative-serving -n knative-serving If Knative Serving is successfully installed, you should see: NAME VERSION READY REASON knative-serving <version number> True","title":"Verify the Knative Serving deployment"},{"location":"install/operator/knative-with-operators/#configure-dns","text":"You can configure DNS to prevent the need to run curl commands with a host header. The following tabs expand to show instructions for configuring DNS. Follow the procedure for the DNS of your choice: Magic DNS (sslip.io) Real DNS Temporary DNS Knative provides a Kubernetes Job called default-domain that configures Knative Serving to use sslip.io as the default DNS suffix. kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-default-domain.yaml Warning This will only work if the cluster LoadBalancer Service exposes an IPv4 address or hostname, so it will not work with IPv6 clusters or local setups like minikube unless minikube tunnel is running. In these cases, see the \"Real DNS\" or \"Temporary DNS\" tabs. To configure DNS for Knative, take the External IP or CNAME from setting up networking, and configure it with your DNS provider as follows: If the networking layer produced an External IP address, then configure a wildcard A record for the domain: # Here knative.example.com is the domain suffix for your cluster *.knative.example.com == A 35.233.41.212 If the networking layer produced a CNAME, then configure a CNAME record for the domain: # Here knative.example.com is the domain suffix for your cluster *.knative.example.com == CNAME a317a278525d111e89f272a164fd35fb-1510370581.eu-central-1.elb.amazonaws.com Once your DNS provider has been configured, add spec.config.domain into your existing Serving CR, and apply it: # Replace knative.example.com with your domain suffix apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : ... config : domain : \"knative.example.com\" : \"\" ... If you are using curl to access the sample applications, or your own Knative app, and are unable to use the \"Magic DNS (sslip.io)\" or \"Real DNS\" methods, there is a temporary approach. This is useful for those who wish to evaluate Knative without altering their DNS configuration, as per the \"Real DNS\" method, or cannot use the \"Magic DNS\" method due to using, for example, minikube locally or IPv6 clusters. To access your application using curl using this method: After starting your application, get the URL of your application: kubectl get ksvc The output should be similar to: NAME URL LATESTCREATED LATESTREADY READY REASON helloworld-go http://helloworld-go.default.example.com helloworld-go-vqjlf helloworld-go-vqjlf True Instruct curl to connect to the External IP or CNAME defined by the networking layer mentioned in section 3, and use the -H \"Host:\" command-line option to specify the Knative application's host name. For example, if the networking layer defines your External IP and port to be http://192.168.39.228:32198 and you wish to access the helloworld-go application mentioned earlier, use: curl -H \"Host: helloworld-go.default.example.com\" http://192.168.39.228:32198 In the case of the provided helloworld-go sample application, using the default configuration, the output is: Hello Go Sample v1! Refer to the \"Real DNS\" method for a permanent solution.","title":"Configure DNS"},{"location":"install/operator/knative-with-operators/#installing-the-knative-eventing-component","text":"To install Knative Eventing you must apply the custom resource (CR). Optionally, you can install the Knative Eventing component with different event sources.","title":"Installing the Knative Eventing component"},{"location":"install/operator/knative-with-operators/#create-the-knative-eventing-custom-resource","text":"Install the current version (default) Install another version of Knative Eventing Install customized Knative Eventing To create the custom resource for the latest available Knative Eventing in the Operator: Copy the following YAML into a file: apiVersion : v1 kind : Namespace metadata : name : knative-eventing --- apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing Note When you do not specify a version by using spec.version field, the Operator defaults to the latest available version. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. You can install a new release of Knative Eventing without upgrading the Operator. To install a new version of Knative Eventing: Create a YAML file containing the following: apiVersion : v1 kind : Namespace metadata : name : knative-eventing --- apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : version : \"<new-version>\" manifests : - URL : https://github.com/knative/eventing/releases/download/v${VERSION}/eventing.yaml - URL : https://github.com/knative/eventing/releases/download/v${VERSION}/eventing-post-install-jobs.yaml Where <new-version> is the Knative version you want to use, for example 1.0 . This field is used to set the version of Knative Eventing and to automatically replace the tag ${VERSION} . Attention The field spec.manifests is used to specify one or multiple URL links of the Knative Serving component. The ordering of the URLs is critical. Put the manifest you want to apply first on the top. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. The Operator provides you with the flexibility to install Knative Eventing customized to your own requirements. As long as the manifests of customized Knative Eventing are accessible to the Operator, you can install them. There are two modes available for you to install customized manifests: overwrite mode and append mode . With overwrite mode, you must define all manifests needed for Knative Eventing to install because the Operator will no longer install any default manifests. With append mode, you only need to define your customized manifests. The customized manifests are installed after default manifests are applied. Overwrite mode: Use overwrite mode when you want to customize all Knative Eventing manifests to be installed. For example, if you want to install a customized Knative Eventing only, you can create and apply the following Eventing CR: apiVersion : v1 kind : Namespace metadata : name : knative-eventing --- apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : version : $spec_version manifests : - URL : https://my-eventing/eventing.yaml This example installs the customized Knative Eventing at version $spec_version which is available at https://my-eventing/eventing.yaml . Attention You can make the customized Knative Eventing available in one or multiple links, as the spec.manifests supports a list of links. The ordering of the URLs is critical. Put the manifest you want to apply first on the top. We strongly recommend you to specify the version and the valid links to the customized Knative Eventing, by leveraging both spec.version and spec.manifests . Do not skip either field. Append mode: You can use append mode to add your customized manifests into the default manifests. For example, if you only want to customize a few resources but you still want to install the default Knative Eventing, you can create and apply the following Eventing CR: apiVersion : v1 kind : Namespace metadata : name : knative-eventing --- apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : version : $spec_version additionalManifests : - URL : https://my-eventing/eventing-custom.yaml This example installs the default Knative Eventing, and installs your customized resources available at https://my-eventing/eventing-custom.yaml . Knative Operator installs the default manifests of Knative Eventing at the version $spec_version , and then installs your customized manifests based on them.","title":"Create the Knative Eventing custom resource"},{"location":"install/operator/knative-with-operators/#installing-knative-eventing-with-event-sources","text":"Knative Operator can configure the Knative Eventing component with different event sources. Click on each of the following tabs to see how you can configure Knative Eventing with different event sources: Ceph Apache CouchDB GitHub GitLab Apache Kafka NATS Streaming Prometheus RabbitMQ Redis To configure Knative Eventing to install Ceph as the event source: Add spec.source.ceph to your Eventing CR YAML file as follows: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : # ... source : ceph : enabled : true Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To configure Knative Eventing to install Apache CouchDB as the event source: Add spec.source.couchdb to your Eventing CR YAML file as follows: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : # ... source : couchdb : enabled : true Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To configure Knative Eventing to install GitHub as the event source: Add spec.source.github to your Eventing CR YAML file as follows: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : # ... source : github : enabled : true Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To configure Knative Eventing to install GitLab as the event source: Add spec.source.gitlab to your Eventing CR YAML file as follows: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : # ... source : gitlab : enabled : true Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To configure Knative Eventing to install Kafka as the event source: Add spec.source.kafka to your Eventing CR YAML file as follows: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : # ... source : kafka : enabled : true Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To configure Knative Eventing to install NATS Streaming as the event source: Add spec.source.natss to your Eventing CR YAML file as follows: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : # ... source : natss : enabled : true Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To configure Knative Eventing to install Prometheus as the event source: Add spec.source.prometheus to your Eventing CR YAML file as follows: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : # ... source : prometheus : enabled : true Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To configure Knative Eventing to install RabbitMQ as the event source, Add spec.source.rabbitmq to your Eventing CR YAML file as follows: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : # ... source : rabbitmq : enabled : true Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To configure Knative Eventing to install Redis as the event source: Add spec.source.redis to your Eventing CR YAML file as follows: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : # ... source : redis : enabled : true Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"Installing Knative Eventing with event sources"},{"location":"install/operator/knative-with-operators/#verify-the-knative-eventing-deployment","text":"Monitor the Knative deployments: kubectl get deployment -n knative-eventing If Knative Eventing has been successfully deployed, all deployments of the Knative Eventing will show READY status. Here is a sample output: NAME READY UP-TO-DATE AVAILABLE AGE eventing-controller 1 /1 1 1 43s eventing-webhook 1 /1 1 1 42s imc-controller 1 /1 1 1 39s imc-dispatcher 1 /1 1 1 38s mt-broker-controller 1 /1 1 1 36s mt-broker-filter 1 /1 1 1 37s mt-broker-ingress 1 /1 1 1 37s pingsource-mt-adapter 0 /0 0 0 43s sugar-controller 1 /1 1 1 36s Check the status of Knative Eventing Custom Resource: kubectl get KnativeEventing knative-eventing -n knative-eventing If Knative Eventing is successfully installed, you should see: NAME VERSION READY REASON knative-eventing <version number> True","title":"Verify the Knative Eventing deployment"},{"location":"install/operator/knative-with-operators/#uninstalling-knative","text":"Knative Operator prevents unsafe removal of Knative resources. Even if the Knative Serving and Knative Eventing CRs are successfully removed, all the CRDs in Knative are still kept in the cluster. All your resources relying on Knative CRDs can still work.","title":"Uninstalling Knative"},{"location":"install/operator/knative-with-operators/#removing-the-knative-serving-component","text":"To remove the Knative Serving CR run the command: kubectl delete KnativeServing knative-serving -n knative-serving","title":"Removing the Knative Serving component"},{"location":"install/operator/knative-with-operators/#removing-knative-eventing-component","text":"To remove the Knative Eventing CR run the command: kubectl delete KnativeEventing knative-eventing -n knative-eventing","title":"Removing Knative Eventing component"},{"location":"install/operator/knative-with-operators/#removing-the-knative-operator","text":"If you have installed Knative using the release page, remove the operator by running the command: kubectl delete -f https://storage.googleapis.com/knative-nightly/operator/latest/operator.yaml If you have installed Knative from source, uninstall it using the following command while in the root directory for the source: ko delete -f config/","title":"Removing the Knative Operator:"},{"location":"install/operator/knative-with-operators/#whats-next","text":"Configure Knative Serving using Operator Configure Knative Eventing using Operator","title":"What's next"},{"location":"install/upgrade/","text":"Upgrading Knative \u00b6 Knative supports upgrading by a single minor version number. For example, if you have v0.21.0 installed, you must upgrade to v0.22.0 before attempting to upgrade to v0.23.0. To verify your current version, see Checking your Knative version . To upgrade Knative: If you installed Knative using YAML, see Upgrading with kubectl . If you installed Knative using the Knative Operator, see Upgrading using the Knative Operator .","title":"\u5173\u4e8e\u66f4\u65b0Knative"},{"location":"install/upgrade/#upgrading-knative","text":"Knative supports upgrading by a single minor version number. For example, if you have v0.21.0 installed, you must upgrade to v0.22.0 before attempting to upgrade to v0.23.0. To verify your current version, see Checking your Knative version . To upgrade Knative: If you installed Knative using YAML, see Upgrading with kubectl . If you installed Knative using the Knative Operator, see Upgrading using the Knative Operator .","title":"Upgrading Knative"},{"location":"install/upgrade/check-install-version/","text":"Checking your Knative version \u00b6 To check the version of your Knative installation, use one of the following commands, depending on whether you installed Knative with YAML or with the Operator. If you installed with YAML \u00b6 To verify the version of the Knative component that you have running on your cluster, query for the <component>.knative.dev/release label. Knative Serving Knative Eventing Check the installed Knative Serving version by running the command: kubectl get namespace knative-serving -o 'go-template={{index .metadata.labels \"serving.knative.dev/release\"}}' Example output: v0.23.0 Check the installed Knative Eventing version by running the command: kubectl get namespace knative-eventing -o 'go-template={{index .metadata.labels \"eventing.knative.dev/release\"}}' Example output: v0.23.0 If you installed with the Operator \u00b6 To verify the version of your current Knative installation: Knative Serving Knative Eventing Check the installed Knative Serving version by running the command: kubectl get KnativeServing knative-serving --namespace knative-serving Example output: NAME VERSION READY REASON knative-serving 0 .23.0 True Check the installed Knative Eventing version by running the command: kubectl get KnativeEventing knative-eventing --namespace knative-eventing Example output: NAME VERSION READY REASON knative-eventing 0 .23.0 True","title":"\u68c0\u67e5Knative\u7248\u672c"},{"location":"install/upgrade/check-install-version/#checking-your-knative-version","text":"To check the version of your Knative installation, use one of the following commands, depending on whether you installed Knative with YAML or with the Operator.","title":"Checking your Knative version"},{"location":"install/upgrade/check-install-version/#if-you-installed-with-yaml","text":"To verify the version of the Knative component that you have running on your cluster, query for the <component>.knative.dev/release label. Knative Serving Knative Eventing Check the installed Knative Serving version by running the command: kubectl get namespace knative-serving -o 'go-template={{index .metadata.labels \"serving.knative.dev/release\"}}' Example output: v0.23.0 Check the installed Knative Eventing version by running the command: kubectl get namespace knative-eventing -o 'go-template={{index .metadata.labels \"eventing.knative.dev/release\"}}' Example output: v0.23.0","title":"If you installed with YAML"},{"location":"install/upgrade/check-install-version/#if-you-installed-with-the-operator","text":"To verify the version of your current Knative installation: Knative Serving Knative Eventing Check the installed Knative Serving version by running the command: kubectl get KnativeServing knative-serving --namespace knative-serving Example output: NAME VERSION READY REASON knative-serving 0 .23.0 True Check the installed Knative Eventing version by running the command: kubectl get KnativeEventing knative-eventing --namespace knative-eventing Example output: NAME VERSION READY REASON knative-eventing 0 .23.0 True","title":"If you installed with the Operator"},{"location":"install/upgrade/upgrade-installation-with-operator/","text":"Upgrading using the Knative Operator \u00b6 This topic describes how to upgrade Knative if you installed using the Operator. If you installed using YAML, see Upgrading with kubectl . The attribute spec.version is the only field you need to change in the Serving or Eventing custom resource to perform an upgrade. You do not need to specify the version for the patch number, because the Knative Operator matches the latest available patch number, as long as you specify major.minor for the version. For example, you only need to specify \"0.23\" to upgrade to the 0.23 release, you do not need to specify the exact patch number. The Knative Operator supports up to the last three major releases. For example, if the current version of the Operator is 0.23, it bundles and supports the installation of Knative versions 0.20, 0.21, 0.22 and 0.23. Note In the following examples, Knative Serving custom resources are installed in the knative-serving namespace, and Knative Eventing custom resources are installed in the knative-eventing namespace. Performing the upgrade \u00b6 To upgrade, apply the Operator custom resources, adding the spec.version for the Knative version that you want to upgrade to: Create a YAML file containing the following: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : version : \"<new-version>\" Where <new-version> is the Knative version that you want to upgrade to. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Verifying an upgrade by viewing pods \u00b6 You can confirm that your Knative components have upgraded successfully, by viewing the status of the pods for the components in the relevant namespace. Note All pods will restart during the upgrade and their age will reset. Knative Serving Knative Eventing Enter the following command to view information about pods in the knative-serving namespace: kubectl get pods --namespace knative-serving The command returns an output similar to the following: NAME READY STATUS RESTARTS AGE activator-6875896748-gdjgs 1 /1 Running 0 58s autoscaler-6bbc885cfd-vkrgg 1 /1 Running 0 57s autoscaler-hpa-5cdd7c6b69-hxzv4 1 /1 Running 0 55s controller-64dd4bd56-wzb2k 1 /1 Running 0 57s net-istio-webhook-75cc84fbd4-dkcgt 1 /1 Running 0 50s net-istio-controller-6dcbd4b5f4-mxm8q 1 /1 Running 0 51s storage-version-migration-serving-serving-0.20.0-82hjt 0 /1 Completed 0 50s webhook-75f5d4845d-zkrdt 1 /1 Running 0 56s Enter the following command to view information about pods in the knative-eventing namespace: kubectl get pods --namespace knative-eventing The command returns an output similar to the following: NAME READY STATUS RESTARTS AGE eventing-controller-6bc59c9fd7-6svbm 1 /1 Running 0 38s eventing-webhook-85cd479f87-4dwxh 1 /1 Running 0 38s imc-controller-97c4fd87c-t9mnm 1 /1 Running 0 33s imc-dispatcher-c6db95ffd-ln4mc 1 /1 Running 0 33s mt-broker-controller-5f87fbd5d9-m69cd 1 /1 Running 0 32s mt-broker-filter-5b9c64cbd5-d27p4 1 /1 Running 0 32s mt-broker-ingress-55c66fdfdf-gn56g 1 /1 Running 0 32s storage-version-migration-eventing-0.20.0-fvgqf 0 /1 Completed 0 31s sugar-controller-684d5cfdbb-67vsv 1 /1 Running 0 31s Verifying an upgrade by viewing custom resources \u00b6 You can verify the status of a Knative component by checking that the custom resource READY status is True . Knative Serving Knative Eventing kubectl get KnativeServing knative-serving --namespace knative-serving This command returns an output similar to the following: NAME VERSION READY REASON knative-serving 0 .20.0 True kubectl get KnativeEventing knative-eventing --namespace knative-eventing This command returns an output similar to the following: NAME VERSION READY REASON knative-eventing 0 .20.0 True Rollback to an earlier version \u00b6 If the upgrade fails, you can rollback to restore your Knative to the previous version. For example, if something goes wrong with an upgrade to 0.23, and your previous version is 0.22, you can apply the following custom resources to restore Knative Serving and Knative Eventing to version 0.22. Knative Serving To rollback to a previous version of Knative Serving: Create a YAML file containing the following: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : version : \"<previous-version>\" Where <previous-version> is the Knative version that you want to downgrade to. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Knative Eventing To rollback to a previous version of Knative Eventing: Create a YAML file containing the following: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : version : \"<previous-version>\" Where <previous-version> is the Knative version that you want to downgrade to. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"\u901a\u8fc7Knative Operator\u66f4\u65b0"},{"location":"install/upgrade/upgrade-installation-with-operator/#upgrading-using-the-knative-operator","text":"This topic describes how to upgrade Knative if you installed using the Operator. If you installed using YAML, see Upgrading with kubectl . The attribute spec.version is the only field you need to change in the Serving or Eventing custom resource to perform an upgrade. You do not need to specify the version for the patch number, because the Knative Operator matches the latest available patch number, as long as you specify major.minor for the version. For example, you only need to specify \"0.23\" to upgrade to the 0.23 release, you do not need to specify the exact patch number. The Knative Operator supports up to the last three major releases. For example, if the current version of the Operator is 0.23, it bundles and supports the installation of Knative versions 0.20, 0.21, 0.22 and 0.23. Note In the following examples, Knative Serving custom resources are installed in the knative-serving namespace, and Knative Eventing custom resources are installed in the knative-eventing namespace.","title":"Upgrading using the Knative Operator"},{"location":"install/upgrade/upgrade-installation-with-operator/#performing-the-upgrade","text":"To upgrade, apply the Operator custom resources, adding the spec.version for the Knative version that you want to upgrade to: Create a YAML file containing the following: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : version : \"<new-version>\" Where <new-version> is the Knative version that you want to upgrade to. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"Performing the upgrade"},{"location":"install/upgrade/upgrade-installation-with-operator/#verifying-an-upgrade-by-viewing-pods","text":"You can confirm that your Knative components have upgraded successfully, by viewing the status of the pods for the components in the relevant namespace. Note All pods will restart during the upgrade and their age will reset. Knative Serving Knative Eventing Enter the following command to view information about pods in the knative-serving namespace: kubectl get pods --namespace knative-serving The command returns an output similar to the following: NAME READY STATUS RESTARTS AGE activator-6875896748-gdjgs 1 /1 Running 0 58s autoscaler-6bbc885cfd-vkrgg 1 /1 Running 0 57s autoscaler-hpa-5cdd7c6b69-hxzv4 1 /1 Running 0 55s controller-64dd4bd56-wzb2k 1 /1 Running 0 57s net-istio-webhook-75cc84fbd4-dkcgt 1 /1 Running 0 50s net-istio-controller-6dcbd4b5f4-mxm8q 1 /1 Running 0 51s storage-version-migration-serving-serving-0.20.0-82hjt 0 /1 Completed 0 50s webhook-75f5d4845d-zkrdt 1 /1 Running 0 56s Enter the following command to view information about pods in the knative-eventing namespace: kubectl get pods --namespace knative-eventing The command returns an output similar to the following: NAME READY STATUS RESTARTS AGE eventing-controller-6bc59c9fd7-6svbm 1 /1 Running 0 38s eventing-webhook-85cd479f87-4dwxh 1 /1 Running 0 38s imc-controller-97c4fd87c-t9mnm 1 /1 Running 0 33s imc-dispatcher-c6db95ffd-ln4mc 1 /1 Running 0 33s mt-broker-controller-5f87fbd5d9-m69cd 1 /1 Running 0 32s mt-broker-filter-5b9c64cbd5-d27p4 1 /1 Running 0 32s mt-broker-ingress-55c66fdfdf-gn56g 1 /1 Running 0 32s storage-version-migration-eventing-0.20.0-fvgqf 0 /1 Completed 0 31s sugar-controller-684d5cfdbb-67vsv 1 /1 Running 0 31s","title":"Verifying an upgrade by viewing pods"},{"location":"install/upgrade/upgrade-installation-with-operator/#verifying-an-upgrade-by-viewing-custom-resources","text":"You can verify the status of a Knative component by checking that the custom resource READY status is True . Knative Serving Knative Eventing kubectl get KnativeServing knative-serving --namespace knative-serving This command returns an output similar to the following: NAME VERSION READY REASON knative-serving 0 .20.0 True kubectl get KnativeEventing knative-eventing --namespace knative-eventing This command returns an output similar to the following: NAME VERSION READY REASON knative-eventing 0 .20.0 True","title":"Verifying an upgrade by viewing custom resources"},{"location":"install/upgrade/upgrade-installation-with-operator/#rollback-to-an-earlier-version","text":"If the upgrade fails, you can rollback to restore your Knative to the previous version. For example, if something goes wrong with an upgrade to 0.23, and your previous version is 0.22, you can apply the following custom resources to restore Knative Serving and Knative Eventing to version 0.22. Knative Serving To rollback to a previous version of Knative Serving: Create a YAML file containing the following: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving namespace : knative-serving spec : version : \"<previous-version>\" Where <previous-version> is the Knative version that you want to downgrade to. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Knative Eventing To rollback to a previous version of Knative Eventing: Create a YAML file containing the following: apiVersion : operator.knative.dev/v1alpha1 kind : KnativeEventing metadata : name : knative-eventing namespace : knative-eventing spec : version : \"<previous-version>\" Where <previous-version> is the Knative version that you want to downgrade to. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"Rollback to an earlier version"},{"location":"install/upgrade/upgrade-installation/","text":"Upgrading with kubectl \u00b6 If you installed Knative using YAML, you can use the kubectl apply command in this topic to upgrade your Knative components and plugins. If you installed using the Operator, see Upgrading using the Knative Operator . Before you begin \u00b6 Before upgrading, there are a few steps you must take to ensure a successful upgrade process. Identify breaking changes \u00b6 You should be aware of any breaking changes between your current and desired versions of Knative. Breaking changes between Knative versions are documented in the Knative release notes. Before upgrading, review the release notes for the target version to learn about any changes you might need to make to your Knative applications: Serving Eventing Release notes are published with each version on the \"Releases\" page of their respective repositories in GitHub. View current pod status \u00b6 Before upgrading, view the status of the pods for the namespaces you plan on upgrading. This allows you to compare the before and after state of your namespace. For example, if you are upgrading Knative Serving and Eventing, enter the following commands to see the current state of each namespace: kubectl get pods --namespace knative-serving kubectl get pods --namespace knative-eventing Upgrade plugins \u00b6 If you have a plugin installed, make sure to upgrade it at the same time as you upgrade your Knative components. Run pre-install tools before upgrade \u00b6 In some upgrades there are some steps that must happen before the actual upgrade, and these are identified in the release notes. Upgrade existing resources to the latest stored version \u00b6 Our custom resources are stored within Kubernetes at a particular version. As we introduce newer and remove older supported versions, you must migrate the resources to the designated stored version. This ensures removing older versions will succeed when upgrading. For the various subprojects there is a K8s job to help operators perform this migration. The release notes for each release will state explicitly whether a migration is required. Performing the upgrade \u00b6 To upgrade, apply the YAML files for the subsequent minor versions of all your installed Knative components and features, remembering to only upgrade by one minor version at a time. Before upgrading, check your Knative version . For a cluster running version 0.22 of the Knative Serving and Knative Eventing components, the following command upgrades the installation to v0.23.0: kubectl apply -f https://github.com/knative/serving/releases/download/v0.23.0/serving-core.yaml \\ -f https://github.com/knative/eventing/releases/download/v0.23.0/eventing.yaml \\ Run post-install tools after the upgrade \u00b6 In some upgrades there are some steps that must happen after the actual upgrade, and these are identified in the release notes. Verifying the upgrade \u00b6 To confirm that your components and plugins have successfully upgraded, view the status of their pods in the relevant namespaces. All pods will restart during the upgrade and their age will reset. If you upgraded Knative Serving and Eventing, enter the following commands to get information about the pods for each namespace: kubectl get pods --namespace knative-serving kubectl get pods --namespace knative-eventing These commands return something similar to: NAME READY STATUS RESTARTS AGE activator-79f674fb7b-dgvss 2 /2 Running 0 43s autoscaler-96dc49858-b24bm 2 /2 Running 1 43s autoscaler-hpa-d887d4895-njtrb 1 /1 Running 0 43s controller-6bcdd87fd6-zz9fx 1 /1 Running 0 41s net-istio-controller-7fcdf7-z2xmr 1 /1 Running 0 40s webhook-747b799559-4sj6q 1 /1 Running 0 41s NAME READY STATUS RESTARTS AGE eventing-controller-69ffcc6f7d-5l7th 1 /1 Running 0 83s eventing-webhook-6c56fcd86c-42dr8 1 /1 Running 0 81s imc-controller-6bcf5957b5-6ccp2 1 /1 Running 0 80s imc-dispatcher-f59b7c57-q9xcl 1 /1 Running 0 80s sources-controller-8596684d7b-jxkmd 1 /1 Running 0 83s If the age of all your pods has been reset and all pods are up and running, the upgrade was completed successfully. You might notice a status of Terminating for the old pods as they are cleaned up. If necessary, repeat the upgrade process until you reach your desired minor version number.","title":"\u901a\u8fc7kubectl\u66f4\u65b0"},{"location":"install/upgrade/upgrade-installation/#upgrading-with-kubectl","text":"If you installed Knative using YAML, you can use the kubectl apply command in this topic to upgrade your Knative components and plugins. If you installed using the Operator, see Upgrading using the Knative Operator .","title":"Upgrading with kubectl"},{"location":"install/upgrade/upgrade-installation/#before-you-begin","text":"Before upgrading, there are a few steps you must take to ensure a successful upgrade process.","title":"Before you begin"},{"location":"install/upgrade/upgrade-installation/#identify-breaking-changes","text":"You should be aware of any breaking changes between your current and desired versions of Knative. Breaking changes between Knative versions are documented in the Knative release notes. Before upgrading, review the release notes for the target version to learn about any changes you might need to make to your Knative applications: Serving Eventing Release notes are published with each version on the \"Releases\" page of their respective repositories in GitHub.","title":"Identify breaking changes"},{"location":"install/upgrade/upgrade-installation/#view-current-pod-status","text":"Before upgrading, view the status of the pods for the namespaces you plan on upgrading. This allows you to compare the before and after state of your namespace. For example, if you are upgrading Knative Serving and Eventing, enter the following commands to see the current state of each namespace: kubectl get pods --namespace knative-serving kubectl get pods --namespace knative-eventing","title":"View current pod status"},{"location":"install/upgrade/upgrade-installation/#upgrade-plugins","text":"If you have a plugin installed, make sure to upgrade it at the same time as you upgrade your Knative components.","title":"Upgrade plugins"},{"location":"install/upgrade/upgrade-installation/#run-pre-install-tools-before-upgrade","text":"In some upgrades there are some steps that must happen before the actual upgrade, and these are identified in the release notes.","title":"Run pre-install tools before upgrade"},{"location":"install/upgrade/upgrade-installation/#upgrade-existing-resources-to-the-latest-stored-version","text":"Our custom resources are stored within Kubernetes at a particular version. As we introduce newer and remove older supported versions, you must migrate the resources to the designated stored version. This ensures removing older versions will succeed when upgrading. For the various subprojects there is a K8s job to help operators perform this migration. The release notes for each release will state explicitly whether a migration is required.","title":"Upgrade existing resources to the latest stored version"},{"location":"install/upgrade/upgrade-installation/#performing-the-upgrade","text":"To upgrade, apply the YAML files for the subsequent minor versions of all your installed Knative components and features, remembering to only upgrade by one minor version at a time. Before upgrading, check your Knative version . For a cluster running version 0.22 of the Knative Serving and Knative Eventing components, the following command upgrades the installation to v0.23.0: kubectl apply -f https://github.com/knative/serving/releases/download/v0.23.0/serving-core.yaml \\ -f https://github.com/knative/eventing/releases/download/v0.23.0/eventing.yaml \\","title":"Performing the upgrade"},{"location":"install/upgrade/upgrade-installation/#run-post-install-tools-after-the-upgrade","text":"In some upgrades there are some steps that must happen after the actual upgrade, and these are identified in the release notes.","title":"Run post-install tools after the upgrade"},{"location":"install/upgrade/upgrade-installation/#verifying-the-upgrade","text":"To confirm that your components and plugins have successfully upgraded, view the status of their pods in the relevant namespaces. All pods will restart during the upgrade and their age will reset. If you upgraded Knative Serving and Eventing, enter the following commands to get information about the pods for each namespace: kubectl get pods --namespace knative-serving kubectl get pods --namespace knative-eventing These commands return something similar to: NAME READY STATUS RESTARTS AGE activator-79f674fb7b-dgvss 2 /2 Running 0 43s autoscaler-96dc49858-b24bm 2 /2 Running 1 43s autoscaler-hpa-d887d4895-njtrb 1 /1 Running 0 43s controller-6bcdd87fd6-zz9fx 1 /1 Running 0 41s net-istio-controller-7fcdf7-z2xmr 1 /1 Running 0 40s webhook-747b799559-4sj6q 1 /1 Running 0 41s NAME READY STATUS RESTARTS AGE eventing-controller-69ffcc6f7d-5l7th 1 /1 Running 0 83s eventing-webhook-6c56fcd86c-42dr8 1 /1 Running 0 81s imc-controller-6bcf5957b5-6ccp2 1 /1 Running 0 80s imc-dispatcher-f59b7c57-q9xcl 1 /1 Running 0 80s sources-controller-8596684d7b-jxkmd 1 /1 Running 0 83s If the age of all your pods has been reset and all pods are up and running, the upgrade was completed successfully. You might notice a status of Terminating for the old pods as they are cleaned up. If necessary, repeat the upgrade process until you reach your desired minor version number.","title":"Verifying the upgrade"},{"location":"install/yaml-install/","text":"About YAML-based installation \u00b6 You can install the Serving component, Eventing component, or both on your cluster by applying YAML files. Install Knative Serving with YAML Install Knative Eventing with YAML","title":"\u5173\u4e8e\u57fa\u4e8eYAML\u7684\u5b89\u88c5"},{"location":"install/yaml-install/#about-yaml-based-installation","text":"You can install the Serving component, Eventing component, or both on your cluster by applying YAML files. Install Knative Serving with YAML Install Knative Eventing with YAML","title":"About YAML-based installation"},{"location":"install/yaml-install/eventing/eventing-installation-files/","text":"Knative Eventing installation files \u00b6 This guide provides reference information about the core Knative Eventing YAML files, including: The custom resource definitions (CRDs) and core components required to install Knative Eventing. Optional components that you can apply to customize your installation. For information about installing these files, see Installing Knative Eventing using YAML files . The following table describes the installation files included in Knative Eventing: File name Description Dependencies eventing-core.yaml Required: Knative Eventing core components. eventing-crds.yaml eventing-crds.yaml Required: Knative Eventing core CRDs. none eventing-post-install.yaml Jobs required for upgrading to a new minor version. eventing-core.yaml, eventing-crds.yaml eventing-sugar-controller.yaml Reconciler that watches for labels and annotations on certain resources to inject eventing components. eventing-core.yaml eventing.yaml Combines eventing-core.yaml , mt-channel-broker.yaml , and in-memory-channel.yaml . none in-memory-channel.yaml Components to configure In-Memory Channels. eventing-core.yaml mt-channel-broker.yaml Components to configure Multi-Tenant (MT) Channel Broker. eventing-core.yaml","title":"Knative Eventing \u5b89\u88c5\u76f8\u5173\u6587\u4ef6"},{"location":"install/yaml-install/eventing/eventing-installation-files/#knative-eventing-installation-files","text":"This guide provides reference information about the core Knative Eventing YAML files, including: The custom resource definitions (CRDs) and core components required to install Knative Eventing. Optional components that you can apply to customize your installation. For information about installing these files, see Installing Knative Eventing using YAML files . The following table describes the installation files included in Knative Eventing: File name Description Dependencies eventing-core.yaml Required: Knative Eventing core components. eventing-crds.yaml eventing-crds.yaml Required: Knative Eventing core CRDs. none eventing-post-install.yaml Jobs required for upgrading to a new minor version. eventing-core.yaml, eventing-crds.yaml eventing-sugar-controller.yaml Reconciler that watches for labels and annotations on certain resources to inject eventing components. eventing-core.yaml eventing.yaml Combines eventing-core.yaml , mt-channel-broker.yaml , and in-memory-channel.yaml . none in-memory-channel.yaml Components to configure In-Memory Channels. eventing-core.yaml mt-channel-broker.yaml Components to configure Multi-Tenant (MT) Channel Broker. eventing-core.yaml","title":"Knative Eventing installation files"},{"location":"install/yaml-install/eventing/install-eventing-with-yaml/","text":"Installing Knative Eventing using YAML files \u00b6 This topic describes how to install Knative Eventing by applying YAML files using the kubectl CLI. Prerequisites \u00b6 Before installing Knative, you must meet the following prerequisites: For prototyping purposes , Knative works on most local deployments of Kubernetes. For example, you can use a local, one-node cluster that has 2 CPUs and 4 GB of memory. Tip You can install a local distribution of Knative for development purposes using the Knative Quickstart plugin For production purposes , it is recommended that: If you have only one node in your cluster, you need at least 6 CPUs, 6 GB of memory, and 30 GB of disk storage. If you have multiple nodes in your cluster, for each node you need at least 2 CPUs, 4 GB of memory, and 20 GB of disk storage. You have a cluster that uses Kubernetes v1.22 or newer. You have installed the kubectl CLI . Your Kubernetes cluster must have access to the internet, because Kubernetes needs to be able to fetch images. To pull from a private registry, see Deploying images from a private container registry . Caution The system requirements provided are recommendations only. The requirements for your installation might vary, depending on whether you use optional components, such as a networking layer. Install Knative Eventing \u00b6 To install Knative Eventing: Install the required custom resource definitions (CRDs) by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-crds.yaml Install the core components of Eventing by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-core.yaml Info For information about the YAML files in Knative Eventing, see Description Tables for YAML Files . Verify the installation \u00b6 Success Monitor the Knative components until all of the components show a STATUS of Running or Completed . You can do this by running the following command and inspecting the output: kubectl get pods -n knative-eventing Example output: NAME READY STATUS RESTARTS AGE eventing-controller-7995d654c7-qg895 1 /1 Running 0 2m18s eventing-webhook-fff97b47c-8hmt8 1 /1 Running 0 2m17s Optional: Install a default Channel (messaging) layer \u00b6 The following tabs expand to show instructions for installing a default Channel layer. Follow the procedure for the Channel of your choice: Apache Kafka Channel Google Cloud Pub/Sub Channel In-Memory (standalone) NATS Channel Install Strimzi . Install the Apache Kafka Channel for Knative from the knative-sandbox repository . Install the Google Cloud Pub/Sub Channel by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/knative-gcp/latest/cloud-run-events.yaml This command installs both the Channel and the GCP Sources. Tip To learn more, try the Google Cloud Pub/Sub channel sample . Warning This simple standalone implementation runs in-memory and is not suitable for production use cases. Install an in-memory implementation of Channel by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/in-memory-channel.yaml Install NATS Streaming for Kubernetes . Install the NATS Streaming Channel by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-natss/latest/eventing-natss.yaml You can change the default channel implementation by following the instructions described in the Configure Channel defaults section. Optional: Install a Broker layer \u00b6 The following tabs expand to show instructions for installing the Broker layer. Follow the procedure for the Broker of your choice: Apache Kafka Broker MT-Channel-based RabbitMQ Broker The following commands install the Apache Kafka Broker and run event routing in a system namespace. The knative-eventing namespace is used by default. Install the Kafka controller by running the following command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Install the Kafka Broker data plane by running the following command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-broker.yaml For more information, see the Kafka Broker documentation. This implementation of Broker uses Channels and runs event routing components in a system namespace, providing a smaller and simpler installation. Install this implementation of Broker by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/mt-channel-broker.yaml To customize which Broker Channel implementation is used, update the following ConfigMap to specify which configurations are used for which namespaces: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing data : default-br-config : | # This is the cluster-wide default broker channel. clusterDefault: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: imc-channel namespace: knative-eventing # This allows you to specify different defaults per-namespace, # in this case the \"some-namespace\" namespace will use the Kafka # channel ConfigMap by default (only for example, you will need # to install kafka also to make use of this). namespaceDefaults: some-namespace: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: kafka-channel namespace: knative-eventing The referenced imc-channel and kafka-channel example ConfigMaps would look like: apiVersion : v1 kind : ConfigMap metadata : name : imc-channel namespace : knative-eventing data : channel-template-spec : | apiVersion: messaging.knative.dev/v1 kind: InMemoryChannel --- apiVersion : v1 kind : ConfigMap metadata : name : kafka-channel namespace : knative-eventing data : channel-template-spec : | apiVersion: messaging.knative.dev/v1alpha1 kind: KafkaChannel spec: numPartitions: 3 replicationFactor: 1 Warning In order to use the KafkaChannel, ensure that it is installed on your cluster, as mentioned previously in this topic. Install the RabbitMQ Broker by following the instructions in the RabbitMQ Knative Eventing Broker README . For more information, see the RabbitMQ Broker in GitHub. Install optional Eventing extensions \u00b6 The following tabs expand to show instructions for installing each Eventing extension. Apache Kafka Sink Sugar Controller GitHub Source Apache Kafka Source GCP Sources Apache CouchDB Source VMware Sources and Bindings Install the Kafka controller by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Install the Kafka Sink data plane by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-sink.yaml For more information, see the Kafka Sink documentation. Install the Eventing Sugar Controller by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-sugar-controller.yaml The Knative Eventing Sugar Controller reacts to special labels and annotations and produce Eventing resources. For example: When a namespace is labeled with eventing.knative.dev/injection=enabled , the controller creates a default Broker in that namespace. When a Trigger is annotated with eventing.knative.dev/injection=enabled , the controller creates a Broker named by that Trigger in the Trigger's namespace. Enable the default Broker on a namespace (here default ) by running the command: kubectl label namespace <namespace-name> eventing.knative.dev/injection = enabled Where <namespace-name> is the name of the namespace. A single-tenant GitHub source creates one Knative service per GitHub source. A multi-tenant GitHub source only creates one Knative Service, which handles all GitHub sources in the cluster. This source does not support logging or tracing configuration. To install a single-tenant GitHub source run the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-github/latest/github.yaml To install a multi-tenant GitHub source run the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-github/latest/mt-github.yaml To learn more, try the GitHub source sample Install the Apache Kafka Source by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka/latest/source.yaml To learn more, try the Apache Kafka source sample . Install the GCP Sources by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/knative-gcp/latest/cloud-run-events.yaml This command installs both the Sources and the Channel. To learn more, try the following samples: Cloud Pub/Sub source sample Cloud Storage source sample Cloud Scheduler source sample Cloud Audit Logs source sample Install the Apache CouchDB Source by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-couchdb/latest/couchdb.yaml To learn more, read the Apache CouchDB source documentation. Install VMware Sources and Bindings by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/sources-for-knative/latest/release.yaml To learn more, try the VMware sources and bindings samples .","title":"\u901a\u8fc7YAML\u5b89\u88c5Eventing"},{"location":"install/yaml-install/eventing/install-eventing-with-yaml/#installing-knative-eventing-using-yaml-files","text":"This topic describes how to install Knative Eventing by applying YAML files using the kubectl CLI.","title":"Installing Knative Eventing using YAML files"},{"location":"install/yaml-install/eventing/install-eventing-with-yaml/#prerequisites","text":"Before installing Knative, you must meet the following prerequisites: For prototyping purposes , Knative works on most local deployments of Kubernetes. For example, you can use a local, one-node cluster that has 2 CPUs and 4 GB of memory. Tip You can install a local distribution of Knative for development purposes using the Knative Quickstart plugin For production purposes , it is recommended that: If you have only one node in your cluster, you need at least 6 CPUs, 6 GB of memory, and 30 GB of disk storage. If you have multiple nodes in your cluster, for each node you need at least 2 CPUs, 4 GB of memory, and 20 GB of disk storage. You have a cluster that uses Kubernetes v1.22 or newer. You have installed the kubectl CLI . Your Kubernetes cluster must have access to the internet, because Kubernetes needs to be able to fetch images. To pull from a private registry, see Deploying images from a private container registry . Caution The system requirements provided are recommendations only. The requirements for your installation might vary, depending on whether you use optional components, such as a networking layer.","title":"Prerequisites"},{"location":"install/yaml-install/eventing/install-eventing-with-yaml/#install-knative-eventing","text":"To install Knative Eventing: Install the required custom resource definitions (CRDs) by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-crds.yaml Install the core components of Eventing by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-core.yaml Info For information about the YAML files in Knative Eventing, see Description Tables for YAML Files .","title":"Install Knative Eventing"},{"location":"install/yaml-install/eventing/install-eventing-with-yaml/#verify-the-installation","text":"Success Monitor the Knative components until all of the components show a STATUS of Running or Completed . You can do this by running the following command and inspecting the output: kubectl get pods -n knative-eventing Example output: NAME READY STATUS RESTARTS AGE eventing-controller-7995d654c7-qg895 1 /1 Running 0 2m18s eventing-webhook-fff97b47c-8hmt8 1 /1 Running 0 2m17s","title":"Verify the installation"},{"location":"install/yaml-install/eventing/install-eventing-with-yaml/#optional-install-a-default-channel-messaging-layer","text":"The following tabs expand to show instructions for installing a default Channel layer. Follow the procedure for the Channel of your choice: Apache Kafka Channel Google Cloud Pub/Sub Channel In-Memory (standalone) NATS Channel Install Strimzi . Install the Apache Kafka Channel for Knative from the knative-sandbox repository . Install the Google Cloud Pub/Sub Channel by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/knative-gcp/latest/cloud-run-events.yaml This command installs both the Channel and the GCP Sources. Tip To learn more, try the Google Cloud Pub/Sub channel sample . Warning This simple standalone implementation runs in-memory and is not suitable for production use cases. Install an in-memory implementation of Channel by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/in-memory-channel.yaml Install NATS Streaming for Kubernetes . Install the NATS Streaming Channel by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-natss/latest/eventing-natss.yaml You can change the default channel implementation by following the instructions described in the Configure Channel defaults section.","title":"Optional: Install a default Channel (messaging) layer"},{"location":"install/yaml-install/eventing/install-eventing-with-yaml/#optional-install-a-broker-layer","text":"The following tabs expand to show instructions for installing the Broker layer. Follow the procedure for the Broker of your choice: Apache Kafka Broker MT-Channel-based RabbitMQ Broker The following commands install the Apache Kafka Broker and run event routing in a system namespace. The knative-eventing namespace is used by default. Install the Kafka controller by running the following command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Install the Kafka Broker data plane by running the following command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-broker.yaml For more information, see the Kafka Broker documentation. This implementation of Broker uses Channels and runs event routing components in a system namespace, providing a smaller and simpler installation. Install this implementation of Broker by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/mt-channel-broker.yaml To customize which Broker Channel implementation is used, update the following ConfigMap to specify which configurations are used for which namespaces: apiVersion : v1 kind : ConfigMap metadata : name : config-br-defaults namespace : knative-eventing data : default-br-config : | # This is the cluster-wide default broker channel. clusterDefault: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: imc-channel namespace: knative-eventing # This allows you to specify different defaults per-namespace, # in this case the \"some-namespace\" namespace will use the Kafka # channel ConfigMap by default (only for example, you will need # to install kafka also to make use of this). namespaceDefaults: some-namespace: brokerClass: MTChannelBasedBroker apiVersion: v1 kind: ConfigMap name: kafka-channel namespace: knative-eventing The referenced imc-channel and kafka-channel example ConfigMaps would look like: apiVersion : v1 kind : ConfigMap metadata : name : imc-channel namespace : knative-eventing data : channel-template-spec : | apiVersion: messaging.knative.dev/v1 kind: InMemoryChannel --- apiVersion : v1 kind : ConfigMap metadata : name : kafka-channel namespace : knative-eventing data : channel-template-spec : | apiVersion: messaging.knative.dev/v1alpha1 kind: KafkaChannel spec: numPartitions: 3 replicationFactor: 1 Warning In order to use the KafkaChannel, ensure that it is installed on your cluster, as mentioned previously in this topic. Install the RabbitMQ Broker by following the instructions in the RabbitMQ Knative Eventing Broker README . For more information, see the RabbitMQ Broker in GitHub.","title":"Optional: Install a Broker layer"},{"location":"install/yaml-install/eventing/install-eventing-with-yaml/#install-optional-eventing-extensions","text":"The following tabs expand to show instructions for installing each Eventing extension. Apache Kafka Sink Sugar Controller GitHub Source Apache Kafka Source GCP Sources Apache CouchDB Source VMware Sources and Bindings Install the Kafka controller by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-controller.yaml Install the Kafka Sink data plane by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka-broker/latest/eventing-kafka-sink.yaml For more information, see the Kafka Sink documentation. Install the Eventing Sugar Controller by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing/latest/eventing-sugar-controller.yaml The Knative Eventing Sugar Controller reacts to special labels and annotations and produce Eventing resources. For example: When a namespace is labeled with eventing.knative.dev/injection=enabled , the controller creates a default Broker in that namespace. When a Trigger is annotated with eventing.knative.dev/injection=enabled , the controller creates a Broker named by that Trigger in the Trigger's namespace. Enable the default Broker on a namespace (here default ) by running the command: kubectl label namespace <namespace-name> eventing.knative.dev/injection = enabled Where <namespace-name> is the name of the namespace. A single-tenant GitHub source creates one Knative service per GitHub source. A multi-tenant GitHub source only creates one Knative Service, which handles all GitHub sources in the cluster. This source does not support logging or tracing configuration. To install a single-tenant GitHub source run the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-github/latest/github.yaml To install a multi-tenant GitHub source run the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-github/latest/mt-github.yaml To learn more, try the GitHub source sample Install the Apache Kafka Source by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-kafka/latest/source.yaml To learn more, try the Apache Kafka source sample . Install the GCP Sources by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/knative-gcp/latest/cloud-run-events.yaml This command installs both the Sources and the Channel. To learn more, try the following samples: Cloud Pub/Sub source sample Cloud Storage source sample Cloud Scheduler source sample Cloud Audit Logs source sample Install the Apache CouchDB Source by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/eventing-couchdb/latest/couchdb.yaml To learn more, read the Apache CouchDB source documentation. Install VMware Sources and Bindings by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/sources-for-knative/latest/release.yaml To learn more, try the VMware sources and bindings samples .","title":"Install optional Eventing extensions"},{"location":"install/yaml-install/serving/install-serving-with-yaml/","text":"Installing Knative Serving using YAML files \u00b6 This topic describes how to install Knative Serving by applying YAML files using the kubectl CLI. Prerequisites \u00b6 Before installing Knative, you must meet the following prerequisites: For prototyping purposes , Knative works on most local deployments of Kubernetes. For example, you can use a local, one-node cluster that has 2 CPUs and 4 GB of memory. Tip You can install a local distribution of Knative for development purposes using the Knative Quickstart plugin For production purposes , it is recommended that: If you have only one node in your cluster, you need at least 6 CPUs, 6 GB of memory, and 30 GB of disk storage. If you have multiple nodes in your cluster, for each node you need at least 2 CPUs, 4 GB of memory, and 20 GB of disk storage. You have a cluster that uses Kubernetes v1.22 or newer. You have installed the kubectl CLI . Your Kubernetes cluster must have access to the internet, because Kubernetes needs to be able to fetch images. To pull from a private registry, see Deploying images from a private container registry . Caution The system requirements provided are recommendations only. The requirements for your installation might vary, depending on whether you use optional components, such as a networking layer. Install the Knative Serving component \u00b6 To install the Knative Serving component: Install the required custom resources by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-crds.yaml Install the core components of Knative Serving by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-core.yaml Info For information about the YAML files in Knative Serving, see Knative Serving installation files . Install a networking layer \u00b6 The following tabs expand to show instructions for installing a networking layer. Follow the procedure for the networking layer of your choice: Kourier (Choose this if you are not sure) Istio Contour The following commands install Kourier and enable its Knative integration. Install the Knative Kourier controller by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-kourier/latest/kourier.yaml Configure Knative Serving to use Kourier by default by running the command: kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"ingress-class\":\"kourier.ingress.networking.knative.dev\"}}' Fetch the External IP address or CNAME by running the command: kubectl --namespace kourier-system get service kourier Tip Save this to use in the following Configure DNS section. The following commands install Istio and enable its Knative integration. Install a properly configured Istio by following the Advanced Istio installation instructions or by running the command: kubectl apply -l knative.dev/crd-install = true -f https://storage.googleapis.com/knative-nightly/net-istio/latest/istio.yaml kubectl apply -f https://storage.googleapis.com/knative-nightly/net-istio/latest/istio.yaml Install the Knative Istio controller by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-istio/latest/net-istio.yaml Fetch the External IP address or CNAME by running the command: kubectl --namespace istio-system get service istio-ingressgateway Tip Save this to use in the following Configure DNS section. The following commands install Contour and enable its Knative integration. Install a properly configured Contour by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-contour/latest/contour.yaml Install the Knative Contour controller by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-contour/latest/net-contour.yaml Configure Knative Serving to use Contour by default by running the command: kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"ingress-class\":\"contour.ingress.networking.knative.dev\"}}' Fetch the External IP address or CNAME by running the command: kubectl --namespace contour-external get service envoy Tip Save this to use in the following Configure DNS section. Verify the installation \u00b6 Success Monitor the Knative components until all of the components show a STATUS of Running or Completed . You can do this by running the following command and inspecting the output: kubectl get pods -n knative-serving Example output: NAME READY STATUS RESTARTS AGE 3scale-kourier-control-54cc54cc58-mmdgq 1 /1 Running 0 81s activator-67656dcbbb-8mftq 1 /1 Running 0 97s autoscaler-df6856b64-5h4lc 1 /1 Running 0 97s controller-788796f49d-4x6pm 1 /1 Running 0 97s domain-mapping-65f58c79dc-9cw6d 1 /1 Running 0 97s domainmapping-webhook-cc646465c-jnwbz 1 /1 Running 0 97s webhook-859796bc7-8n5g2 1 /1 Running 0 96s Configure DNS \u00b6 You can configure DNS to prevent the need to run curl commands with a host header. The following tabs expand to show instructions for configuring DNS. Follow the procedure for the DNS of your choice: Magic DNS (sslip.io) Real DNS Temporary DNS Knative provides a Kubernetes Job called default-domain that configures Knative Serving to use sslip.io as the default DNS suffix. kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-default-domain.yaml Warning This will only work if the cluster LoadBalancer Service exposes an IPv4 address or hostname, so it will not work with IPv6 clusters or local setups like minikube unless minikube tunnel is running. In these cases, see the \"Real DNS\" or \"Temporary DNS\" tabs. To configure DNS for Knative, take the External IP or CNAME from setting up networking, and configure it with your DNS provider as follows: If the networking layer produced an External IP address, then configure a wildcard A record for the domain: # Here knative.example.com is the domain suffix for your cluster *.knative.example.com == A 35.233.41.212 If the networking layer produced a CNAME, then configure a CNAME record for the domain: # Here knative.example.com is the domain suffix for your cluster *.knative.example.com == CNAME a317a278525d111e89f272a164fd35fb-1510370581.eu-central-1.elb.amazonaws.com Once your DNS provider has been configured, direct Knative to use that domain: # Replace knative.example.com with your domain suffix kubectl patch configmap/config-domain \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"knative.example.com\":\"\"}}' If you are using curl to access the sample applications, or your own Knative app, and are unable to use the \"Magic DNS (sslip.io)\" or \"Real DNS\" methods, there is a temporary approach. This is useful for those who wish to evaluate Knative without altering their DNS configuration, as per the \"Real DNS\" method, or cannot use the \"Magic DNS\" method due to using, for example, minikube locally or IPv6 clusters. To access your application using curl using this method: After starting your application, get the URL of your application: kubectl get ksvc The output should be similar to: NAME URL LATESTCREATED LATESTREADY READY REASON helloworld-go http://helloworld-go.default.example.com helloworld-go-vqjlf helloworld-go-vqjlf True Instruct curl to connect to the External IP or CNAME defined by the networking layer mentioned in section 3, and use the -H \"Host:\" command-line option to specify the Knative application's host name. For example, if the networking layer defines your External IP and port to be http://192.168.39.228:32198 and you wish to access the helloworld-go application mentioned earlier, use: curl -H \"Host: helloworld-go.default.example.com\" http://192.168.39.228:32198 In the case of the provided helloworld-go sample application, using the default configuration, the output is: Hello Go Sample v1! Refer to the \"Real DNS\" method for a permanent solution. Install optional Serving extensions \u00b6 The following tabs expand to show instructions for installing each Serving extension. HPA autoscaling TLS with cert-manager TLS with HTTP01 Knative also supports the use of the Kubernetes Horizontal Pod Autoscaler (HPA) for driving autoscaling decisions. Install the components needed to support HPA-class autoscaling by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-hpa.yaml Knative supports automatically provisioning TLS certificates through cert-manager . The following commands install the components needed to support the provisioning of TLS certificates through cert-manager. Install cert-manager version v1.0.0 or later . Install the component that integrates Knative with cert-manager by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-certmanager/latest/release.yaml Configure Knative to automatically configure TLS certificates by following the steps in Enabling automatic TLS certificate provisioning . Knative supports automatically provisioning TLS certificates using Encrypt HTTP01 challenges. The following commands install the components needed to support TLS. Install the net-http01 controller by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-http01/latest/release.yaml Configure the certificate-class to use this certificate type by running the command: kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"certificate-class\":\"net-http01.certificate.networking.knative.dev\"}}' Enable autoTLS by running the command: kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"auto-tls\":\"Enabled\"}}'","title":"\u901a\u8fc7YAML\u5b89\u88c5Serving"},{"location":"install/yaml-install/serving/install-serving-with-yaml/#installing-knative-serving-using-yaml-files","text":"This topic describes how to install Knative Serving by applying YAML files using the kubectl CLI.","title":"Installing Knative Serving using YAML files"},{"location":"install/yaml-install/serving/install-serving-with-yaml/#prerequisites","text":"Before installing Knative, you must meet the following prerequisites: For prototyping purposes , Knative works on most local deployments of Kubernetes. For example, you can use a local, one-node cluster that has 2 CPUs and 4 GB of memory. Tip You can install a local distribution of Knative for development purposes using the Knative Quickstart plugin For production purposes , it is recommended that: If you have only one node in your cluster, you need at least 6 CPUs, 6 GB of memory, and 30 GB of disk storage. If you have multiple nodes in your cluster, for each node you need at least 2 CPUs, 4 GB of memory, and 20 GB of disk storage. You have a cluster that uses Kubernetes v1.22 or newer. You have installed the kubectl CLI . Your Kubernetes cluster must have access to the internet, because Kubernetes needs to be able to fetch images. To pull from a private registry, see Deploying images from a private container registry . Caution The system requirements provided are recommendations only. The requirements for your installation might vary, depending on whether you use optional components, such as a networking layer.","title":"Prerequisites"},{"location":"install/yaml-install/serving/install-serving-with-yaml/#install-the-knative-serving-component","text":"To install the Knative Serving component: Install the required custom resources by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-crds.yaml Install the core components of Knative Serving by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-core.yaml Info For information about the YAML files in Knative Serving, see Knative Serving installation files .","title":"Install the Knative Serving component"},{"location":"install/yaml-install/serving/install-serving-with-yaml/#install-a-networking-layer","text":"The following tabs expand to show instructions for installing a networking layer. Follow the procedure for the networking layer of your choice: Kourier (Choose this if you are not sure) Istio Contour The following commands install Kourier and enable its Knative integration. Install the Knative Kourier controller by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-kourier/latest/kourier.yaml Configure Knative Serving to use Kourier by default by running the command: kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"ingress-class\":\"kourier.ingress.networking.knative.dev\"}}' Fetch the External IP address or CNAME by running the command: kubectl --namespace kourier-system get service kourier Tip Save this to use in the following Configure DNS section. The following commands install Istio and enable its Knative integration. Install a properly configured Istio by following the Advanced Istio installation instructions or by running the command: kubectl apply -l knative.dev/crd-install = true -f https://storage.googleapis.com/knative-nightly/net-istio/latest/istio.yaml kubectl apply -f https://storage.googleapis.com/knative-nightly/net-istio/latest/istio.yaml Install the Knative Istio controller by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-istio/latest/net-istio.yaml Fetch the External IP address or CNAME by running the command: kubectl --namespace istio-system get service istio-ingressgateway Tip Save this to use in the following Configure DNS section. The following commands install Contour and enable its Knative integration. Install a properly configured Contour by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-contour/latest/contour.yaml Install the Knative Contour controller by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-contour/latest/net-contour.yaml Configure Knative Serving to use Contour by default by running the command: kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"ingress-class\":\"contour.ingress.networking.knative.dev\"}}' Fetch the External IP address or CNAME by running the command: kubectl --namespace contour-external get service envoy Tip Save this to use in the following Configure DNS section.","title":"Install a networking layer"},{"location":"install/yaml-install/serving/install-serving-with-yaml/#verify-the-installation","text":"Success Monitor the Knative components until all of the components show a STATUS of Running or Completed . You can do this by running the following command and inspecting the output: kubectl get pods -n knative-serving Example output: NAME READY STATUS RESTARTS AGE 3scale-kourier-control-54cc54cc58-mmdgq 1 /1 Running 0 81s activator-67656dcbbb-8mftq 1 /1 Running 0 97s autoscaler-df6856b64-5h4lc 1 /1 Running 0 97s controller-788796f49d-4x6pm 1 /1 Running 0 97s domain-mapping-65f58c79dc-9cw6d 1 /1 Running 0 97s domainmapping-webhook-cc646465c-jnwbz 1 /1 Running 0 97s webhook-859796bc7-8n5g2 1 /1 Running 0 96s","title":"Verify the installation"},{"location":"install/yaml-install/serving/install-serving-with-yaml/#configure-dns","text":"You can configure DNS to prevent the need to run curl commands with a host header. The following tabs expand to show instructions for configuring DNS. Follow the procedure for the DNS of your choice: Magic DNS (sslip.io) Real DNS Temporary DNS Knative provides a Kubernetes Job called default-domain that configures Knative Serving to use sslip.io as the default DNS suffix. kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-default-domain.yaml Warning This will only work if the cluster LoadBalancer Service exposes an IPv4 address or hostname, so it will not work with IPv6 clusters or local setups like minikube unless minikube tunnel is running. In these cases, see the \"Real DNS\" or \"Temporary DNS\" tabs. To configure DNS for Knative, take the External IP or CNAME from setting up networking, and configure it with your DNS provider as follows: If the networking layer produced an External IP address, then configure a wildcard A record for the domain: # Here knative.example.com is the domain suffix for your cluster *.knative.example.com == A 35.233.41.212 If the networking layer produced a CNAME, then configure a CNAME record for the domain: # Here knative.example.com is the domain suffix for your cluster *.knative.example.com == CNAME a317a278525d111e89f272a164fd35fb-1510370581.eu-central-1.elb.amazonaws.com Once your DNS provider has been configured, direct Knative to use that domain: # Replace knative.example.com with your domain suffix kubectl patch configmap/config-domain \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"knative.example.com\":\"\"}}' If you are using curl to access the sample applications, or your own Knative app, and are unable to use the \"Magic DNS (sslip.io)\" or \"Real DNS\" methods, there is a temporary approach. This is useful for those who wish to evaluate Knative without altering their DNS configuration, as per the \"Real DNS\" method, or cannot use the \"Magic DNS\" method due to using, for example, minikube locally or IPv6 clusters. To access your application using curl using this method: After starting your application, get the URL of your application: kubectl get ksvc The output should be similar to: NAME URL LATESTCREATED LATESTREADY READY REASON helloworld-go http://helloworld-go.default.example.com helloworld-go-vqjlf helloworld-go-vqjlf True Instruct curl to connect to the External IP or CNAME defined by the networking layer mentioned in section 3, and use the -H \"Host:\" command-line option to specify the Knative application's host name. For example, if the networking layer defines your External IP and port to be http://192.168.39.228:32198 and you wish to access the helloworld-go application mentioned earlier, use: curl -H \"Host: helloworld-go.default.example.com\" http://192.168.39.228:32198 In the case of the provided helloworld-go sample application, using the default configuration, the output is: Hello Go Sample v1! Refer to the \"Real DNS\" method for a permanent solution.","title":"Configure DNS"},{"location":"install/yaml-install/serving/install-serving-with-yaml/#install-optional-serving-extensions","text":"The following tabs expand to show instructions for installing each Serving extension. HPA autoscaling TLS with cert-manager TLS with HTTP01 Knative also supports the use of the Kubernetes Horizontal Pod Autoscaler (HPA) for driving autoscaling decisions. Install the components needed to support HPA-class autoscaling by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/serving/latest/serving-hpa.yaml Knative supports automatically provisioning TLS certificates through cert-manager . The following commands install the components needed to support the provisioning of TLS certificates through cert-manager. Install cert-manager version v1.0.0 or later . Install the component that integrates Knative with cert-manager by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-certmanager/latest/release.yaml Configure Knative to automatically configure TLS certificates by following the steps in Enabling automatic TLS certificate provisioning . Knative supports automatically provisioning TLS certificates using Encrypt HTTP01 challenges. The following commands install the components needed to support TLS. Install the net-http01 controller by running the command: kubectl apply -f https://storage.googleapis.com/knative-nightly/net-http01/latest/release.yaml Configure the certificate-class to use this certificate type by running the command: kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"certificate-class\":\"net-http01.certificate.networking.knative.dev\"}}' Enable autoTLS by running the command: kubectl patch configmap/config-network \\ --namespace knative-serving \\ --type merge \\ --patch '{\"data\":{\"auto-tls\":\"Enabled\"}}'","title":"Install optional Serving extensions"},{"location":"install/yaml-install/serving/serving-installation-files/","text":"Knative Serving installation files \u00b6 This guide provides reference information about the core Knative Serving YAML files, including: The custom resource definitions (CRDs) and core components required to install Knative Serving. Optional components that you can apply to customize your installation. For information about installing these files, see Installing Knative Serving using YAML files . The following table describes the installation files included in Knative Serving: File name Description Dependencies serving-core.yaml Required: Knative Serving core components. serving-crds.yaml serving-crds.yaml Required: Knative Serving core CRDs. none serving-default-domain.yaml Configures Knative Serving to use http://sslip.io as the default DNS suffix. serving-core.yaml serving-hpa.yaml Components to autoscale Knative revisions through the Kubernetes Horizontal Pod Autoscaler. serving-core.yaml serving-post-install-jobs.yaml Additional jobs after installing serving-core.yaml . Currently it is the same as serving-storage-version-migration.yaml . serving-core.yaml serving-storage-version-migration.yaml Migrates the storage version of Knative resources, including Service, Route, Revision, and Configuration, from v1alpha1 and v1beta1 to v1 . Required by upgrade from version 0.18 to 0.19. serving-core.yaml","title":"Knative Serving \u5b89\u88c5\u76f8\u5173\u6587\u4ef6"},{"location":"install/yaml-install/serving/serving-installation-files/#knative-serving-installation-files","text":"This guide provides reference information about the core Knative Serving YAML files, including: The custom resource definitions (CRDs) and core components required to install Knative Serving. Optional components that you can apply to customize your installation. For information about installing these files, see Installing Knative Serving using YAML files . The following table describes the installation files included in Knative Serving: File name Description Dependencies serving-core.yaml Required: Knative Serving core components. serving-crds.yaml serving-crds.yaml Required: Knative Serving core CRDs. none serving-default-domain.yaml Configures Knative Serving to use http://sslip.io as the default DNS suffix. serving-core.yaml serving-hpa.yaml Components to autoscale Knative revisions through the Kubernetes Horizontal Pod Autoscaler. serving-core.yaml serving-post-install-jobs.yaml Additional jobs after installing serving-core.yaml . Currently it is the same as serving-storage-version-migration.yaml . serving-core.yaml serving-storage-version-migration.yaml Migrates the storage version of Knative resources, including Service, Route, Revision, and Configuration, from v1alpha1 and v1beta1 to v1 . Required by upgrade from version 0.18 to 0.19. serving-core.yaml","title":"Knative Serving installation files"},{"location":"reference/","text":"Knative Reference Documentation \u00b6 Knative reference documentation.","title":"\u53c2\u8003\u6982\u8ff0"},{"location":"reference/#knative-reference-documentation","text":"Knative reference documentation.","title":"Knative Reference Documentation"},{"location":"reference/api/eventing-api/","text":"This file is updated to the correct version from the eventing repo (docs/eventing-api.md) during the build.","title":"Eventing"},{"location":"reference/api/serving-api/","text":"This file is updated to the correct version from the serving repo (docs/serving-api.md) during the build.","title":"Serving"},{"location":"reference/client/","text":"Knative Client Reference \u00b6 See the kn documentation in Github.","title":"Client"},{"location":"reference/client/#knative-client-reference","text":"See the kn documentation in Github.","title":"Knative Client Reference"},{"location":"reference/concepts/duck-typing/","text":"Duck typing \u00b6 Knative enables loose coupling of its components by using duck typing . Duck typing means that the compatibility of a resource for use in a Knative system is determined by certain properties that are used to identify the resource control plane shape and behaviors. These properties are based on a set of common definitions for different types of resources, called duck types. Knative can use a resource as if it is the generic duck type, without specific knowledge about the resource type, if: The resource has the same fields in the same schema locations as the common definition specifies The same control or data plane behaviors as the common definition specifies Some resources can opt in to multiple duck types. A fundamental use of duck typing in Knative is using object references in resource specs to point to other resources. The definition of the object containing the reference prescribes the expected duck type of the resource being referenced. Example \u00b6 In the following example, a Knative Example resource named pointer references a Dog resource named pointee in its spec: apiVersion : sample.knative.dev/v1 kind : Example metadata : name : pointer spec : size : apiVersion : extension.example.com/v1 kind : Dog name : pointee If the expected shape of a Sizable duck type is that, in the status , the schema shape is the following: status : height : <in centimetres> weight : <in kilograms> Now the instance of pointee could look like this: apiVersion : extension.example.com/v1 kind : Dog metadata : name : pointee spec : owner : Smith Family etc : more here status : lastFeeding : 2 hours ago hungry : true age : 2 height : 60 weight : 20 When the Example resource functions, it only acts on the information in the Sizable duck type shape, and the Dog implementation is free to have the information that makes the most sense for that resource. The power of duck typing is apparent when we extend the system with a new type, for example, Human , if the new resource adheres to the contract set by Sizable. apiVersion : sample.knative.dev/v1 kind : Example metadata : name : pointer spec : size : apiVersion : people.example.com/v1 kind : human name : pointee --- apiVersion : people.example.com/v1 kind : Human metadata : name : pointee spec : etc : even more here status : college : true hungry : true age : 22 height : 170 weight : 50 The Example resource is able to apply the logic configured for it, without explicit knowledge of Human or Dog . Knative Duck Types \u00b6 Knative defines several duck type contracts that are in use across the project: Addressable Binding Channelable Podspecable Source Addressable \u00b6 Addressable is expected to be the following shape: apiVersion : group/version kind : Kind status : address : url : http://host/path?query Binding \u00b6 With a direct subject , Binding is expected to be in the following shape: apiVersion : group/version kind : Kind spec : subject : apiVersion : group/version kind : SomeKind namespace : the-namespace name : a-name With an indirect subject , Binding is expected to be in the following shape: apiVersion : group/version kind : Kind spec : subject : apiVersion : group/version kind : SomeKind namespace : the-namespace selector : matchLabels : key : value Source \u00b6 With a ref Sink, Source is expected to be in the following shape: apiVersion : group/version kind : Kind spec : sink : ref : apiVersion : group/version kind : AnAddressableKind name : a-name ceOverrides : extensions : key : value status : observedGeneration : 1 conditions : - type : Ready status : \"True\" sinkUri : http://host With a uri Sink, Source is expected to be in the following shape: apiVersion : group/version kind : Kind spec : sink : uri : http://host/path?query ceOverrides : extensions : key : value status : observedGeneration : 1 conditions : - type : Ready status : \"True\" sinkUri : http://host/path?query With ref and uri Sinks, Source is expected to be in the following shape: apiVersion : group/version kind : Kind spec : sink : ref : apiVersion : group/version kind : AnAddressableKind name : a-name uri : /path?query ceOverrides : extensions : key : value status : observedGeneration : 1 conditions : - type : Ready status : \"True\" sinkUri : http://host/path?query","title":"Duck types"},{"location":"reference/concepts/duck-typing/#duck-typing","text":"Knative enables loose coupling of its components by using duck typing . Duck typing means that the compatibility of a resource for use in a Knative system is determined by certain properties that are used to identify the resource control plane shape and behaviors. These properties are based on a set of common definitions for different types of resources, called duck types. Knative can use a resource as if it is the generic duck type, without specific knowledge about the resource type, if: The resource has the same fields in the same schema locations as the common definition specifies The same control or data plane behaviors as the common definition specifies Some resources can opt in to multiple duck types. A fundamental use of duck typing in Knative is using object references in resource specs to point to other resources. The definition of the object containing the reference prescribes the expected duck type of the resource being referenced.","title":"Duck typing"},{"location":"reference/concepts/duck-typing/#example","text":"In the following example, a Knative Example resource named pointer references a Dog resource named pointee in its spec: apiVersion : sample.knative.dev/v1 kind : Example metadata : name : pointer spec : size : apiVersion : extension.example.com/v1 kind : Dog name : pointee If the expected shape of a Sizable duck type is that, in the status , the schema shape is the following: status : height : <in centimetres> weight : <in kilograms> Now the instance of pointee could look like this: apiVersion : extension.example.com/v1 kind : Dog metadata : name : pointee spec : owner : Smith Family etc : more here status : lastFeeding : 2 hours ago hungry : true age : 2 height : 60 weight : 20 When the Example resource functions, it only acts on the information in the Sizable duck type shape, and the Dog implementation is free to have the information that makes the most sense for that resource. The power of duck typing is apparent when we extend the system with a new type, for example, Human , if the new resource adheres to the contract set by Sizable. apiVersion : sample.knative.dev/v1 kind : Example metadata : name : pointer spec : size : apiVersion : people.example.com/v1 kind : human name : pointee --- apiVersion : people.example.com/v1 kind : Human metadata : name : pointee spec : etc : even more here status : college : true hungry : true age : 22 height : 170 weight : 50 The Example resource is able to apply the logic configured for it, without explicit knowledge of Human or Dog .","title":"Example"},{"location":"reference/concepts/duck-typing/#knative-duck-types","text":"Knative defines several duck type contracts that are in use across the project: Addressable Binding Channelable Podspecable Source","title":"Knative Duck Types"},{"location":"reference/concepts/duck-typing/#addressable","text":"Addressable is expected to be the following shape: apiVersion : group/version kind : Kind status : address : url : http://host/path?query","title":"Addressable"},{"location":"reference/concepts/duck-typing/#binding","text":"With a direct subject , Binding is expected to be in the following shape: apiVersion : group/version kind : Kind spec : subject : apiVersion : group/version kind : SomeKind namespace : the-namespace name : a-name With an indirect subject , Binding is expected to be in the following shape: apiVersion : group/version kind : Kind spec : subject : apiVersion : group/version kind : SomeKind namespace : the-namespace selector : matchLabels : key : value","title":"Binding"},{"location":"reference/concepts/duck-typing/#source","text":"With a ref Sink, Source is expected to be in the following shape: apiVersion : group/version kind : Kind spec : sink : ref : apiVersion : group/version kind : AnAddressableKind name : a-name ceOverrides : extensions : key : value status : observedGeneration : 1 conditions : - type : Ready status : \"True\" sinkUri : http://host With a uri Sink, Source is expected to be in the following shape: apiVersion : group/version kind : Kind spec : sink : uri : http://host/path?query ceOverrides : extensions : key : value status : observedGeneration : 1 conditions : - type : Ready status : \"True\" sinkUri : http://host/path?query With ref and uri Sinks, Source is expected to be in the following shape: apiVersion : group/version kind : Kind spec : sink : ref : apiVersion : group/version kind : AnAddressableKind name : a-name uri : /path?query ceOverrides : extensions : key : value status : observedGeneration : 1 conditions : - type : Ready status : \"True\" sinkUri : http://host/path?query","title":"Source"},{"location":"reference/relnotes/","text":"Knative release notes \u00b6 For details about the Knative releases, see the following pages: Knative CLI releases Knative Eventing releases Knative Serving releases Knative Operator releases","title":"Knative release notes"},{"location":"reference/relnotes/#knative-release-notes","text":"For details about the Knative releases, see the following pages: Knative CLI releases Knative Eventing releases Knative Serving releases Knative Operator releases","title":"Knative release notes"},{"location":"reference/security/","text":"Knative Security and Disclosure Information \u00b6 This page describes Knative security and disclosure information. Knative threat model \u00b6 Threat model Report a vulnerability \u00b6 We're extremely grateful for security researchers and users that report vulnerabilities to the Knative Open Source Community. All reports are thoroughly investigated by a set of community volunteers. To make a report, please email the private security@knative.team list with the security detauls and the details expected for all Knative bug reports. When Should I Report a Vulnerability? \u00b6 You think you discovered a potential security vulnerability in Knative You are unsure how a vulnerability affects Knative You think you discovered a vulnerability in another project that Knative depends on For projects with their own vulnerability reporting and disclosure process, please report it directly there When Should I NOT Report a Vulnerability? \u00b6 You need help tuning Knative components for security You need help applying security related updates Your issue is not security related Vulnerability response \u00b6 Early disclosure of security vulnerabilities Vulnerability disclosure response policy Security working group \u00b6 General information Security Working Group Charter","title":"Security"},{"location":"reference/security/#knative-security-and-disclosure-information","text":"This page describes Knative security and disclosure information.","title":"Knative Security and Disclosure Information"},{"location":"reference/security/#knative-threat-model","text":"Threat model","title":"Knative threat model"},{"location":"reference/security/#report-a-vulnerability","text":"We're extremely grateful for security researchers and users that report vulnerabilities to the Knative Open Source Community. All reports are thoroughly investigated by a set of community volunteers. To make a report, please email the private security@knative.team list with the security detauls and the details expected for all Knative bug reports.","title":"Report a vulnerability"},{"location":"reference/security/#when-should-i-report-a-vulnerability","text":"You think you discovered a potential security vulnerability in Knative You are unsure how a vulnerability affects Knative You think you discovered a vulnerability in another project that Knative depends on For projects with their own vulnerability reporting and disclosure process, please report it directly there","title":"When Should I Report a Vulnerability?"},{"location":"reference/security/#when-should-i-not-report-a-vulnerability","text":"You need help tuning Knative components for security You need help applying security related updates Your issue is not security related","title":"When Should I NOT Report a Vulnerability?"},{"location":"reference/security/#vulnerability-response","text":"Early disclosure of security vulnerabilities Vulnerability disclosure response policy","title":"Vulnerability response"},{"location":"reference/security/#security-working-group","text":"General information Security Working Group Charter","title":"Security working group"},{"location":"samples/","text":"Knative code samples \u00b6 Find and use Knative code samples to help you get up and running with common use cases. Code samples include content from the Knative team and community members. Browse all code samples to find other languages and use cases that might align closer with your goals. Knative owned and maintained \u00b6 View the set of Knative code samples that are actively tested and maintained: Eventing and Eventing Sources code samples Serving code samples Community owned and maintained \u00b6 View code samples that are contributed and maintained by the community . External code samples \u00b6 A list of links to Knative code samples located outside of Knative repos: Image processing using Knative Eventing, Cloud Run on GKE (Knative Serving implementation) and Google Cloud Vision API A potpourri of Knative Eventing Examples Knfun - a complete Knative example of three functions using Twitter and Watson API that use kn to deploy and manage functions Knative Eventing (Cloud Events) example using spring-boot and spring-cloud-streams + Kafka Image processing pipeline using Knative Eventing on GKE, Google Cloud Vision API and ImageSharp library BigQuery processing pipeline using Knative Eventing on GKE, Cloud Scheduler, BigQuery, mathplotlib and SendGrid Performance (load) testing with SLO validation for Knative HTTP services Performance (load) testing with SLO validation for Knative gRPC services Please add links to your externally hosted Knative code sample.","title":"\u4ee3\u7801\u793a\u4f8b\u6982\u8ff0"},{"location":"samples/#knative-code-samples","text":"Find and use Knative code samples to help you get up and running with common use cases. Code samples include content from the Knative team and community members. Browse all code samples to find other languages and use cases that might align closer with your goals.","title":"Knative code samples"},{"location":"samples/#knative-owned-and-maintained","text":"View the set of Knative code samples that are actively tested and maintained: Eventing and Eventing Sources code samples Serving code samples","title":"Knative owned and maintained"},{"location":"samples/#community-owned-and-maintained","text":"View code samples that are contributed and maintained by the community .","title":"Community owned and maintained"},{"location":"samples/#external-code-samples","text":"A list of links to Knative code samples located outside of Knative repos: Image processing using Knative Eventing, Cloud Run on GKE (Knative Serving implementation) and Google Cloud Vision API A potpourri of Knative Eventing Examples Knfun - a complete Knative example of three functions using Twitter and Watson API that use kn to deploy and manage functions Knative Eventing (Cloud Events) example using spring-boot and spring-cloud-streams + Kafka Image processing pipeline using Knative Eventing on GKE, Google Cloud Vision API and ImageSharp library BigQuery processing pipeline using Knative Eventing on GKE, Cloud Scheduler, BigQuery, mathplotlib and SendGrid Performance (load) testing with SLO validation for Knative HTTP services Performance (load) testing with SLO validation for Knative gRPC services Please add links to your externally hosted Knative code sample.","title":"External code samples"},{"location":"samples/eventing/","text":"Knative Eventing code samples \u00b6 Use the following code samples to help you understand the various use cases for Knative Eventing and Event Sources. Learn more about Knative Eventing and Eventing Sources . See all Knative code samples in GitHub. Name Description Languages Hello World A quick introduction that highlights how to deploy an app using Knative. Go and Python CloudAuditLogsSource Configure a CloudAuditLogsSource resource to read data from Cloud Audit Logs and directly publish to the underlying transport (Pub/Sub), in CloudEvents format. YAML CloudPubSubSource Configure a CloudPubSubSource that fires a new event each time a message is published on a Cloud Pub/Sub topic. This source sends events using a Push-compatible format. YAML CloudSchedulerSource Configure a CloudSchedulerSource resource for receiving scheduled events from Google Cloud Scheduler. YAML CloudStorageSource Configure a CloudStorageSource resource to deliver Object Notifications for when a new object is added to Google Cloud Storage (GCS). YAML GitHub source Shows how to wire GitHub events for consumption by a Knative Service. YAML GitLab source Shows how to wire GitLab events for consumption by a Knative Service. YAML Apache Kafka Binding KafkaBinding is responsible for injecting Kafka bootstrap connection information into a Kubernetes resource that embed a PodSpec (as spec.template.spec ). This enables easy bootstrapping of a Kafka client. YAML Apache Kafka Channel Install and configure the Apache Kafka Channel as the default Channel configuration for Knative Eventing. YAML ResetOffset Only supported by the Distributed KafkaChannel implementation. The ResetOffset custom resource definition (CRD) exposes the ability to manipulate the location of the ConsumerGroup Offsets in the event stream of a given Knative Subscription. Without the ResetOffset CRD, you must manually stop ConsumerGroups and manipulate the Offsets. YAML Writing an event source using JavaScript This tutorial provides instructions to build an event source in JavaScript and implement it with a ContainerSource or SinkBinding. JavaScript Parallel with multiple cases Create a Parallel with two branches. YAML Parallel with mutually exclusive cases Create a Parallel with mutually exclusive branches. YAML","title":"Eventing\u793a\u4f8b\u4ee3\u7801"},{"location":"samples/eventing/#knative-eventing-code-samples","text":"Use the following code samples to help you understand the various use cases for Knative Eventing and Event Sources. Learn more about Knative Eventing and Eventing Sources . See all Knative code samples in GitHub. Name Description Languages Hello World A quick introduction that highlights how to deploy an app using Knative. Go and Python CloudAuditLogsSource Configure a CloudAuditLogsSource resource to read data from Cloud Audit Logs and directly publish to the underlying transport (Pub/Sub), in CloudEvents format. YAML CloudPubSubSource Configure a CloudPubSubSource that fires a new event each time a message is published on a Cloud Pub/Sub topic. This source sends events using a Push-compatible format. YAML CloudSchedulerSource Configure a CloudSchedulerSource resource for receiving scheduled events from Google Cloud Scheduler. YAML CloudStorageSource Configure a CloudStorageSource resource to deliver Object Notifications for when a new object is added to Google Cloud Storage (GCS). YAML GitHub source Shows how to wire GitHub events for consumption by a Knative Service. YAML GitLab source Shows how to wire GitLab events for consumption by a Knative Service. YAML Apache Kafka Binding KafkaBinding is responsible for injecting Kafka bootstrap connection information into a Kubernetes resource that embed a PodSpec (as spec.template.spec ). This enables easy bootstrapping of a Kafka client. YAML Apache Kafka Channel Install and configure the Apache Kafka Channel as the default Channel configuration for Knative Eventing. YAML ResetOffset Only supported by the Distributed KafkaChannel implementation. The ResetOffset custom resource definition (CRD) exposes the ability to manipulate the location of the ConsumerGroup Offsets in the event stream of a given Knative Subscription. Without the ResetOffset CRD, you must manually stop ConsumerGroups and manipulate the Offsets. YAML Writing an event source using JavaScript This tutorial provides instructions to build an event source in JavaScript and implement it with a ContainerSource or SinkBinding. JavaScript Parallel with multiple cases Create a Parallel with two branches. YAML Parallel with mutually exclusive cases Create a Parallel with mutually exclusive branches. YAML","title":"Knative Eventing code samples"},{"location":"samples/serving/","text":"Knative Serving code samples \u00b6 Use the following code samples to help you understand the various Knative Serving resources and how they can be applied across common use cases. Learn more about Knative Serving . See all Knative code samples in GitHub. Name Description Languages Hello World A quick introduction that highlights how to deploy an app using Knative Serving. C# , Go , Java (Spark) , Java (Spring) , Kotlin , Node.js , PHP , Python , Ruby , Scala , Shell Cloud Events A quick introduction that highlights how to send and receive Cloud Events. C# , Go , Node.js , Rust , Java (Vert.x) Traffic Splitting An example of manual traffic splitting. YAML Advanced Deployment Simple blue/green-like application deployment pattern illustrating the process of updating a live application without dropping any traffic. YAML Autoscale A demonstration of the autoscaling capabilities of Knative. Go Github Webhook A simple webhook handler that demonstrates interacting with Github. Go gRPC A simple gRPC server. Go Knative Routing An example of mapping multiple Knative services to different paths under a single domain name using the Istio VirtualService concept. Go Kong Routing An example of mapping multiple Knative services to different paths under a single domain name using the Kong API gateway. Go Knative Secrets A simple app that demonstrates how to use a Kubernetes secret as a Volume in Knative. Go Multi Container A quick introduction that highlights how to build and deploy an app using Knative Serving for multiple containers. Go","title":"Serving\u793a\u4f8b\u4ee3\u7801"},{"location":"samples/serving/#knative-serving-code-samples","text":"Use the following code samples to help you understand the various Knative Serving resources and how they can be applied across common use cases. Learn more about Knative Serving . See all Knative code samples in GitHub. Name Description Languages Hello World A quick introduction that highlights how to deploy an app using Knative Serving. C# , Go , Java (Spark) , Java (Spring) , Kotlin , Node.js , PHP , Python , Ruby , Scala , Shell Cloud Events A quick introduction that highlights how to send and receive Cloud Events. C# , Go , Node.js , Rust , Java (Vert.x) Traffic Splitting An example of manual traffic splitting. YAML Advanced Deployment Simple blue/green-like application deployment pattern illustrating the process of updating a live application without dropping any traffic. YAML Autoscale A demonstration of the autoscaling capabilities of Knative. Go Github Webhook A simple webhook handler that demonstrates interacting with Github. Go gRPC A simple gRPC server. Go Knative Routing An example of mapping multiple Knative services to different paths under a single domain name using the Istio VirtualService concept. Go Kong Routing An example of mapping multiple Knative services to different paths under a single domain name using the Kong API gateway. Go Knative Secrets A simple app that demonstrates how to use a Kubernetes secret as a Volume in Knative. Go Multi Container A quick introduction that highlights how to build and deploy an app using Knative Serving for multiple containers. Go","title":"Knative Serving code samples"},{"location":"serving/","text":"Knative Serving \u00b6 Knative Serving \u63d0\u4f9b\u652f\u6301\u4ee5\u4e0b\u529f\u80fd\u7684\u7ec4\u4ef6\uff1a \u5feb\u901f\u90e8\u7f72serverless\u5bb9\u5668\u3002 \u81ea\u52a8\u4f38\u7f29\uff0c\u5305\u62ec\u5c06 pod \u6570\u91cf\u7f29\u5c0f\u5230\u96f6\u3002 \u652f\u6301\u591a\u79cd\u7f51\u7edc\u5c42\uff0c\u4f8b\u5982 Contour\u3001Kourier \u548c Istio\uff0c\u4ee5\u96c6\u6210\u5230\u73b0\u6709\u73af\u5883\u4e2d\u3002 \u5df2\u90e8\u7f72\u4ee3\u7801\u548c\u914d\u7f6e\u7684\u65f6\u95f4\u70b9\u5feb\u7167\uff08Point-in-time snapshots\uff09\u3002 Knative Serving \u652f\u6301 HTTP \u548c HTTPS \u7f51\u7edc\u534f\u8bae\u3002 \u5b89\u88c5 \u00b6 \u60a8\u53ef\u4ee5\u901a\u8fc7 \u5b89\u88c5\u9875\u9762 \u4e0a\u5217\u51fa\u7684\u65b9\u6cd5\u5b89\u88c5 Knative Serving \u3002 Serving \u8d44\u6e90 \u00b6 Knative Serving \u5c06\u4e00\u7ec4\u5bf9\u8c61\u5b9a\u4e49\u4e3a Kubernetes \u81ea\u5b9a\u4e49\u8d44\u6e90 (Kubernetes Custom Resource Definitions\uff0cCRD)\u3002\u8fd9\u4e9b\u5bf9\u8c61\u7528\u4e8e\u5b9a\u4e49\u548c\u63a7\u5236serverless\u5de5\u4f5c\u8d1f\u8f7d\u5728\u96c6\u7fa4\u4e0a\u7684\u884c\u4e3a\u65b9\u5f0f\uff1a \u670d\u52a1 : service.serving.knative.dev \u8d44\u6e90\u81ea\u52a8\u7ba1\u7406\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6574\u4e2a\u751f\u547d\u5468\u671f\u3002\u5b83\u63a7\u5236\u5176\u4ed6\u5bf9\u8c61\u7684\u521b\u5efa\uff0c\u4ee5\u786e\u4fdd\u60a8\u7684\u5e94\u7528\u7a0b\u5e8f\u5177\u6709\u8def\u7531\u3001 \u914d\u7f6e\u548c\u6bcf\u6b21\u66f4\u65b0\u670d\u52a1\u540e\u62e5\u6709\u65b0\u7684\u7248\u672c(revision)\u3002\u53ef\u4ee5\u5c06\u670d\u52a1\u5b9a\u4e49\u4e3a\u59cb\u7ec8\u5c06\u6d41\u91cf\u8def\u7531\u5230\u6700\u65b0\u7248\u7248\u6216\u7279\u5b9a\u7248\u672c \u8def\u7531 : route.serving.knative.dev \u8d44\u6e90\u5c06\u7f51\u7edc\u7aef\u70b9(network endpoint)\u6620\u5c04\u5230\u4e00\u4e2a\u6216\u591a\u4e2a\u7248\u672c\u3002\u60a8\u53ef\u4ee5\u901a\u8fc7\u591a\u79cd\u65b9\u5f0f\u7ba1\u7406\u6d41\u91cf\uff0c \u5305\u62ec\u6309\u6bd4\u4f8b\u5206\u914d\u6d41\u91cf\uff08fractional traffic\uff09\u548c\u547d\u540d\u8def\u7531\uff08named routes\uff09\u3002 \u914d\u7f6e : configuration.serving.knative.dev \u8d44\u6e90\u4fdd\u6301\u90e8\u7f72(deployment)\u6240\u9700\u7684\u72b6\u6001\u3002\u5b83\u5728\u4ee3\u7801\u548c\u914d\u7f6e\u4e4b\u95f4\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u5206\u79bb\uff0c \u5e76\u9075\u5faa\u5341\u4e8c\u56e0\u7d20\u5e94\u7528\u65b9\u6cd5\u8bba\uff08Twelve-Factor App methodology\uff09\u3002\u4fee\u6539\u914d\u7f6e\u4f1a\u521b\u5efa\u4e00\u4e2a\u65b0\u7248\u672c\u3002 \u7248\u672c : revision.serving.knative.dev \u8d44\u6e90\u662f\u5bf9\u5de5\u4f5c\u8d1f\u8f7d\u8fdb\u884c\u7684\u6bcf\u6b21\u4fee\u6539\u7684\u4ee3\u7801\u548c\u914d\u7f6e\u7684\u65f6\u95f4\u70b9\u5feb\u7167\u3002\u7248\u672c\u662f\u4e0d\u53ef\u53d8\u7684\u5bf9\u8c61\uff0c \u53ea\u8981\u6709\u7528\u5c31\u53ef\u4ee5\u4fdd\u7559\uff08can be retained for as long as useful\uff09\u3002Knative Serving Revisions \u53ef\u4ee5\u6839\u636e\u4f20\u5165\u7684\u6d41\u91cf\u81ea\u52a8\u4f38\u7f29\u3002 \u6709\u5173\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 \u914d\u7f6e\u81ea\u52a8\u4f38\u7f29 \u3002 \u5165\u95e8 \u00b6 \u8981\u5f00\u59cb\u4f7f\u7528 Serving\uff0c\u8bf7\u67e5\u770b hello world \u793a\u4f8b\u9879\u76ee\u4e4b\u4e00 \u3002\u8fd9\u4e9b\u9879\u76ee\u4f7f\u7528Service\u8d44\u6e90\uff0c\u5b83\u4e3a\u60a8\u7ba1\u7406\u6240\u6709\u7ec6\u8282\u3002 \u901a\u8fc7\u4f7f\u7528 Service \u8d44\u6e90\uff0c\u90e8\u7f72\u7684\u670d\u52a1\u5c06\u81ea\u52a8\u521b\u5efa\u5339\u914d\u7684\u8def\u7531\u548c\u914d\u7f6e\u3002\u6bcf\u6b21 Service \u66f4\u65b0\u65f6\uff0c\u90fd\u4f1a\u521b\u5efa\u4e00\u4e2a\u65b0\u7248\u672c \u6709\u5173\u8d44\u6e90\u53ca\u5176\u4ea4\u4e92\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 Knative Serving \u4ed3\u5e93\u4e2d\u7684 \u8d44\u6e90\u7c7b\u578b\u6982\u8ff0 \u3002 \u66f4\u591a\u793a\u4f8b\u548c\u6f14\u793a \u00b6 Knative Serving \u4ee3\u7801\u793a\u4f8b \u8c03\u8bd5 Knative Serving \u95ee\u9898 \u00b6 \u8c03\u8bd5\u5e94\u7528\u7a0b\u5e8f\u95ee\u9898 \u914d\u7f6e\u548c\u7f51\u7edc \u00b6 \u914d\u7f6e\u96c6\u7fa4\u672c\u5730\u8def\u7531 \u4f7f\u7528\u81ea\u5b9a\u4e49\u57df\u540d \u6d41\u91cf\u7ba1\u7406 \u53ef\u89c2\u6d4b\u6027 \u00b6 Serving \u6307\u6807 API \u5df2\u77e5\u95ee\u9898 \u00b6 \u6709\u5173\u5df2\u77e5\u95ee\u9898\u7684\u5b8c\u6574\u5217\u8868\uff0c\u8bf7\u53c2\u9605 Knative Serving \u95ee\u9898\u9875\u9762 \u3002","title":"Knative Serving \u6982\u8ff0"},{"location":"serving/#knative-serving","text":"Knative Serving \u63d0\u4f9b\u652f\u6301\u4ee5\u4e0b\u529f\u80fd\u7684\u7ec4\u4ef6\uff1a \u5feb\u901f\u90e8\u7f72serverless\u5bb9\u5668\u3002 \u81ea\u52a8\u4f38\u7f29\uff0c\u5305\u62ec\u5c06 pod \u6570\u91cf\u7f29\u5c0f\u5230\u96f6\u3002 \u652f\u6301\u591a\u79cd\u7f51\u7edc\u5c42\uff0c\u4f8b\u5982 Contour\u3001Kourier \u548c Istio\uff0c\u4ee5\u96c6\u6210\u5230\u73b0\u6709\u73af\u5883\u4e2d\u3002 \u5df2\u90e8\u7f72\u4ee3\u7801\u548c\u914d\u7f6e\u7684\u65f6\u95f4\u70b9\u5feb\u7167\uff08Point-in-time snapshots\uff09\u3002 Knative Serving \u652f\u6301 HTTP \u548c HTTPS \u7f51\u7edc\u534f\u8bae\u3002","title":"Knative Serving"},{"location":"serving/#_1","text":"\u60a8\u53ef\u4ee5\u901a\u8fc7 \u5b89\u88c5\u9875\u9762 \u4e0a\u5217\u51fa\u7684\u65b9\u6cd5\u5b89\u88c5 Knative Serving \u3002","title":"\u5b89\u88c5"},{"location":"serving/#serving","text":"Knative Serving \u5c06\u4e00\u7ec4\u5bf9\u8c61\u5b9a\u4e49\u4e3a Kubernetes \u81ea\u5b9a\u4e49\u8d44\u6e90 (Kubernetes Custom Resource Definitions\uff0cCRD)\u3002\u8fd9\u4e9b\u5bf9\u8c61\u7528\u4e8e\u5b9a\u4e49\u548c\u63a7\u5236serverless\u5de5\u4f5c\u8d1f\u8f7d\u5728\u96c6\u7fa4\u4e0a\u7684\u884c\u4e3a\u65b9\u5f0f\uff1a \u670d\u52a1 : service.serving.knative.dev \u8d44\u6e90\u81ea\u52a8\u7ba1\u7406\u5de5\u4f5c\u8d1f\u8f7d\u7684\u6574\u4e2a\u751f\u547d\u5468\u671f\u3002\u5b83\u63a7\u5236\u5176\u4ed6\u5bf9\u8c61\u7684\u521b\u5efa\uff0c\u4ee5\u786e\u4fdd\u60a8\u7684\u5e94\u7528\u7a0b\u5e8f\u5177\u6709\u8def\u7531\u3001 \u914d\u7f6e\u548c\u6bcf\u6b21\u66f4\u65b0\u670d\u52a1\u540e\u62e5\u6709\u65b0\u7684\u7248\u672c(revision)\u3002\u53ef\u4ee5\u5c06\u670d\u52a1\u5b9a\u4e49\u4e3a\u59cb\u7ec8\u5c06\u6d41\u91cf\u8def\u7531\u5230\u6700\u65b0\u7248\u7248\u6216\u7279\u5b9a\u7248\u672c \u8def\u7531 : route.serving.knative.dev \u8d44\u6e90\u5c06\u7f51\u7edc\u7aef\u70b9(network endpoint)\u6620\u5c04\u5230\u4e00\u4e2a\u6216\u591a\u4e2a\u7248\u672c\u3002\u60a8\u53ef\u4ee5\u901a\u8fc7\u591a\u79cd\u65b9\u5f0f\u7ba1\u7406\u6d41\u91cf\uff0c \u5305\u62ec\u6309\u6bd4\u4f8b\u5206\u914d\u6d41\u91cf\uff08fractional traffic\uff09\u548c\u547d\u540d\u8def\u7531\uff08named routes\uff09\u3002 \u914d\u7f6e : configuration.serving.knative.dev \u8d44\u6e90\u4fdd\u6301\u90e8\u7f72(deployment)\u6240\u9700\u7684\u72b6\u6001\u3002\u5b83\u5728\u4ee3\u7801\u548c\u914d\u7f6e\u4e4b\u95f4\u63d0\u4f9b\u4e86\u6e05\u6670\u7684\u5206\u79bb\uff0c \u5e76\u9075\u5faa\u5341\u4e8c\u56e0\u7d20\u5e94\u7528\u65b9\u6cd5\u8bba\uff08Twelve-Factor App methodology\uff09\u3002\u4fee\u6539\u914d\u7f6e\u4f1a\u521b\u5efa\u4e00\u4e2a\u65b0\u7248\u672c\u3002 \u7248\u672c : revision.serving.knative.dev \u8d44\u6e90\u662f\u5bf9\u5de5\u4f5c\u8d1f\u8f7d\u8fdb\u884c\u7684\u6bcf\u6b21\u4fee\u6539\u7684\u4ee3\u7801\u548c\u914d\u7f6e\u7684\u65f6\u95f4\u70b9\u5feb\u7167\u3002\u7248\u672c\u662f\u4e0d\u53ef\u53d8\u7684\u5bf9\u8c61\uff0c \u53ea\u8981\u6709\u7528\u5c31\u53ef\u4ee5\u4fdd\u7559\uff08can be retained for as long as useful\uff09\u3002Knative Serving Revisions \u53ef\u4ee5\u6839\u636e\u4f20\u5165\u7684\u6d41\u91cf\u81ea\u52a8\u4f38\u7f29\u3002 \u6709\u5173\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 \u914d\u7f6e\u81ea\u52a8\u4f38\u7f29 \u3002","title":"Serving \u8d44\u6e90"},{"location":"serving/#_2","text":"\u8981\u5f00\u59cb\u4f7f\u7528 Serving\uff0c\u8bf7\u67e5\u770b hello world \u793a\u4f8b\u9879\u76ee\u4e4b\u4e00 \u3002\u8fd9\u4e9b\u9879\u76ee\u4f7f\u7528Service\u8d44\u6e90\uff0c\u5b83\u4e3a\u60a8\u7ba1\u7406\u6240\u6709\u7ec6\u8282\u3002 \u901a\u8fc7\u4f7f\u7528 Service \u8d44\u6e90\uff0c\u90e8\u7f72\u7684\u670d\u52a1\u5c06\u81ea\u52a8\u521b\u5efa\u5339\u914d\u7684\u8def\u7531\u548c\u914d\u7f6e\u3002\u6bcf\u6b21 Service \u66f4\u65b0\u65f6\uff0c\u90fd\u4f1a\u521b\u5efa\u4e00\u4e2a\u65b0\u7248\u672c \u6709\u5173\u8d44\u6e90\u53ca\u5176\u4ea4\u4e92\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 Knative Serving \u4ed3\u5e93\u4e2d\u7684 \u8d44\u6e90\u7c7b\u578b\u6982\u8ff0 \u3002","title":"\u5165\u95e8"},{"location":"serving/#_3","text":"Knative Serving \u4ee3\u7801\u793a\u4f8b","title":"\u66f4\u591a\u793a\u4f8b\u548c\u6f14\u793a"},{"location":"serving/#knative-serving_1","text":"\u8c03\u8bd5\u5e94\u7528\u7a0b\u5e8f\u95ee\u9898","title":"\u8c03\u8bd5 Knative Serving \u95ee\u9898"},{"location":"serving/#_4","text":"\u914d\u7f6e\u96c6\u7fa4\u672c\u5730\u8def\u7531 \u4f7f\u7528\u81ea\u5b9a\u4e49\u57df\u540d \u6d41\u91cf\u7ba1\u7406","title":"\u914d\u7f6e\u548c\u7f51\u7edc"},{"location":"serving/#_5","text":"Serving \u6307\u6807 API","title":"\u53ef\u89c2\u6d4b\u6027"},{"location":"serving/#_6","text":"\u6709\u5173\u5df2\u77e5\u95ee\u9898\u7684\u5b8c\u6574\u5217\u8868\uff0c\u8bf7\u53c2\u9605 Knative Serving \u95ee\u9898\u9875\u9762 \u3002","title":"\u5df2\u77e5\u95ee\u9898"},{"location":"serving/accessing-traces/","text":"Accessing request traces \u00b6 Depending on the request tracing tool that you have installed on your Knative Serving cluster, see the corresponding section for details about how to visualize and trace your requests. Configuring Traces \u00b6 You can update the configuration file for tracing in config-tracing.yaml . Follow the instructions in the file to set your configuration options. This file includes options such as sample rate (to determine what percentage of requests to trace), debug mode, and backend selection (zipkin or stackdriver). You can quickly explore and update the ConfigMap object with the following command: kubectl -n knative-serving edit configmap config-tracing Zipkin \u00b6 In order to access request traces, you use the Zipkin visualization tool. To open the Zipkin UI, enter the following command: kubectl proxy This command starts a local proxy of Zipkin on port 8001. For security reasons, the Zipkin UI is exposed only within the cluster. Access the Zipkin UI at the following URL: http://localhost:8001/api/v1/namespaces/<namespace>/services/zipkin:9411/proxy/zipkin/ Where <namespace> is the namespace where Zipkin is deployed, for example, knative-serving . 1. Click \"Find Traces\" to see the latest traces. You can search for a trace ID or look at traces of a specific application. Click on a trace to see a detailed view of a specific call. Jaeger \u00b6 In order to access request traces, you use the Jaeger visualization tool. To open the Jaeger UI, enter the following command: kubectl proxy This command starts a local proxy of Jaeger on port 8001. For security reasons, the Jaeger UI is exposed only within the cluster. Access the Jaeger UI at the following URL: http://localhost:8001/api/v1/namespaces/<namespace>/services/jaeger-query:16686/proxy/search/ Where <namespace> is the namespace where Jaeger is deployed, for example, knative-serving . Select the service of interest and click \"Find Traces\" to see the latest traces. Click on a trace to see a detailed view of a specific call.","title":"Accessing request traces"},{"location":"serving/accessing-traces/#accessing-request-traces","text":"Depending on the request tracing tool that you have installed on your Knative Serving cluster, see the corresponding section for details about how to visualize and trace your requests.","title":"Accessing request traces"},{"location":"serving/accessing-traces/#configuring-traces","text":"You can update the configuration file for tracing in config-tracing.yaml . Follow the instructions in the file to set your configuration options. This file includes options such as sample rate (to determine what percentage of requests to trace), debug mode, and backend selection (zipkin or stackdriver). You can quickly explore and update the ConfigMap object with the following command: kubectl -n knative-serving edit configmap config-tracing","title":"Configuring Traces"},{"location":"serving/accessing-traces/#zipkin","text":"In order to access request traces, you use the Zipkin visualization tool. To open the Zipkin UI, enter the following command: kubectl proxy This command starts a local proxy of Zipkin on port 8001. For security reasons, the Zipkin UI is exposed only within the cluster. Access the Zipkin UI at the following URL: http://localhost:8001/api/v1/namespaces/<namespace>/services/zipkin:9411/proxy/zipkin/ Where <namespace> is the namespace where Zipkin is deployed, for example, knative-serving . 1. Click \"Find Traces\" to see the latest traces. You can search for a trace ID or look at traces of a specific application. Click on a trace to see a detailed view of a specific call.","title":"Zipkin"},{"location":"serving/accessing-traces/#jaeger","text":"In order to access request traces, you use the Jaeger visualization tool. To open the Jaeger UI, enter the following command: kubectl proxy This command starts a local proxy of Jaeger on port 8001. For security reasons, the Jaeger UI is exposed only within the cluster. Access the Jaeger UI at the following URL: http://localhost:8001/api/v1/namespaces/<namespace>/services/jaeger-query:16686/proxy/search/ Where <namespace> is the namespace where Jaeger is deployed, for example, knative-serving . Select the service of interest and click \"Find Traces\" to see the latest traces. Click on a trace to see a detailed view of a specific call.","title":"Jaeger"},{"location":"serving/config-ha/","text":"Configuring high-availability components \u00b6 Active/passive high availability (HA) is a standard feature of Kubernetes APIs that helps to ensure that APIs stay operational if a disruption occurs. In an HA deployment, if an active controller crashes or is deleted, another controller is available to take over processing of the APIs that were being serviced by the controller that is now unavailable. When using a leader election HA pattern, instances of controllers are already scheduled and running inside the cluster before they are required. These controller instances compete to use a shared resource, known as the leader election lock. The instance of the controller that has access to the leader election lock resource at any given time is referred to as the leader. Leader election is enabled by default for all Knative Serving components. HA functionality is disabled by default for all Knative Serving components, which are configured with only one replica. Disabling leader election \u00b6 For components leveraging leader election to achieve HA, this capability can be disabled by passing the flag: --disable-ha . This option will go away when HA graduates to \"stable\". Scaling the control plane \u00b6 With the exception of the activator component you can scale up any deployment running in knative-serving (or kourier-system ) with a command like: $ kubectl -n knative-serving scale deployment <deployment-name> --replicas = 2 Setting --replicas to a value of 2 enables HA. You can use a higher value if you have a use case that requires more replicas of a deployment. For example, if you require a minimum of 3 controller deployments, set --replicas=3 . Setting --replicas=1 disables HA. Note If you scale down the Autoscaler, you may observe inaccurate autoscaling results for some Revisions for a period of time up to the stable-window value. This is because when an autoscaler pod is terminating, ownership of the revisions belonging to that pod is passed to other autoscaler pods that are on stand by. The autoscaler pods that take over ownership of those revisions use the stable-window time to build the scaling metrics state for those Revisions. Scaling the data plane \u00b6 The scale of the activator component is governed by the Kubernetes HPA component. You can see the current HPA scale limits and the current scale by running: $ kubectl get hpa activator -n knative-serving The output looks similar to the following: NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE activator Deployment/activator 2 %/100% 5 15 11 346d By default minReplicas and maxReplicas are set to 1 and 20 , correspondingly. If those values are not desirable for some reason, then, for example, you can change those values to minScale=9 and maxScale=19 using the following command: $ kubectl patch hpa activator -n knative-serving -p '{\"spec\":{\"minReplicas\":9,\"maxReplicas\":19}}' To set the activator scale to a particular value, just set minScale and maxScale to the same desired value. It is recommended for production deployments to run at least 3 activator instances for redundancy and avoiding single point of failure if a Knative service needs to be scaled from 0.","title":"Configuring high-availability components"},{"location":"serving/config-ha/#configuring-high-availability-components","text":"Active/passive high availability (HA) is a standard feature of Kubernetes APIs that helps to ensure that APIs stay operational if a disruption occurs. In an HA deployment, if an active controller crashes or is deleted, another controller is available to take over processing of the APIs that were being serviced by the controller that is now unavailable. When using a leader election HA pattern, instances of controllers are already scheduled and running inside the cluster before they are required. These controller instances compete to use a shared resource, known as the leader election lock. The instance of the controller that has access to the leader election lock resource at any given time is referred to as the leader. Leader election is enabled by default for all Knative Serving components. HA functionality is disabled by default for all Knative Serving components, which are configured with only one replica.","title":"Configuring high-availability components"},{"location":"serving/config-ha/#disabling-leader-election","text":"For components leveraging leader election to achieve HA, this capability can be disabled by passing the flag: --disable-ha . This option will go away when HA graduates to \"stable\".","title":"Disabling leader election"},{"location":"serving/config-ha/#scaling-the-control-plane","text":"With the exception of the activator component you can scale up any deployment running in knative-serving (or kourier-system ) with a command like: $ kubectl -n knative-serving scale deployment <deployment-name> --replicas = 2 Setting --replicas to a value of 2 enables HA. You can use a higher value if you have a use case that requires more replicas of a deployment. For example, if you require a minimum of 3 controller deployments, set --replicas=3 . Setting --replicas=1 disables HA. Note If you scale down the Autoscaler, you may observe inaccurate autoscaling results for some Revisions for a period of time up to the stable-window value. This is because when an autoscaler pod is terminating, ownership of the revisions belonging to that pod is passed to other autoscaler pods that are on stand by. The autoscaler pods that take over ownership of those revisions use the stable-window time to build the scaling metrics state for those Revisions.","title":"Scaling the control plane"},{"location":"serving/config-ha/#scaling-the-data-plane","text":"The scale of the activator component is governed by the Kubernetes HPA component. You can see the current HPA scale limits and the current scale by running: $ kubectl get hpa activator -n knative-serving The output looks similar to the following: NAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGE activator Deployment/activator 2 %/100% 5 15 11 346d By default minReplicas and maxReplicas are set to 1 and 20 , correspondingly. If those values are not desirable for some reason, then, for example, you can change those values to minScale=9 and maxScale=19 using the following command: $ kubectl patch hpa activator -n knative-serving -p '{\"spec\":{\"minReplicas\":9,\"maxReplicas\":19}}' To set the activator scale to a particular value, just set minScale and maxScale to the same desired value. It is recommended for production deployments to run at least 3 activator instances for redundancy and avoiding single point of failure if a Knative service needs to be scaled from 0.","title":"Scaling the data plane"},{"location":"serving/convert-deployment-to-knative-service/","text":"Converting a Kubernetes Deployment to a Knative Service \u00b6 This topic shows how to convert a Kubernetes Deployment to a Knative Service. Benefits \u00b6 Converting to a Knative Service has the following benefits: Reduces the footprint of the service instance because the instance scales to 0 when it becomes idle. Improves performance due to built-in autoscaling for the Knative Service. Determine if your workload is a good fit for Knative \u00b6 In general, if your Kubernetes workload is a good fit for Knative, you can remove a lot of your manifest to create a Knative Service. There are three aspects you need to consider: All work done is triggered by HTTP. The container is stateless. All state is stored elsewhere or can be re-created. Your workload uses only Secret and ConfigMap volumes. Example conversion \u00b6 The following example shows a Kubernetes Nginx Deployment and Service , and shows how it converts to a Knative Service. Kubernetes Nginx Deployment and Service \u00b6 apiVersion : apps/v1 kind : Deployment metadata : name : my-nginx spec : selector : matchLabels : run : my-nginx replicas : 2 template : metadata : labels : run : my-nginx spec : containers : - name : my-nginx image : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : my-nginx labels : run : my-nginx spec : ports : - port : 80 protocol : TCP selector : run : my-nginx Knative Service \u00b6 apiVersion : serving.knative.dev/v1 kind : Service metadata : name : my-nginx spec : template : spec : containers : - image : nginx ports : - containerPort : 80","title":"Converting a Kubernetes Deployment to a Knative Service"},{"location":"serving/convert-deployment-to-knative-service/#converting-a-kubernetes-deployment-to-a-knative-service","text":"This topic shows how to convert a Kubernetes Deployment to a Knative Service.","title":"Converting a Kubernetes Deployment to a Knative Service"},{"location":"serving/convert-deployment-to-knative-service/#benefits","text":"Converting to a Knative Service has the following benefits: Reduces the footprint of the service instance because the instance scales to 0 when it becomes idle. Improves performance due to built-in autoscaling for the Knative Service.","title":"Benefits"},{"location":"serving/convert-deployment-to-knative-service/#determine-if-your-workload-is-a-good-fit-for-knative","text":"In general, if your Kubernetes workload is a good fit for Knative, you can remove a lot of your manifest to create a Knative Service. There are three aspects you need to consider: All work done is triggered by HTTP. The container is stateless. All state is stored elsewhere or can be re-created. Your workload uses only Secret and ConfigMap volumes.","title":"Determine if your workload is a good fit for Knative"},{"location":"serving/convert-deployment-to-knative-service/#example-conversion","text":"The following example shows a Kubernetes Nginx Deployment and Service , and shows how it converts to a Knative Service.","title":"Example conversion"},{"location":"serving/convert-deployment-to-knative-service/#kubernetes-nginx-deployment-and-service","text":"apiVersion : apps/v1 kind : Deployment metadata : name : my-nginx spec : selector : matchLabels : run : my-nginx replicas : 2 template : metadata : labels : run : my-nginx spec : containers : - name : my-nginx image : nginx ports : - containerPort : 80 --- apiVersion : v1 kind : Service metadata : name : my-nginx labels : run : my-nginx spec : ports : - port : 80 protocol : TCP selector : run : my-nginx","title":"Kubernetes Nginx Deployment and Service"},{"location":"serving/convert-deployment-to-knative-service/#knative-service","text":"apiVersion : serving.knative.dev/v1 kind : Service metadata : name : my-nginx spec : template : spec : containers : - image : nginx ports : - containerPort : 80","title":"Knative Service"},{"location":"serving/deploying-from-private-registry/","text":"Deploying images from a private container registry \u00b6 You can share access to private container images across multiple Services and Revisions by configuring your Knative cluster to deploy images from a private container registry. To configure using a private container registry, you must: Create a list of Kubernetes secrets ( imagePullSecrets ) by using your registry credentials. Add those imagePullSecrets to the default service account . Deploy those configurations to your Knative cluster. Prerequisites \u00b6 You must have a Kubernetes cluster with Knative Serving installed. You must have access to credentials for the private container registry where your container images are stored. Procedure \u00b6 Create a imagePullSecrets object that contains your credentials as a list of secrets: kubectl create secret docker-registry <registry-credential-secrets> \\ --docker-server = <private-registry-url> \\ --docker-email = <private-registry-email> \\ --docker-username = <private-registry-user> \\ --docker-password = <private-registry-password> Where: <registry-credential-secrets> is the name that you want to use for your secrets (the imagePullSecrets object). For example, container-registry . <private-registry-url> is the URL of the private registry where your container images are stored. Examples include Google Container Registry or DockerHub . <private-registry-email> is the email address that is associated with the private registry. <private-registry-user> is the username that you use to access the private container registry. <private-registry-password> is the password that you use to access the private container registry. Example: kubectl create secret docker-registry container-registry \\ --docker-server = https://gcr.io/ \\ --docker-email = my-account-email@address.com \\ --docker-username = my-grc-username \\ --docker-password = my-gcr-password Tip After you have created the imagePullSecrets object, you can view the secrets by running: kubectl get secret <registry-credential-secrets> -o = yaml Add the imagePullSecrets to the default service account in the default namespace. Note By default, the default service account in each of the namespaces of your Knative cluster are used by your Revisions, unless the serviceAccountName is specified. For example, if have you named your secrets container-registry , you can run the following command to modify the default service account: kubectl patch serviceaccount default -p \"{\\\"imagePullSecrets\\\": [{\\\"name\\\": \\\"container-registry\\\"}]}\" New pods that are created in the default namespace now include your credentials and have access to your container images in the private registry.","title":"Deploying from private registries"},{"location":"serving/deploying-from-private-registry/#deploying-images-from-a-private-container-registry","text":"You can share access to private container images across multiple Services and Revisions by configuring your Knative cluster to deploy images from a private container registry. To configure using a private container registry, you must: Create a list of Kubernetes secrets ( imagePullSecrets ) by using your registry credentials. Add those imagePullSecrets to the default service account . Deploy those configurations to your Knative cluster.","title":"Deploying images from a private container registry"},{"location":"serving/deploying-from-private-registry/#prerequisites","text":"You must have a Kubernetes cluster with Knative Serving installed. You must have access to credentials for the private container registry where your container images are stored.","title":"Prerequisites"},{"location":"serving/deploying-from-private-registry/#procedure","text":"Create a imagePullSecrets object that contains your credentials as a list of secrets: kubectl create secret docker-registry <registry-credential-secrets> \\ --docker-server = <private-registry-url> \\ --docker-email = <private-registry-email> \\ --docker-username = <private-registry-user> \\ --docker-password = <private-registry-password> Where: <registry-credential-secrets> is the name that you want to use for your secrets (the imagePullSecrets object). For example, container-registry . <private-registry-url> is the URL of the private registry where your container images are stored. Examples include Google Container Registry or DockerHub . <private-registry-email> is the email address that is associated with the private registry. <private-registry-user> is the username that you use to access the private container registry. <private-registry-password> is the password that you use to access the private container registry. Example: kubectl create secret docker-registry container-registry \\ --docker-server = https://gcr.io/ \\ --docker-email = my-account-email@address.com \\ --docker-username = my-grc-username \\ --docker-password = my-gcr-password Tip After you have created the imagePullSecrets object, you can view the secrets by running: kubectl get secret <registry-credential-secrets> -o = yaml Add the imagePullSecrets to the default service account in the default namespace. Note By default, the default service account in each of the namespaces of your Knative cluster are used by your Revisions, unless the serviceAccountName is specified. For example, if have you named your secrets container-registry , you can run the following command to modify the default service account: kubectl patch serviceaccount default -p \"{\\\"imagePullSecrets\\\": [{\\\"name\\\": \\\"container-registry\\\"}]}\" New pods that are created in the default namespace now include your credentials and have access to your container images in the private registry.","title":"Procedure"},{"location":"serving/istio-authorization/","text":"Enabling requests to Knative services when additional authorization policies are enabled \u00b6 Knative Serving system pods, such as the activator and autoscaler components, require access to your deployed Knative services. If you have configured additional security features, such as Istio's authorization policy, you must enable access to your Knative service for these system pods. Before you begin \u00b6 You must meet the following prerequisites to use Istio AuthorizationPolicy: Istio must be used for your Knative Ingress. See Install a networking layer . Istio sidecar injection must be enabled. See the Istio Documentation . Mutual TLS in Knative \u00b6 Because Knative requests are frequently routed through activator, some considerations need to be made when using mutual TLS. Generally, mutual TLS can be configured normally as in Istio's documentation . However, since the activator can be in the request path of Knative services, it must have sidecars injected. The simplest way to do this is to label the knative-serving namespace: kubectl label namespace knative-serving istio-injection = enabled If the activator isn't injected: In PERMISSIVE mode, you'll see requests appear without the expected X-Forwarded-Client-Cert header when forwarded by the activator. $ kubectl exec deployment/httpbin -c httpbin -it -- curl -s http://httpbin.knative.svc.cluster.local/headers { \"headers\" : { \"Accept\" : \"*/*\" , \"Accept-Encoding\" : \"gzip\" , \"Forwarded\" : \"for=10.72.0.30;proto=http\" , \"Host\" : \"httpbin.knative.svc.cluster.local\" , \"K-Proxy-Request\" : \"activator\" , \"User-Agent\" : \"curl/7.58.0\" , \"X-B3-Parentspanid\" : \"b240bdb1c29ae638\" , \"X-B3-Sampled\" : \"0\" , \"X-B3-Spanid\" : \"416960c27be6d484\" , \"X-B3-Traceid\" : \"750362ce9d878281b240bdb1c29ae638\" , \"X-Envoy-Attempt-Count\" : \"1\" , \"X-Envoy-Internal\" : \"true\" } } In STRICT mode, requests will simply be rejected. To understand when requests are forwarded through the activator, see the target burst capacity documentation. This also means that many Istio AuthorizationPolicies won't work as expected. For example, if you set up a rule allowing requests from a particular source into a Knative service, you will see requests being rejected if they are forwarded by the activator. For example, the following policy allows requests from within pods in the serving-tests namespace to other pods in the serving-tests namespace. apiVersion : security.istio.io/v1beta1 kind : AuthorizationPolicy metadata : name : allow-serving-tests namespace : serving-tests spec : action : ALLOW rules : - from : - source : namespaces : [ \"serving-tests\" ] Requests here will fail when forwarded by the activator, because the Istio proxy at the destination service will see the source namespace of the requests as knative-serving , which is the namespace of the activator. Currently, the easiest way around this is to explicitly allow requests from the knative-serving namespace, for example by adding it to the list in the policy mentioned earlier: apiVersion : security.istio.io/v1beta1 kind : AuthorizationPolicy metadata : name : allow-serving-tests namespace : serving-tests spec : action : ALLOW rules : - from : - source : namespaces : [ \"serving-tests\" , \"knative-serving\" ] Health checking and metrics collection \u00b6 In addition to allowing your application path, you'll need to configure Istio AuthorizationPolicy to allow health checking and metrics collection to your applications from system pods. You can allow access from system pods by paths. Allowing access from system pods by paths \u00b6 Knative system pods access your application using the following paths: /metrics /healthz The /metrics path allows the autoscaler pod to collect metrics. The /healthz path allows system pods to probe the service. To add the /metrics and /healthz paths to the AuthorizationPolicy: Create a YAML file for your AuthorizationPolicy using the following example: apiVersion : security.istio.io/v1beta1 kind : AuthorizationPolicy metadata : name : allowlist-by-paths namespace : serving-tests spec : action : ALLOW rules : - to : - operation : paths : - /metrics # The path to collect metrics by system pod. - /healthz # The path to probe by system pod. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"Enabling requests to Knative services when additional authorization policies are enabled"},{"location":"serving/istio-authorization/#enabling-requests-to-knative-services-when-additional-authorization-policies-are-enabled","text":"Knative Serving system pods, such as the activator and autoscaler components, require access to your deployed Knative services. If you have configured additional security features, such as Istio's authorization policy, you must enable access to your Knative service for these system pods.","title":"Enabling requests to Knative services when additional authorization policies are enabled"},{"location":"serving/istio-authorization/#before-you-begin","text":"You must meet the following prerequisites to use Istio AuthorizationPolicy: Istio must be used for your Knative Ingress. See Install a networking layer . Istio sidecar injection must be enabled. See the Istio Documentation .","title":"Before you begin"},{"location":"serving/istio-authorization/#mutual-tls-in-knative","text":"Because Knative requests are frequently routed through activator, some considerations need to be made when using mutual TLS. Generally, mutual TLS can be configured normally as in Istio's documentation . However, since the activator can be in the request path of Knative services, it must have sidecars injected. The simplest way to do this is to label the knative-serving namespace: kubectl label namespace knative-serving istio-injection = enabled If the activator isn't injected: In PERMISSIVE mode, you'll see requests appear without the expected X-Forwarded-Client-Cert header when forwarded by the activator. $ kubectl exec deployment/httpbin -c httpbin -it -- curl -s http://httpbin.knative.svc.cluster.local/headers { \"headers\" : { \"Accept\" : \"*/*\" , \"Accept-Encoding\" : \"gzip\" , \"Forwarded\" : \"for=10.72.0.30;proto=http\" , \"Host\" : \"httpbin.knative.svc.cluster.local\" , \"K-Proxy-Request\" : \"activator\" , \"User-Agent\" : \"curl/7.58.0\" , \"X-B3-Parentspanid\" : \"b240bdb1c29ae638\" , \"X-B3-Sampled\" : \"0\" , \"X-B3-Spanid\" : \"416960c27be6d484\" , \"X-B3-Traceid\" : \"750362ce9d878281b240bdb1c29ae638\" , \"X-Envoy-Attempt-Count\" : \"1\" , \"X-Envoy-Internal\" : \"true\" } } In STRICT mode, requests will simply be rejected. To understand when requests are forwarded through the activator, see the target burst capacity documentation. This also means that many Istio AuthorizationPolicies won't work as expected. For example, if you set up a rule allowing requests from a particular source into a Knative service, you will see requests being rejected if they are forwarded by the activator. For example, the following policy allows requests from within pods in the serving-tests namespace to other pods in the serving-tests namespace. apiVersion : security.istio.io/v1beta1 kind : AuthorizationPolicy metadata : name : allow-serving-tests namespace : serving-tests spec : action : ALLOW rules : - from : - source : namespaces : [ \"serving-tests\" ] Requests here will fail when forwarded by the activator, because the Istio proxy at the destination service will see the source namespace of the requests as knative-serving , which is the namespace of the activator. Currently, the easiest way around this is to explicitly allow requests from the knative-serving namespace, for example by adding it to the list in the policy mentioned earlier: apiVersion : security.istio.io/v1beta1 kind : AuthorizationPolicy metadata : name : allow-serving-tests namespace : serving-tests spec : action : ALLOW rules : - from : - source : namespaces : [ \"serving-tests\" , \"knative-serving\" ]","title":"Mutual TLS in Knative"},{"location":"serving/istio-authorization/#health-checking-and-metrics-collection","text":"In addition to allowing your application path, you'll need to configure Istio AuthorizationPolicy to allow health checking and metrics collection to your applications from system pods. You can allow access from system pods by paths.","title":"Health checking and metrics collection"},{"location":"serving/istio-authorization/#allowing-access-from-system-pods-by-paths","text":"Knative system pods access your application using the following paths: /metrics /healthz The /metrics path allows the autoscaler pod to collect metrics. The /healthz path allows system pods to probe the service. To add the /metrics and /healthz paths to the AuthorizationPolicy: Create a YAML file for your AuthorizationPolicy using the following example: apiVersion : security.istio.io/v1beta1 kind : AuthorizationPolicy metadata : name : allowlist-by-paths namespace : serving-tests spec : action : ALLOW rules : - to : - operation : paths : - /metrics # The path to collect metrics by system pod. - /healthz # The path to probe by system pod. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"Allowing access from system pods by paths"},{"location":"serving/knative-kubernetes-services/","text":"Kubernetes services \u00b6 This guide describes the Kubernetes Services that are active when running Knative Serving. Before You Begin \u00b6 This guide assumes that you have installed Knative Serving . Verify that you have the proper components in your cluster. To view the services installed in your cluster, use the command: $ kubectl get services -n knative-serving This returns an output similar to the following: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE activator-service ClusterIP 10 .96.61.11 <none> 80 /TCP,81/TCP,9090/TCP 1h autoscaler ClusterIP 10 .104.217.223 <none> 8080 /TCP,9090/TCP 1h controller ClusterIP 10 .101.39.220 <none> 9090 /TCP 1h webhook ClusterIP 10 .107.144.50 <none> 443 /TCP 1h To view the deployments in your cluster, use the following command: $ kubectl get deployments -n knative-serving This returns an output similar to the following: NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE activator 1 1 1 1 1h autoscaler 1 1 1 1 1h controller 1 1 1 1 1h net-certmanager-controller 1 1 1 1 1h net-istio-controller 1 1 1 1 1h webhook 1 1 1 1 1h These services and deployments are installed by the serving.yaml file during install. The next section describes their function. Components \u00b6 Service: activator \u00b6 The activator is responsible for receiving & buffering requests for inactive revisions and reporting metrics to the autoscaler. It also retries requests to a revision after the autoscaler scales the revision based on the reported metrics. Service: autoscaler \u00b6 The autoscaler receives request metrics and adjusts the number of pods required to handle the load of traffic. Service: controller \u00b6 The controller service reconciles all the public Knative objects and autoscaling CRDs. When a user applies a Knative service to the Kubernetes API, this creates the configuration and route. It will convert the configuration into revisions and the revisions into deployments and Knative Pod Autoscalers (KPAs). Service: webhook \u00b6 The webhook intercepts all Kubernetes API calls as well as all CRD insertions and updates. It sets default values, rejects inconsitent and invalid objects, and validates and mutates Kubernetes API calls. Deployment: net-certmanager-controller \u00b6 The certmanager reconciles cluster ingresses into cert manager objects. Deployment: net-istio-controller \u00b6 The net-istio-controller deployment reconciles a cluster's ingress into an Istio virtual service . What's Next \u00b6 For a deeper look at the services and deployments involved in Knative Serving, see the specs repository. For a high-level analysis of Knative Serving, see the Knative Serving overview . For hands-on tutorials, see the Knative Serving code samples .","title":"Kubernetes services"},{"location":"serving/knative-kubernetes-services/#kubernetes-services","text":"This guide describes the Kubernetes Services that are active when running Knative Serving.","title":"Kubernetes services"},{"location":"serving/knative-kubernetes-services/#before-you-begin","text":"This guide assumes that you have installed Knative Serving . Verify that you have the proper components in your cluster. To view the services installed in your cluster, use the command: $ kubectl get services -n knative-serving This returns an output similar to the following: NAME TYPE CLUSTER-IP EXTERNAL-IP PORT ( S ) AGE activator-service ClusterIP 10 .96.61.11 <none> 80 /TCP,81/TCP,9090/TCP 1h autoscaler ClusterIP 10 .104.217.223 <none> 8080 /TCP,9090/TCP 1h controller ClusterIP 10 .101.39.220 <none> 9090 /TCP 1h webhook ClusterIP 10 .107.144.50 <none> 443 /TCP 1h To view the deployments in your cluster, use the following command: $ kubectl get deployments -n knative-serving This returns an output similar to the following: NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE activator 1 1 1 1 1h autoscaler 1 1 1 1 1h controller 1 1 1 1 1h net-certmanager-controller 1 1 1 1 1h net-istio-controller 1 1 1 1 1h webhook 1 1 1 1 1h These services and deployments are installed by the serving.yaml file during install. The next section describes their function.","title":"Before You Begin"},{"location":"serving/knative-kubernetes-services/#components","text":"","title":"Components"},{"location":"serving/knative-kubernetes-services/#service-activator","text":"The activator is responsible for receiving & buffering requests for inactive revisions and reporting metrics to the autoscaler. It also retries requests to a revision after the autoscaler scales the revision based on the reported metrics.","title":"Service: activator"},{"location":"serving/knative-kubernetes-services/#service-autoscaler","text":"The autoscaler receives request metrics and adjusts the number of pods required to handle the load of traffic.","title":"Service: autoscaler"},{"location":"serving/knative-kubernetes-services/#service-controller","text":"The controller service reconciles all the public Knative objects and autoscaling CRDs. When a user applies a Knative service to the Kubernetes API, this creates the configuration and route. It will convert the configuration into revisions and the revisions into deployments and Knative Pod Autoscalers (KPAs).","title":"Service: controller"},{"location":"serving/knative-kubernetes-services/#service-webhook","text":"The webhook intercepts all Kubernetes API calls as well as all CRD insertions and updates. It sets default values, rejects inconsitent and invalid objects, and validates and mutates Kubernetes API calls.","title":"Service: webhook"},{"location":"serving/knative-kubernetes-services/#deployment-net-certmanager-controller","text":"The certmanager reconciles cluster ingresses into cert manager objects.","title":"Deployment: net-certmanager-controller"},{"location":"serving/knative-kubernetes-services/#deployment-net-istio-controller","text":"The net-istio-controller deployment reconciles a cluster's ingress into an Istio virtual service .","title":"Deployment: net-istio-controller"},{"location":"serving/knative-kubernetes-services/#whats-next","text":"For a deeper look at the services and deployments involved in Knative Serving, see the specs repository. For a high-level analysis of Knative Serving, see the Knative Serving overview . For hands-on tutorials, see the Knative Serving code samples .","title":"What's Next"},{"location":"serving/revision-gc/","text":"Revision garbage collection \u00b6 Knative automatically cleans up inactive revisions as configured by the Operator. For more information, see the Operator settings . You can configure a revision so that it is never garbage collected by adding the serving.knative.dev/no-gc: \"true\" annotation: apiVersion : serving.knative.dev/v1 kind : Revision metadata : annotations : serving.knative.dev/no-gc : \"true\" spec : ...","title":"Revision garbage collection"},{"location":"serving/revision-gc/#revision-garbage-collection","text":"Knative automatically cleans up inactive revisions as configured by the Operator. For more information, see the Operator settings . You can configure a revision so that it is never garbage collected by adding the serving.knative.dev/no-gc: \"true\" annotation: apiVersion : serving.knative.dev/v1 kind : Revision metadata : annotations : serving.knative.dev/no-gc : \"true\" spec : ...","title":"Revision garbage collection"},{"location":"serving/rolling-out-latest-revision/","text":"Configuring gradual rollout of traffic to Revisions \u00b6 If your traffic configuration points to a Configuration target instead of a Revision target, when a new Revision is created and ready, 100% of the traffic from the target is immediately shifted to the new Revision. This might make the request queue too long, either at the QP or Activator, and cause the requests to expire or be rejected by the QP. Knative provides a rollout-duration parameter, which can be used to gradually shift traffic to the latest Revision, preventing requests from being queued or rejected. Affected Configuration targets are rolled out to 1% of traffic first, and then in equal incremental steps for the rest of the assigned traffic. Note rollout-duration is time-based, and does not interact with the autoscaling subsystem. This feature is available for tagged and untagged traffic targets, configured for either Knative Services or Routes without a service. Procedure \u00b6 You can configure the rollout-duration parameter per Knative Service or Route by using an annotation. Tip For information about global, ConfigMap configurations for rollout durations, see the Administration guide . apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default annotations : serving.knative.dev/rollout-duration : \"380s\" Route status updates \u00b6 During a rollout, the system updates the Route and Knative Service status conditions. Both the traffic and conditions status parameters are affected. For example, for the following traffic configuration: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 55 configurationName : config # Pinned to latest ready Revision - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. Initially 1% of the traffic is rolled out to the Revisions: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 54 revisionName : config-00008 - percent : 1 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. Then the rest of the traffic is rolled out in increments of 18%: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 36 revisionName : config-00008 - percent : 19 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. The rollout continues until the target traffic configuration is reached: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 55 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. During the rollout, the Route and Knative Service status conditions are as follows: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... status : conditions : ... - lastTransitionTime : \"...\" message : A gradual rollout of the latest revision(s) is in progress. reason : RolloutInProgress status : Unknown type : Ready Multiple rollouts \u00b6 If a new revision is created while a rollout is in progress, the system begins to shift traffic immediately to the newest Revision, and drains the incomplete rollouts from newest to oldest.","title":"Configuring gradual rollout of traffic to Revisions"},{"location":"serving/rolling-out-latest-revision/#configuring-gradual-rollout-of-traffic-to-revisions","text":"If your traffic configuration points to a Configuration target instead of a Revision target, when a new Revision is created and ready, 100% of the traffic from the target is immediately shifted to the new Revision. This might make the request queue too long, either at the QP or Activator, and cause the requests to expire or be rejected by the QP. Knative provides a rollout-duration parameter, which can be used to gradually shift traffic to the latest Revision, preventing requests from being queued or rejected. Affected Configuration targets are rolled out to 1% of traffic first, and then in equal incremental steps for the rest of the assigned traffic. Note rollout-duration is time-based, and does not interact with the autoscaling subsystem. This feature is available for tagged and untagged traffic targets, configured for either Knative Services or Routes without a service.","title":"Configuring gradual rollout of traffic to Revisions"},{"location":"serving/rolling-out-latest-revision/#procedure","text":"You can configure the rollout-duration parameter per Knative Service or Route by using an annotation. Tip For information about global, ConfigMap configurations for rollout durations, see the Administration guide . apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default annotations : serving.knative.dev/rollout-duration : \"380s\"","title":"Procedure"},{"location":"serving/rolling-out-latest-revision/#route-status-updates","text":"During a rollout, the system updates the Route and Knative Service status conditions. Both the traffic and conditions status parameters are affected. For example, for the following traffic configuration: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 55 configurationName : config # Pinned to latest ready Revision - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. Initially 1% of the traffic is rolled out to the Revisions: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 54 revisionName : config-00008 - percent : 1 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. Then the rest of the traffic is rolled out in increments of 18%: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 36 revisionName : config-00008 - percent : 19 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. The rollout continues until the target traffic configuration is reached: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 55 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. During the rollout, the Route and Knative Service status conditions are as follows: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... status : conditions : ... - lastTransitionTime : \"...\" message : A gradual rollout of the latest revision(s) is in progress. reason : RolloutInProgress status : Unknown type : Ready","title":"Route status updates"},{"location":"serving/rolling-out-latest-revision/#multiple-rollouts","text":"If a new revision is created while a rollout is in progress, the system begins to shift traffic immediately to the newest Revision, and drains the incomplete rollouts from newest to oldest.","title":"Multiple rollouts"},{"location":"serving/setting-up-custom-ingress-gateway/","text":"Configuring the ingress gateway \u00b6 Knative uses a shared ingress Gateway to serve all incoming traffic within Knative service mesh, which is the knative-ingress-gateway Gateway under the knative-serving namespace. By default, we use Istio gateway service istio-ingressgateway under istio-system namespace as its underlying service. You can replace the service and the gateway with that of your own as follows. Replace the default istio-ingressgateway service \u00b6 Step 1: Create the gateway service and deployment instance \u00b6 You'll need to create the gateway service and deployment instance to handle traffic first. Let's say you customized the default istio-ingressgateway to custom-ingressgateway as follows. apiVersion : install.istio.io/v1alpha1 kind : IstioOperator spec : components : ingressGateways : - name : custom-ingressgateway enabled : true namespace : custom-ns label : istio : custom-gateway Step 2: Update the Knative gateway \u00b6 Update gateway instance knative-ingress-gateway under knative-serving namespace: kubectl edit gateway knative-ingress-gateway -n knative-serving Replace the label selector with the label of your service: istio: ingressgateway For the example custom-ingressgateway service mentioned earlier, it should be updated to: istio: custom-gateway If there is a change in service ports (compared with that of istio-ingressgateway ), update the port info in the gateway accordingly. Step 3: Update the gateway ConfigMap \u00b6 Update gateway configmap config-istio under knative-serving namespace: kubectl edit configmap config-istio -n knative-serving This command opens your default text editor and allows you to edit the config-istio ConfigMap. apiVersion : v1 data : _example : | ################################ # # # EXAMPLE CONFIGURATION # # # ################################ # ... gateway.knative-serving.knative-ingress-gateway: \"istio-ingressgateway.istio-system.svc.cluster.local\" Edit the file to add the gateway.knative-serving.knative-ingress-gateway: <ingress_name>.<ingress_namespace>.svc.cluster.local field with the fully qualified url of your service. For the example custom-ingressgateway service mentioned earlier, it should be updated to: apiVersion : v1 data : gateway.knative-serving.knative-ingress-gateway : custom-ingressgateway.custom-ns.svc.cluster.local kind : ConfigMap [ ... ] Replace the knative-ingress-gateway gateway \u00b6 We customized the gateway service so far, but we may also want to use our own gateway. We can replace the default gateway with our own gateway with following steps. Step 1: Create the gateway \u00b6 Let's say you replace the default knative-ingress-gateway gateway with knative-custom-gateway in custom-ns . First, create the knative-custom-gateway gateway: Create a YAML file using the following template: apiVersion : networking.istio.io/v1alpha3 kind : Gateway metadata : name : knative-custom-gateway namespace : custom-ns spec : selector : istio : <service-label> servers : - port : number : 80 name : http protocol : HTTP hosts : - \"*\" Where <service-label> is a label to select your service, for example, ingressgateway . Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Step 2: Update the gateway ConfigMap \u00b6 Update gateway configmap config-istio under knative-serving namespace: kubectl edit configmap config-istio -n knative-serving This command opens your default text editor and allows you to edit the config-istio ConfigMap. apiVersion : v1 data : _example : | ################################ # # # EXAMPLE CONFIGURATION # # # ################################ # ... gateway.knative-serving.knative-ingress-gateway: \"istio-ingressgateway.istio-system.svc.cluster.local\" Edit the file to add the gateway.<gateway-namespace>.<gateway-name>: istio-ingressgateway.istio-system.svc.cluster.local field with the customized gateway. For the example knative-custom-gateway mentioned earlier, it should be updated to: apiVersion : v1 data : gateway.custom-ns.knative-custom-gateway : \"istio-ingressgateway.istio-system.svc.cluster.local\" kind : ConfigMap [ ... ] The configuration format should be gateway.<gateway-namespace>.<gateway-name> . <gateway-namespace> is optional. When it is omitted, the system searches for the gateway in the serving system namespace knative-serving .","title":"Configuring the ingress gateway"},{"location":"serving/setting-up-custom-ingress-gateway/#configuring-the-ingress-gateway","text":"Knative uses a shared ingress Gateway to serve all incoming traffic within Knative service mesh, which is the knative-ingress-gateway Gateway under the knative-serving namespace. By default, we use Istio gateway service istio-ingressgateway under istio-system namespace as its underlying service. You can replace the service and the gateway with that of your own as follows.","title":"Configuring the ingress gateway"},{"location":"serving/setting-up-custom-ingress-gateway/#replace-the-default-istio-ingressgateway-service","text":"","title":"Replace the default istio-ingressgateway service"},{"location":"serving/setting-up-custom-ingress-gateway/#step-1-create-the-gateway-service-and-deployment-instance","text":"You'll need to create the gateway service and deployment instance to handle traffic first. Let's say you customized the default istio-ingressgateway to custom-ingressgateway as follows. apiVersion : install.istio.io/v1alpha1 kind : IstioOperator spec : components : ingressGateways : - name : custom-ingressgateway enabled : true namespace : custom-ns label : istio : custom-gateway","title":"Step 1: Create the gateway service and deployment instance"},{"location":"serving/setting-up-custom-ingress-gateway/#step-2-update-the-knative-gateway","text":"Update gateway instance knative-ingress-gateway under knative-serving namespace: kubectl edit gateway knative-ingress-gateway -n knative-serving Replace the label selector with the label of your service: istio: ingressgateway For the example custom-ingressgateway service mentioned earlier, it should be updated to: istio: custom-gateway If there is a change in service ports (compared with that of istio-ingressgateway ), update the port info in the gateway accordingly.","title":"Step 2: Update the Knative gateway"},{"location":"serving/setting-up-custom-ingress-gateway/#step-3-update-the-gateway-configmap","text":"Update gateway configmap config-istio under knative-serving namespace: kubectl edit configmap config-istio -n knative-serving This command opens your default text editor and allows you to edit the config-istio ConfigMap. apiVersion : v1 data : _example : | ################################ # # # EXAMPLE CONFIGURATION # # # ################################ # ... gateway.knative-serving.knative-ingress-gateway: \"istio-ingressgateway.istio-system.svc.cluster.local\" Edit the file to add the gateway.knative-serving.knative-ingress-gateway: <ingress_name>.<ingress_namespace>.svc.cluster.local field with the fully qualified url of your service. For the example custom-ingressgateway service mentioned earlier, it should be updated to: apiVersion : v1 data : gateway.knative-serving.knative-ingress-gateway : custom-ingressgateway.custom-ns.svc.cluster.local kind : ConfigMap [ ... ]","title":"Step 3: Update the gateway ConfigMap"},{"location":"serving/setting-up-custom-ingress-gateway/#replace-the-knative-ingress-gateway-gateway","text":"We customized the gateway service so far, but we may also want to use our own gateway. We can replace the default gateway with our own gateway with following steps.","title":"Replace the knative-ingress-gateway gateway"},{"location":"serving/setting-up-custom-ingress-gateway/#step-1-create-the-gateway","text":"Let's say you replace the default knative-ingress-gateway gateway with knative-custom-gateway in custom-ns . First, create the knative-custom-gateway gateway: Create a YAML file using the following template: apiVersion : networking.istio.io/v1alpha3 kind : Gateway metadata : name : knative-custom-gateway namespace : custom-ns spec : selector : istio : <service-label> servers : - port : number : 80 name : http protocol : HTTP hosts : - \"*\" Where <service-label> is a label to select your service, for example, ingressgateway . Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step.","title":"Step 1: Create the gateway"},{"location":"serving/setting-up-custom-ingress-gateway/#step-2-update-the-gateway-configmap","text":"Update gateway configmap config-istio under knative-serving namespace: kubectl edit configmap config-istio -n knative-serving This command opens your default text editor and allows you to edit the config-istio ConfigMap. apiVersion : v1 data : _example : | ################################ # # # EXAMPLE CONFIGURATION # # # ################################ # ... gateway.knative-serving.knative-ingress-gateway: \"istio-ingressgateway.istio-system.svc.cluster.local\" Edit the file to add the gateway.<gateway-namespace>.<gateway-name>: istio-ingressgateway.istio-system.svc.cluster.local field with the customized gateway. For the example knative-custom-gateway mentioned earlier, it should be updated to: apiVersion : v1 data : gateway.custom-ns.knative-custom-gateway : \"istio-ingressgateway.istio-system.svc.cluster.local\" kind : ConfigMap [ ... ] The configuration format should be gateway.<gateway-namespace>.<gateway-name> . <gateway-namespace> is optional. When it is omitted, the system searches for the gateway in the serving system namespace knative-serving .","title":"Step 2: Update the gateway ConfigMap"},{"location":"serving/tag-resolution/","text":"Tag resolution \u00b6 Knative Serving resolves image tags to a digest when you create a Revision. This helps to provide consistency for Deployments. For more information, see the documentation on Why we resolve tags in Knative . Important The Knative Serving controller must be configured to access the container registry to use this feature. Custom certificates \u00b6 If you are using a registry that has a self-signed certificate, you must configure the Knative Serving controller to trust that certificate. Knative Serving accepts the SSL_CERT_FILE and SSL_CERT_DIR environment variables. You can configure trusting certificates by mounting your certificates into the controller Deployment, and then setting the environment variable appropriately. For example, if you are using a custom-certs secret that contains your CA certificates, the Deployment object is as follows: apiVersion : apps/v1 kind : Deployment metadata : name : controller namespace : knative-serving spec : template : spec : containers : - name : controller volumeMounts : - name : custom-certs mountPath : /path/to/custom/certs env : - name : SSL_CERT_DIR value : /path/to/custom/certs volumes : - name : custom-certs secret : secretName : custom-certs Corporate proxy \u00b6 If you are behind a corporate proxy, you must proxy the tag resolution requests between the controller and your registry. Knative accepts the HTTP_PROXY and HTTPS_PROXY environment variables, so you can configure the controller Deployment as follows: apiVersion : apps/v1 kind : Deployment metadata : name : controller namespace : knative-serving spec : template : spec : containers : - name : controller env : - name : HTTP_PROXY value : http://proxy.example.com - name : HTTPS_PROXY value : https://proxy.example.com","title":"Tag resolution"},{"location":"serving/tag-resolution/#tag-resolution","text":"Knative Serving resolves image tags to a digest when you create a Revision. This helps to provide consistency for Deployments. For more information, see the documentation on Why we resolve tags in Knative . Important The Knative Serving controller must be configured to access the container registry to use this feature.","title":"Tag resolution"},{"location":"serving/tag-resolution/#custom-certificates","text":"If you are using a registry that has a self-signed certificate, you must configure the Knative Serving controller to trust that certificate. Knative Serving accepts the SSL_CERT_FILE and SSL_CERT_DIR environment variables. You can configure trusting certificates by mounting your certificates into the controller Deployment, and then setting the environment variable appropriately. For example, if you are using a custom-certs secret that contains your CA certificates, the Deployment object is as follows: apiVersion : apps/v1 kind : Deployment metadata : name : controller namespace : knative-serving spec : template : spec : containers : - name : controller volumeMounts : - name : custom-certs mountPath : /path/to/custom/certs env : - name : SSL_CERT_DIR value : /path/to/custom/certs volumes : - name : custom-certs secret : secretName : custom-certs","title":"Custom certificates"},{"location":"serving/tag-resolution/#corporate-proxy","text":"If you are behind a corporate proxy, you must proxy the tag resolution requests between the controller and your registry. Knative accepts the HTTP_PROXY and HTTPS_PROXY environment variables, so you can configure the controller Deployment as follows: apiVersion : apps/v1 kind : Deployment metadata : name : controller namespace : knative-serving spec : template : spec : containers : - name : controller env : - name : HTTP_PROXY value : http://proxy.example.com - name : HTTPS_PROXY value : https://proxy.example.com","title":"Corporate proxy"},{"location":"serving/traffic-management/","text":"Traffic management \u00b6 You can manage traffic routing to different Revisions of a Knative Service by modifying the traffic spec of the Service resource. When you create a Knative Service, it does not have any default traffic spec settings. By setting the traffic spec, you can split traffic over any number of fixed Revisions, or send traffic to the latest Revision by setting latestRevision: true in the spec for a Service. Using tags to create target URLs \u00b6 In the following example, the spec defines an attribute called tag : apiVersion : serving.knative.dev/v1 kind : Service metadata : name : example-service namespace : default spec : ... traffic : - percent : 0 revisionName : example-service-1 tag : staging - percent : 40 revisionName : example-service-2 - percent : 60 revisionName : example-service-3 When a tag attribute is applied to a Route, an address for the specific traffic target is created. In this example, you can access the staging target by accessing staging-<route name>.<namespace>.<domain> . The targets for example-service-2 and example-service-3 can only be accessed using the main route, <route name>.<namespace>.<domain> . When a traffic target is tagged, a new Kubernetes Service is created for that Service, so that other Services can access it within the cluster. From the previous example, a new Kubernetes Service called staging-<route name> will be created in the same namespace. This Service has the ability to override the visibility of this specific Route by applying the label networking.knative.dev/visibility with value cluster-local . See the documentation on private services for more information about how to restrict visibility on specific Routes. Traffic routing examples \u00b6 The following example shows a traffic spec where 100% of traffic is routed to the latestRevision of the Service. Under status you can see the name of the latest Revision that latestRevision was resolved to: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : example-service namespace : default spec : ... traffic : - latestRevision : true percent : 100 status : ... traffic : - percent : 100 revisionName : example-service-1 The following example shows a traffic spec where 100% of traffic is routed to the current Revision, and the name of that Revision is specified as example-service-1 . The latest ready Revision is kept available, even though no traffic is being routed to it: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : example-service namespace : default spec : ... traffic : - tag : current revisionName : example-service-1 percent : 100 - tag : latest latestRevision : true percent : 0 The following example shows how the list of Revisions in the traffic spec can be extended so that traffic is split between multiple Revisions. This example sends 50% of traffic to the current Revision, example-service-1 , and 50% of traffic to the candidate Revision, example-service-2 : apiVersion : serving.knative.dev/v1 kind : Service metadata : name : example-service namespace : default spec : ... traffic : - tag : current revisionName : example-service-1 percent : 50 - tag : candidate revisionName : example-service-2 percent : 50 - tag : latest latestRevision : true percent : 0 Routing and managing traffic by using the Knative CLI \u00b6 You can use the following kn CLI command to split traffic between revisions: kn service update <service-name> --traffic <revision-name> = <percent> Where: <service-name> is the name of the Knative Service that you are configuring traffic routing for. <revision-name> is the name of the revision that you want to configure to receive a percentage of traffic. <percent> is the percentage of traffic that you want to send to the revision specified by <revision-name> . For example, to split traffic for a Service named example , by sending 80% of traffic to the Revision green and 20% of traffic to the Revision blue , you could run the following command: kn service update example-service --traffic green = 80 --traffic blue = 20 It is also possible to add tags to Revisions and then split traffic according to the tags you have set: kn service update example --tag green = revision-0001 --tag blue = @latest The @latest tag means that blue resolves to the latest Revision of the Service. The following example sends 80% of traffic to the latest Revision and 20% to a Revision named v1 . kn service update example-service --traffic @latest = 80 --traffic v1 = 20 Routing and managing traffic with blue/green deployment \u00b6 You can safely reroute traffic from a live version of an application to a new version by using a blue/green deployment strategy . Procedure \u00b6 Create and deploy an app as a Knative Service. Find the name of the first Revision that was created when you deployed the Service, by running the command: kubectl get configurations <service-name> -o = jsonpath = '{.status.latestCreatedRevisionName}' Where <service-name> is the name of the Service that you have deployed. Define a Route to send inbound traffic to the Revision. Example Route apiVersion : serving.knative.dev/v1 kind : Route metadata : name : <route-name> namespace : default spec : traffic : - revisionName : <first-revision-name> percent : 100 # All traffic goes to this revision Where; <route-name> is the name you choose for your route. <first-revision-name> is the name of the initial Revision from the previous step. Verify that you can view your app at the URL output you get from using the following command: kubectl get route <route-name> Where <route-name> is the name of the Route you created in the previous step. Deploy a second Revision of your app by modifying at least one field in the template spec of the Service resource. For example, you can modify the image of the Service, or an env environment variable. Redeploy the Service by applying the updated Service resource. You can do this by applying the Service YAML file or by using the kn service update command if you have installed the kn CLI. Find the name of the second, latest Revision that was created when you redeployed the Service, by running the command: kubectl get configurations <service-name> -o = jsonpath = '{.status.latestCreatedRevisionName}' Where <service-name> is the name of the Service that you have redeployed. At this point, both the first and second Revisions of the Service are deployed and running. Update your existing Route to create a new, test endpoint for the second Revision, while still sending all other traffic to the first Revision. Example of updated Route apiVersion : serving.knative.dev/v1 kind : Route metadata : name : <route-name> namespace : default spec : traffic : - revisionName : <first-revision-name> percent : 100 # All traffic is still being routed to the first revision - revisionName : <second-revision-name> percent : 0 # 0% of traffic routed to the second revision tag : v2 # A named route Once you redeploy this Route by reapplying the YAML resource, the second Revision of the app is now staged. No traffic is routed to the second Revision at the main URL, and Knative creates a new Route named v2 for testing the newly deployed Revision. Get the URL of the new Route for the second Revision, by running the command: kubectl get route <route-name> --output jsonpath = \"{.status.traffic[*].url}\" You can use this URL to validate that the new version of the app is behaving as expected before you route any traffic to it. Update your existing Route resource again, so that 50% of traffic is being sent to the first Revision, and 50% is being sent to the second Revision: Example of updated Route apiVersion : serving.knative.dev/v1 kind : Route metadata : name : <route-name> namespace : default spec : traffic : - revisionName : <first-revision-name> percent : 50 - revisionName : <second-revision-name> percent : 50 tag : v2 Once you are ready to route all traffic to the new version of the app, update the Route again to send 100% of traffic to the second Revision: Example of updated Route apiVersion : serving.knative.dev/v1 kind : Route metadata : name : <route-name> namespace : default spec : traffic : - revisionName : <first-revision-name> percent : 0 - revisionName : <second-revision-name> percent : 100 tag : v2 Tip You can remove the first Revision instead of setting it to 0% of traffic if you do not plan to roll back the Revision. Non-routeable Revision objects are then garbage-collected. Visit the URL of the first Revision to verify that no more traffic is being sent to the old version of the app.","title":"Traffic management"},{"location":"serving/traffic-management/#traffic-management","text":"You can manage traffic routing to different Revisions of a Knative Service by modifying the traffic spec of the Service resource. When you create a Knative Service, it does not have any default traffic spec settings. By setting the traffic spec, you can split traffic over any number of fixed Revisions, or send traffic to the latest Revision by setting latestRevision: true in the spec for a Service.","title":"Traffic management"},{"location":"serving/traffic-management/#using-tags-to-create-target-urls","text":"In the following example, the spec defines an attribute called tag : apiVersion : serving.knative.dev/v1 kind : Service metadata : name : example-service namespace : default spec : ... traffic : - percent : 0 revisionName : example-service-1 tag : staging - percent : 40 revisionName : example-service-2 - percent : 60 revisionName : example-service-3 When a tag attribute is applied to a Route, an address for the specific traffic target is created. In this example, you can access the staging target by accessing staging-<route name>.<namespace>.<domain> . The targets for example-service-2 and example-service-3 can only be accessed using the main route, <route name>.<namespace>.<domain> . When a traffic target is tagged, a new Kubernetes Service is created for that Service, so that other Services can access it within the cluster. From the previous example, a new Kubernetes Service called staging-<route name> will be created in the same namespace. This Service has the ability to override the visibility of this specific Route by applying the label networking.knative.dev/visibility with value cluster-local . See the documentation on private services for more information about how to restrict visibility on specific Routes.","title":"Using tags to create target URLs"},{"location":"serving/traffic-management/#traffic-routing-examples","text":"The following example shows a traffic spec where 100% of traffic is routed to the latestRevision of the Service. Under status you can see the name of the latest Revision that latestRevision was resolved to: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : example-service namespace : default spec : ... traffic : - latestRevision : true percent : 100 status : ... traffic : - percent : 100 revisionName : example-service-1 The following example shows a traffic spec where 100% of traffic is routed to the current Revision, and the name of that Revision is specified as example-service-1 . The latest ready Revision is kept available, even though no traffic is being routed to it: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : example-service namespace : default spec : ... traffic : - tag : current revisionName : example-service-1 percent : 100 - tag : latest latestRevision : true percent : 0 The following example shows how the list of Revisions in the traffic spec can be extended so that traffic is split between multiple Revisions. This example sends 50% of traffic to the current Revision, example-service-1 , and 50% of traffic to the candidate Revision, example-service-2 : apiVersion : serving.knative.dev/v1 kind : Service metadata : name : example-service namespace : default spec : ... traffic : - tag : current revisionName : example-service-1 percent : 50 - tag : candidate revisionName : example-service-2 percent : 50 - tag : latest latestRevision : true percent : 0","title":"Traffic routing examples"},{"location":"serving/traffic-management/#routing-and-managing-traffic-by-using-the-knative-cli","text":"You can use the following kn CLI command to split traffic between revisions: kn service update <service-name> --traffic <revision-name> = <percent> Where: <service-name> is the name of the Knative Service that you are configuring traffic routing for. <revision-name> is the name of the revision that you want to configure to receive a percentage of traffic. <percent> is the percentage of traffic that you want to send to the revision specified by <revision-name> . For example, to split traffic for a Service named example , by sending 80% of traffic to the Revision green and 20% of traffic to the Revision blue , you could run the following command: kn service update example-service --traffic green = 80 --traffic blue = 20 It is also possible to add tags to Revisions and then split traffic according to the tags you have set: kn service update example --tag green = revision-0001 --tag blue = @latest The @latest tag means that blue resolves to the latest Revision of the Service. The following example sends 80% of traffic to the latest Revision and 20% to a Revision named v1 . kn service update example-service --traffic @latest = 80 --traffic v1 = 20","title":"Routing and managing traffic by using the Knative CLI"},{"location":"serving/traffic-management/#routing-and-managing-traffic-with-bluegreen-deployment","text":"You can safely reroute traffic from a live version of an application to a new version by using a blue/green deployment strategy .","title":"Routing and managing traffic with blue/green deployment"},{"location":"serving/traffic-management/#procedure","text":"Create and deploy an app as a Knative Service. Find the name of the first Revision that was created when you deployed the Service, by running the command: kubectl get configurations <service-name> -o = jsonpath = '{.status.latestCreatedRevisionName}' Where <service-name> is the name of the Service that you have deployed. Define a Route to send inbound traffic to the Revision. Example Route apiVersion : serving.knative.dev/v1 kind : Route metadata : name : <route-name> namespace : default spec : traffic : - revisionName : <first-revision-name> percent : 100 # All traffic goes to this revision Where; <route-name> is the name you choose for your route. <first-revision-name> is the name of the initial Revision from the previous step. Verify that you can view your app at the URL output you get from using the following command: kubectl get route <route-name> Where <route-name> is the name of the Route you created in the previous step. Deploy a second Revision of your app by modifying at least one field in the template spec of the Service resource. For example, you can modify the image of the Service, or an env environment variable. Redeploy the Service by applying the updated Service resource. You can do this by applying the Service YAML file or by using the kn service update command if you have installed the kn CLI. Find the name of the second, latest Revision that was created when you redeployed the Service, by running the command: kubectl get configurations <service-name> -o = jsonpath = '{.status.latestCreatedRevisionName}' Where <service-name> is the name of the Service that you have redeployed. At this point, both the first and second Revisions of the Service are deployed and running. Update your existing Route to create a new, test endpoint for the second Revision, while still sending all other traffic to the first Revision. Example of updated Route apiVersion : serving.knative.dev/v1 kind : Route metadata : name : <route-name> namespace : default spec : traffic : - revisionName : <first-revision-name> percent : 100 # All traffic is still being routed to the first revision - revisionName : <second-revision-name> percent : 0 # 0% of traffic routed to the second revision tag : v2 # A named route Once you redeploy this Route by reapplying the YAML resource, the second Revision of the app is now staged. No traffic is routed to the second Revision at the main URL, and Knative creates a new Route named v2 for testing the newly deployed Revision. Get the URL of the new Route for the second Revision, by running the command: kubectl get route <route-name> --output jsonpath = \"{.status.traffic[*].url}\" You can use this URL to validate that the new version of the app is behaving as expected before you route any traffic to it. Update your existing Route resource again, so that 50% of traffic is being sent to the first Revision, and 50% is being sent to the second Revision: Example of updated Route apiVersion : serving.knative.dev/v1 kind : Route metadata : name : <route-name> namespace : default spec : traffic : - revisionName : <first-revision-name> percent : 50 - revisionName : <second-revision-name> percent : 50 tag : v2 Once you are ready to route all traffic to the new version of the app, update the Route again to send 100% of traffic to the second Revision: Example of updated Route apiVersion : serving.knative.dev/v1 kind : Route metadata : name : <route-name> namespace : default spec : traffic : - revisionName : <first-revision-name> percent : 0 - revisionName : <second-revision-name> percent : 100 tag : v2 Tip You can remove the first Revision instead of setting it to 0% of traffic if you do not plan to roll back the Revision. Non-routeable Revision objects are then garbage-collected. Visit the URL of the first Revision to verify that no more traffic is being sent to the old version of the app.","title":"Procedure"},{"location":"serving/using-a-custom-domain/","text":"Default domain name settings \u00b6 The fully qualified domain name for a route by default is {route}.{namespace}.{default-domain} . Knative Serving routes use example.com as the default domain. You can change the default domain by modifying the config-domain ConfigMap . Warning Customizing a domain using this ConfigMap affects your cluster globally. If you want to customize the domain of an individual Service, use DomainMapping instead. Procedure \u00b6 Open the config-domain ConfigMap in your default text editor: kubectl edit configmap config-domain -n knative-serving Edit the file to replace example.com with the domain you want to use, then remove the _example key and save your changes. In this example, mydomain.com is configured as the domain for all routes: apiVersion : v1 data : mydomain.com : \"\" kind : ConfigMap [ ... ] If you have an existing deployment, Knative reconciles the change made to the ConfigMap, and automatically updates the host name for all of the deployed Services and Routes. Verification steps \u00b6 Deploy an app to your cluster. Retrieve the URL for the Route: kubectl get route <route-name> --output jsonpath = \"{.status.url}\" Where <route-name> is the name of the Route. Observe the customized domain that you have configured. Publish your Domain \u00b6 To make your domain publicly accessible, you must update your DNS provider to point to the IP address for your service ingress. Create a wildcard record for the namespace and custom domain to the ingress IP Address, which would enable hostnames for multiple services in the same namespace to work without creating additional DNS entries. *.default.mydomain.com 59 IN A 35.237.28.44 Create an A record to point from the fully qualified domain name to the IP address of your Knative gateway. This step needs to be done for each Knative Service or Route created. helloworld-go.default.mydomain.com 59 IN A 35.237.28.44 After the domain update has propagated, you can access your app by using the fully qualified domain name of the deployed route.","title":"Changing the default domain"},{"location":"serving/using-a-custom-domain/#default-domain-name-settings","text":"The fully qualified domain name for a route by default is {route}.{namespace}.{default-domain} . Knative Serving routes use example.com as the default domain. You can change the default domain by modifying the config-domain ConfigMap . Warning Customizing a domain using this ConfigMap affects your cluster globally. If you want to customize the domain of an individual Service, use DomainMapping instead.","title":"Default domain name settings"},{"location":"serving/using-a-custom-domain/#procedure","text":"Open the config-domain ConfigMap in your default text editor: kubectl edit configmap config-domain -n knative-serving Edit the file to replace example.com with the domain you want to use, then remove the _example key and save your changes. In this example, mydomain.com is configured as the domain for all routes: apiVersion : v1 data : mydomain.com : \"\" kind : ConfigMap [ ... ] If you have an existing deployment, Knative reconciles the change made to the ConfigMap, and automatically updates the host name for all of the deployed Services and Routes.","title":"Procedure"},{"location":"serving/using-a-custom-domain/#verification-steps","text":"Deploy an app to your cluster. Retrieve the URL for the Route: kubectl get route <route-name> --output jsonpath = \"{.status.url}\" Where <route-name> is the name of the Route. Observe the customized domain that you have configured.","title":"Verification steps"},{"location":"serving/using-a-custom-domain/#publish-your-domain","text":"To make your domain publicly accessible, you must update your DNS provider to point to the IP address for your service ingress. Create a wildcard record for the namespace and custom domain to the ingress IP Address, which would enable hostnames for multiple services in the same namespace to work without creating additional DNS entries. *.default.mydomain.com 59 IN A 35.237.28.44 Create an A record to point from the fully qualified domain name to the IP address of your Knative gateway. This step needs to be done for each Knative Service or Route created. helloworld-go.default.mydomain.com 59 IN A 35.237.28.44 After the domain update has propagated, you can access your app by using the fully qualified domain name of the deployed route.","title":"Publish your Domain"},{"location":"serving/using-a-tls-cert/","text":"Configuring HTTPS with TLS certificates \u00b6 Learn how to configure secure HTTPS connections in Knative using TLS certificates ( TLS replaces SSL ). Configure secure HTTPS connections to enable your Knative services and routes to terminate external TLS connections . You can configure Knative to handle certificates that you manually specify, or you can enable Knative to automatically obtain and renew certificates. You can use either Certbot or cert-manager to obtain certificates. Both tools support TLS certificates but if you want to enable Knative for automatic TLS certificate provisioning, you must install and configure the cert-manager tool: Manually obtain and renew certificates : Both the Certbot and cert-manager tools can be used to manually obtain TLS certificates. In general, after you obtain a certificate, you must create a Kubernetes secret to use that certificate in your cluster. See the procedures later in this topic for details about manually obtaining and configuring certificates. Enable Knative to automatically obtain and renew TLS certificates : You can also use cert-manager to configure Knative to automatically obtain new TLS certificates and renew existing ones. If you want to enable Knative to automatically provision TLS certificates, instead see the Enabling automatic TLS certificate provisioning topic. By default, the Let's Encrypt Certificate Authority (CA) is used to demonstrate how to enable HTTPS connections, but you can configure Knative to use any certificate from a CA that supports the ACME protocol. However, you must use and configure your certificate issuer to use the DNS-01 challenge type . Warning Certificates issued by Let's Encrypt are valid for only 90days . Therefore, if you choose to manually obtain and configure your certificates, you must ensure that you renew each certificate before it expires. Before you begin \u00b6 You must meet the following requirements to enable secure HTTPS connections: Knative Serving must be installed. For details about installing the Serving component, see the Knative installation guides . You must configure your Knative cluster to use a custom domain . Warning Istio only supports a single certificate per Kubernetes cluster. To serve multiple domains using your Knative cluster, you must ensure that your new or existing certificate is signed for each of the domains that you want to serve. Obtaining a TLS certificate \u00b6 If you already have a signed certificate for your domain, see Manually adding a TLS certificate for details about configuring your Knative cluster. If you need a new TLS certificate, you can choose to use one of the following tools to obtain a certificate from Let's Encrypt: Setup Certbot to manually obtain Let's Encrypt certificates Setup cert-manager to either manually obtain a certificate, or to automatically provision certificates This page covers details for both options. For details about using other CA's, see the tool's reference documentation: Certbot supported providers cert-manager supported providers Using Certbot to manually obtain Let\u2019s Encrypt certificates \u00b6 Use the following steps to install Certbot and the use the tool to manually obtain a TLS certificate from Let's Encrypt. Install Certbot by following the certbot-auto wrapper script instructions. Run the following command to use Certbot to request a certificate using DNS challenge during authorization: ./certbot-auto certonly --manual --preferred-challenges dns -d '*.default.yourdomain.com' where -d specifies your domain. If you want to validate multiple domain's, you can include multiple flags: -d MY.EXAMPLEDOMAIN.1 -d MY.EXAMPLEDOMAIN.2 . For more information, see the Cerbot command-line reference. The Certbot tool walks you through the steps of validating that you own each domain that you specify by creating TXT records in those domains. Result: CertBot creates two files: Certificate: fullchain.pem Private key: privkey.pem What's next: Add the certificate and private key to your Knative cluster by creating a Kubernetes secret . Using cert-manager to obtain Let's Encrypt certificates \u00b6 You can install and use cert-manager to either manually obtain a certificate or to configure your Knative cluster for automatic certificate provisioning: Manual certificates : Install cert-manager and then use the tool to manually obtain a certificate. To use cert-manager to manually obtain certificates: Install and configure cert-manager . Continue to the steps about manually adding a TLS certificate by creating and using a Kubernetes secret. Automatic certificates : Configure Knative to use cert-manager for automatically obtaining and renewing TLS certificate. The steps for installing and configuring cert-manager for this method are covered in full in the Enabling automatic TLS cert provisioning topic. Manually adding a TLS certificate \u00b6 If you have an existing certificate or have used one of the Certbot or cert-manager tool to manually obtain a new certificate, you can use the following steps to add that certificate to your Knative cluster. For instructions about enabling Knative for automatic certificate provisioning, see Enabling automatic TLS cert provisioning . Otherwise, follow the steps in the relevant tab to manually add a certificate: Contour Istio To manually add a TLS certificate to your Knative cluster, you must create a Kubernetes secret and then configure the Knative Contour plugin. Create a Kubernetes secret to hold your TLS certificate, cert.pem , and the private key, key.pem , by running the command: kubectl create -n contour-external secret tls default-cert \\ --key key.pem \\ --cert cert.pem Note Take note of the namespace and secret name. You will need these in future steps. To use this certificate and private key in different namespaces, you must create a delegation. To do so, create a YAML file using the following template: apiVersion : projectcontour.io/v1 kind : TLSCertificateDelegation metadata : name : default-delegation namespace : contour-external spec : delegations : - secretName : default-cert targetNamespaces : - \"*\" Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Update the Knative Contour plugin to use the certificate as a fallback when autoTLS is disabled by running the command: kubectl patch configmap config-contour -n knative-serving \\ -p '{\"data\":{\"default-tls-secret\":\"contour-external/default-cert\"}}' To manually add a TLS certificate to your Knative cluster, you create a Kubernetes secret and then configure the knative-ingress-gateway : Create a Kubernetes secret to hold your TLS certificate, cert.pem , and the private key, key.pem , by entering the following command: kubectl create --namespace istio-system secret tls tls-cert \\ --key key.pem \\ --cert cert.pem Configure Knative to use the new secret that you created for HTTPS connections: Run the following command to open the Knative shared gateway in edit mode: kubectl edit gateway knative-ingress-gateway --namespace knative-serving Update the gateway to include the following tls: section and configuration: tls : mode : SIMPLE credentialName : tls-cert Example: # Edit the following object. Lines beginning with a '#' will be ignored. # An empty file will abort the edit. If an error occurs while saving this # file will be reopened with the relevant failures. apiVersion : networking.istio.io/v1alpha3 kind : Gateway metadata : # ... skipped ... spec : selector : istio : ingressgateway servers : - hosts : - \"*\" port : name : http number : 80 protocol : HTTP - hosts : - TLS_HOSTS port : name : https number : 443 protocol : HTTPS tls : mode : SIMPLE credentialName : tls-cert In this example, TLS_HOSTS represents the hosts of your TLS certificate. It can be a single host, multiple hosts, or a wildcard host. For detailed instructions, please refer Istio documentation What's next: \u00b6 After your changes are running on your Knative cluster, you can begin using the HTTPS protocol for secure access your deployed Knative services.","title":"Configuring HTTPS connections"},{"location":"serving/using-a-tls-cert/#configuring-https-with-tls-certificates","text":"Learn how to configure secure HTTPS connections in Knative using TLS certificates ( TLS replaces SSL ). Configure secure HTTPS connections to enable your Knative services and routes to terminate external TLS connections . You can configure Knative to handle certificates that you manually specify, or you can enable Knative to automatically obtain and renew certificates. You can use either Certbot or cert-manager to obtain certificates. Both tools support TLS certificates but if you want to enable Knative for automatic TLS certificate provisioning, you must install and configure the cert-manager tool: Manually obtain and renew certificates : Both the Certbot and cert-manager tools can be used to manually obtain TLS certificates. In general, after you obtain a certificate, you must create a Kubernetes secret to use that certificate in your cluster. See the procedures later in this topic for details about manually obtaining and configuring certificates. Enable Knative to automatically obtain and renew TLS certificates : You can also use cert-manager to configure Knative to automatically obtain new TLS certificates and renew existing ones. If you want to enable Knative to automatically provision TLS certificates, instead see the Enabling automatic TLS certificate provisioning topic. By default, the Let's Encrypt Certificate Authority (CA) is used to demonstrate how to enable HTTPS connections, but you can configure Knative to use any certificate from a CA that supports the ACME protocol. However, you must use and configure your certificate issuer to use the DNS-01 challenge type . Warning Certificates issued by Let's Encrypt are valid for only 90days . Therefore, if you choose to manually obtain and configure your certificates, you must ensure that you renew each certificate before it expires.","title":"Configuring HTTPS with TLS certificates"},{"location":"serving/using-a-tls-cert/#before-you-begin","text":"You must meet the following requirements to enable secure HTTPS connections: Knative Serving must be installed. For details about installing the Serving component, see the Knative installation guides . You must configure your Knative cluster to use a custom domain . Warning Istio only supports a single certificate per Kubernetes cluster. To serve multiple domains using your Knative cluster, you must ensure that your new or existing certificate is signed for each of the domains that you want to serve.","title":"Before you begin"},{"location":"serving/using-a-tls-cert/#obtaining-a-tls-certificate","text":"If you already have a signed certificate for your domain, see Manually adding a TLS certificate for details about configuring your Knative cluster. If you need a new TLS certificate, you can choose to use one of the following tools to obtain a certificate from Let's Encrypt: Setup Certbot to manually obtain Let's Encrypt certificates Setup cert-manager to either manually obtain a certificate, or to automatically provision certificates This page covers details for both options. For details about using other CA's, see the tool's reference documentation: Certbot supported providers cert-manager supported providers","title":"Obtaining a TLS certificate"},{"location":"serving/using-a-tls-cert/#using-certbot-to-manually-obtain-lets-encrypt-certificates","text":"Use the following steps to install Certbot and the use the tool to manually obtain a TLS certificate from Let's Encrypt. Install Certbot by following the certbot-auto wrapper script instructions. Run the following command to use Certbot to request a certificate using DNS challenge during authorization: ./certbot-auto certonly --manual --preferred-challenges dns -d '*.default.yourdomain.com' where -d specifies your domain. If you want to validate multiple domain's, you can include multiple flags: -d MY.EXAMPLEDOMAIN.1 -d MY.EXAMPLEDOMAIN.2 . For more information, see the Cerbot command-line reference. The Certbot tool walks you through the steps of validating that you own each domain that you specify by creating TXT records in those domains. Result: CertBot creates two files: Certificate: fullchain.pem Private key: privkey.pem What's next: Add the certificate and private key to your Knative cluster by creating a Kubernetes secret .","title":"Using Certbot to manually obtain Let\u2019s Encrypt certificates"},{"location":"serving/using-a-tls-cert/#using-cert-manager-to-obtain-lets-encrypt-certificates","text":"You can install and use cert-manager to either manually obtain a certificate or to configure your Knative cluster for automatic certificate provisioning: Manual certificates : Install cert-manager and then use the tool to manually obtain a certificate. To use cert-manager to manually obtain certificates: Install and configure cert-manager . Continue to the steps about manually adding a TLS certificate by creating and using a Kubernetes secret. Automatic certificates : Configure Knative to use cert-manager for automatically obtaining and renewing TLS certificate. The steps for installing and configuring cert-manager for this method are covered in full in the Enabling automatic TLS cert provisioning topic.","title":"Using cert-manager to obtain Let's Encrypt certificates"},{"location":"serving/using-a-tls-cert/#manually-adding-a-tls-certificate","text":"If you have an existing certificate or have used one of the Certbot or cert-manager tool to manually obtain a new certificate, you can use the following steps to add that certificate to your Knative cluster. For instructions about enabling Knative for automatic certificate provisioning, see Enabling automatic TLS cert provisioning . Otherwise, follow the steps in the relevant tab to manually add a certificate: Contour Istio To manually add a TLS certificate to your Knative cluster, you must create a Kubernetes secret and then configure the Knative Contour plugin. Create a Kubernetes secret to hold your TLS certificate, cert.pem , and the private key, key.pem , by running the command: kubectl create -n contour-external secret tls default-cert \\ --key key.pem \\ --cert cert.pem Note Take note of the namespace and secret name. You will need these in future steps. To use this certificate and private key in different namespaces, you must create a delegation. To do so, create a YAML file using the following template: apiVersion : projectcontour.io/v1 kind : TLSCertificateDelegation metadata : name : default-delegation namespace : contour-external spec : delegations : - secretName : default-cert targetNamespaces : - \"*\" Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Update the Knative Contour plugin to use the certificate as a fallback when autoTLS is disabled by running the command: kubectl patch configmap config-contour -n knative-serving \\ -p '{\"data\":{\"default-tls-secret\":\"contour-external/default-cert\"}}' To manually add a TLS certificate to your Knative cluster, you create a Kubernetes secret and then configure the knative-ingress-gateway : Create a Kubernetes secret to hold your TLS certificate, cert.pem , and the private key, key.pem , by entering the following command: kubectl create --namespace istio-system secret tls tls-cert \\ --key key.pem \\ --cert cert.pem Configure Knative to use the new secret that you created for HTTPS connections: Run the following command to open the Knative shared gateway in edit mode: kubectl edit gateway knative-ingress-gateway --namespace knative-serving Update the gateway to include the following tls: section and configuration: tls : mode : SIMPLE credentialName : tls-cert Example: # Edit the following object. Lines beginning with a '#' will be ignored. # An empty file will abort the edit. If an error occurs while saving this # file will be reopened with the relevant failures. apiVersion : networking.istio.io/v1alpha3 kind : Gateway metadata : # ... skipped ... spec : selector : istio : ingressgateway servers : - hosts : - \"*\" port : name : http number : 80 protocol : HTTP - hosts : - TLS_HOSTS port : name : https number : 443 protocol : HTTPS tls : mode : SIMPLE credentialName : tls-cert In this example, TLS_HOSTS represents the hosts of your TLS certificate. It can be a single host, multiple hosts, or a wildcard host. For detailed instructions, please refer Istio documentation","title":"Manually adding a TLS certificate"},{"location":"serving/using-a-tls-cert/#whats-next","text":"After your changes are running on your Knative cluster, you can begin using the HTTPS protocol for secure access your deployed Knative services.","title":"What's next:"},{"location":"serving/using-auto-tls/","text":"Enabling automatic TLS certificate provisioning \u00b6 If you install and configure cert-manager, you can configure Knative to automatically obtain new TLS certificates and renew existing ones for Knative Services. To learn more about using secure connections in Knative, see Configuring HTTPS with TLS certificates . Before you begin \u00b6 The following must be installed on your Knative cluster: Knative Serving . A Networking layer such as Kourier, Istio with SDS v1.3 or higher, or Contour v1.1 or higher. See Install a networking layer or Istio with SDS, version 1.3 or higher . cert-manager version 1.0.0 or higher . Your Knative cluster must be configured to use a custom domain . Your DNS provider must be setup and configured to your domain. If you want to use HTTP-01 challenge, you need to configure your custom domain to map to the IP of ingress. You can achieve this by adding a DNS A record to map the domain to the IP according to the instructions of your DNS provider. Automatic TLS provision mode \u00b6 Knative supports the following Auto TLS modes: Using DNS-01 challenge In this mode, your cluster needs to be able to talk to your DNS server to verify the ownership of your domain. - Provision Certificate per namespace is supported when using DNS-01 challenge mode. - This is the recommended mode for faster certificate provision. - In this mode, a single Certificate will be provisioned per namespace and is reused across the Knative Services within the same namespace. Provision Certificate per Knative Service is supported when using DNS-01 challenge mode. This is the recommended mode for better certificate isolation between Knative Services. In this mode, a Certificate will be provisioned for each Knative Service. The TLS effective time is longer as it needs Certificate provision for each Knative Service creation. Using HTTP-01 challenge In this type, your cluster does not need to be able to talk to your DNS server. You must map your domain to the IP of the cluser ingress. When using HTTP-01 challenge, a certificate will be provisioned per Knative Service. HTTP-01 does not support provisioning a certificate per namespace. Enabling Auto TLS \u00b6 Create and add the ClusterIssuer configuration file to your Knative cluster to define who issues the TLS certificates, how requests are validated, and which DNS provider validates those requests. ClusterIssuer for DNS-01 challenge: use the cert-manager reference to determine how to configure your ClusterIssuer file. See the generic ClusterIssuer example Also see the DNS01 example For example, the following ClusterIssuer file named letsencrypt-issuer is configured for the Let's Encrypt CA and Google Cloud DNS. The Let's Encrypt account info, required DNS-01 challenge type, and Cloud DNS provider info is defined under spec . apiVersion : cert-manager.io/v1 kind : ClusterIssuer metadata : name : letsencrypt-dns-issuer spec : acme : server : https://acme-v02.api.letsencrypt.org/directory # This will register an issuer with LetsEncrypt. Replace # with your admin email address. email : myemail@gmail.com privateKeySecretRef : # Set privateKeySecretRef to any unused secret name. name : letsencrypt-dns-issuer solvers : - dns01 : cloudDNS : # Set this to your GCP project-id project : $PROJECT_ID # Set this to the secret that we publish our service account key # in the previous step. serviceAccountSecretRef : name : cloud-dns-key key : key.json ClusterIssuer for HTTP-01 challenge To apply the ClusterIssuer for HTTP01 challenge: Create a YAML file using the following template: apiVersion : cert-manager.io/v1 kind : ClusterIssuer metadata : name : letsencrypt-http01-issuer spec : acme : privateKeySecretRef : name : letsencrypt server : https://acme-v02.api.letsencrypt.org/directory solvers : - http01 : ingress : class : istio Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Ensure that the ClusterIssuer is created successfully: kubectl get clusterissuer <cluster-issuer-name> -o yaml Result: The Status.Conditions should include Ready=True . DNS-01 challenge only: Configure your DNS provider \u00b6 If you choose to use DNS-01 challenge, configure which DNS provider is used to validate the DNS-01 challenge requests. Instructions about configuring cert-manager, for all the supported DNS providers, are provided in DNS01 challenge providers and configuration instructions . Note that DNS-01 challenges can be used to either validate an individual domain name or to validate an entire namespace using a wildcard certificate like *.my-ns.example.com . Install net-certmanager-controller deployment \u00b6 Determine if net-certmanager-controller is already installed by running the following command: kubectl get deployment net-certmanager-controller -n knative-serving If net-certmanager-controller is not found, run the following command: kubectl apply --filename https://storage.googleapis.com/knative-nightly/net-certmanager/latest/release.yaml Provisioning certificates per namespace (wildcard certificates) \u00b6 Warning Provisioning a certificate per namespace only works with DNS-01 challenge. This component cannot be used with HTTP-01 challenge. The per-namespace certificate manager uses namespace labels to select which namespaces should have a certificate applied. For more details on namespace selectors, see the Kubernetes documentation . Prior to release 1.0, the fixed label networking.knative.dev/disableWildcardCert: true was used to disable certificate generation for a namespace. In 1.0 and later, other labels such as kubernetes.io/metadata.name may be used to select or restrict namespaces. To enable certificates for all namespaces except those with the networking.knative.dev/disableWildcardCert: true label, use the following command: kubectl patch --namespace knative-serving configmap config-network -p '{\"data\": {\"namespace-wildcard-cert-selector\": \"{\\\"matchExpressions\\\": [{\\\"key\\\":\\\"networking.knative.dev/disableWildcardCert\\\", \\\"operator\\\": \\\"NotIn\\\", \\\"values\\\":[\\\"true\\\"]}]}\"}}' This selects all namespaces where the label value is not in the set \"true\" . Configure config-certmanager ConfigMap \u00b6 Update your config-certmanager ConfigMap in the knative-serving namespace to reference your new ClusterIssuer . Run the following command to edit your config-certmanager ConfigMap: kubectl edit configmap config-certmanager --namespace knative-serving Add the issuerRef within the data section: data: issuerRef: | kind: ClusterIssuer name: letsencrypt-http01-issuer Example: apiVersion: v1 kind: ConfigMap metadata: name: config-certmanager namespace: knative-serving labels: networking.knative.dev/certificate-provider: cert-manager data: issuerRef: | kind: ClusterIssuer name: letsencrypt-http01-issuer issueRef defines which ClusterIssuer will be used by Knative to issue certificates. Ensure that the file was updated successfully: kubectl get configmap config-certmanager --namespace knative-serving --output yaml Turn on Auto TLS \u00b6 Update the config-network ConfigMap in the knative-serving namespace to enable auto-tls and specify how HTTP requests are handled: Run the following command to edit your config-network ConfigMap: kubectl edit configmap config-network --namespace knative-serving Add the auto-tls: Enabled attribute under the data section: data: auto-tls: Enabled Example: apiVersion: v1 kind: ConfigMap metadata: name: config-network namespace: knative-serving data: ... auto-tls: Enabled ... Configure how HTTP and HTTPS requests are handled in the http-protocol attribute. By default, Knative ingress is configured to serve HTTP traffic ( http-protocol: Enabled ). Now that your cluster is configured to use TLS certificates and handle HTTPS traffic, you can specify whether or not any HTTP traffic is allowed. Supported http-protocol values: Enabled : Serve HTTP traffic. Disabled : Rejects all HTTP traffic. Redirected : Responds to HTTP request with a 302 redirect to ask the clients to use HTTPS. data: http-protocol: Redirected Example: apiVersion: v1 kind: ConfigMap metadata: name: config-network namespace: knative-serving data: ... auto-tls: Enabled http-protocol: Redirected ... Note When using HTTP-01 challenge, http-protocol field has to be set to Enabled to make sure HTTP-01 challenge requests can be accepted by the cluster. Ensure that the file was updated successfully: kubectl get configmap config-network --namespace knative-serving --output yaml Congratulations! Knative is now configured to obtain and renew TLS certificates. When your TLS certificate is active on your cluster, your Knative services will be able to handle HTTPS traffic. Verify Auto TLS \u00b6 Run the following comand to create a Knative Service: kubectl apply -f https://raw.githubusercontent.com/knative/docs/main/docs/serving/autoscaling/autoscale-go/service.yaml When the certificate is provisioned (which could take up to several minutes depending on the challenge type), you should see something like: NAME URL LATESTCREATED LATESTREADY READY REASON autoscale-go https://autoscale-go.default.{custom-domain} autoscale-go-6jf85 autoscale-go-6jf85 True Note that the URL will be https in this case. Disable Auto TLS per service or route \u00b6 If you have Auto TLS enabled in your cluster, you can choose to disable Auto TLS for individual services or routes by adding the annotation networking.knative.dev/disable-auto-tls: true . Using the previous autoscale-go example: Edit the service using kubectl edit service.serving.knative.dev/autoscale-go -n default and add the annotation: apiVersion : serving.knative.dev/v1 kind : Service metadata : annotations : ... networking.knative.dev/disable-auto-tls : \"true\" ... The service URL should now be http , indicating that AutoTLS is disabled: NAME URL LATEST AGE CONDITIONS READY REASON autoscale-go http://autoscale-go.default.1.arenault.dev autoscale-go-dd42t 8m17s 3 OK / 3 True","title":"Enabling auto-TLS certs"},{"location":"serving/using-auto-tls/#enabling-automatic-tls-certificate-provisioning","text":"If you install and configure cert-manager, you can configure Knative to automatically obtain new TLS certificates and renew existing ones for Knative Services. To learn more about using secure connections in Knative, see Configuring HTTPS with TLS certificates .","title":"Enabling automatic TLS certificate provisioning"},{"location":"serving/using-auto-tls/#before-you-begin","text":"The following must be installed on your Knative cluster: Knative Serving . A Networking layer such as Kourier, Istio with SDS v1.3 or higher, or Contour v1.1 or higher. See Install a networking layer or Istio with SDS, version 1.3 or higher . cert-manager version 1.0.0 or higher . Your Knative cluster must be configured to use a custom domain . Your DNS provider must be setup and configured to your domain. If you want to use HTTP-01 challenge, you need to configure your custom domain to map to the IP of ingress. You can achieve this by adding a DNS A record to map the domain to the IP according to the instructions of your DNS provider.","title":"Before you begin"},{"location":"serving/using-auto-tls/#automatic-tls-provision-mode","text":"Knative supports the following Auto TLS modes: Using DNS-01 challenge In this mode, your cluster needs to be able to talk to your DNS server to verify the ownership of your domain. - Provision Certificate per namespace is supported when using DNS-01 challenge mode. - This is the recommended mode for faster certificate provision. - In this mode, a single Certificate will be provisioned per namespace and is reused across the Knative Services within the same namespace. Provision Certificate per Knative Service is supported when using DNS-01 challenge mode. This is the recommended mode for better certificate isolation between Knative Services. In this mode, a Certificate will be provisioned for each Knative Service. The TLS effective time is longer as it needs Certificate provision for each Knative Service creation. Using HTTP-01 challenge In this type, your cluster does not need to be able to talk to your DNS server. You must map your domain to the IP of the cluser ingress. When using HTTP-01 challenge, a certificate will be provisioned per Knative Service. HTTP-01 does not support provisioning a certificate per namespace.","title":"Automatic TLS provision mode"},{"location":"serving/using-auto-tls/#enabling-auto-tls","text":"Create and add the ClusterIssuer configuration file to your Knative cluster to define who issues the TLS certificates, how requests are validated, and which DNS provider validates those requests. ClusterIssuer for DNS-01 challenge: use the cert-manager reference to determine how to configure your ClusterIssuer file. See the generic ClusterIssuer example Also see the DNS01 example For example, the following ClusterIssuer file named letsencrypt-issuer is configured for the Let's Encrypt CA and Google Cloud DNS. The Let's Encrypt account info, required DNS-01 challenge type, and Cloud DNS provider info is defined under spec . apiVersion : cert-manager.io/v1 kind : ClusterIssuer metadata : name : letsencrypt-dns-issuer spec : acme : server : https://acme-v02.api.letsencrypt.org/directory # This will register an issuer with LetsEncrypt. Replace # with your admin email address. email : myemail@gmail.com privateKeySecretRef : # Set privateKeySecretRef to any unused secret name. name : letsencrypt-dns-issuer solvers : - dns01 : cloudDNS : # Set this to your GCP project-id project : $PROJECT_ID # Set this to the secret that we publish our service account key # in the previous step. serviceAccountSecretRef : name : cloud-dns-key key : key.json ClusterIssuer for HTTP-01 challenge To apply the ClusterIssuer for HTTP01 challenge: Create a YAML file using the following template: apiVersion : cert-manager.io/v1 kind : ClusterIssuer metadata : name : letsencrypt-http01-issuer spec : acme : privateKeySecretRef : name : letsencrypt server : https://acme-v02.api.letsencrypt.org/directory solvers : - http01 : ingress : class : istio Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Ensure that the ClusterIssuer is created successfully: kubectl get clusterissuer <cluster-issuer-name> -o yaml Result: The Status.Conditions should include Ready=True .","title":"Enabling Auto TLS"},{"location":"serving/using-auto-tls/#dns-01-challenge-only-configure-your-dns-provider","text":"If you choose to use DNS-01 challenge, configure which DNS provider is used to validate the DNS-01 challenge requests. Instructions about configuring cert-manager, for all the supported DNS providers, are provided in DNS01 challenge providers and configuration instructions . Note that DNS-01 challenges can be used to either validate an individual domain name or to validate an entire namespace using a wildcard certificate like *.my-ns.example.com .","title":"DNS-01 challenge only: Configure your DNS provider"},{"location":"serving/using-auto-tls/#install-net-certmanager-controller-deployment","text":"Determine if net-certmanager-controller is already installed by running the following command: kubectl get deployment net-certmanager-controller -n knative-serving If net-certmanager-controller is not found, run the following command: kubectl apply --filename https://storage.googleapis.com/knative-nightly/net-certmanager/latest/release.yaml","title":"Install net-certmanager-controller deployment"},{"location":"serving/using-auto-tls/#provisioning-certificates-per-namespace-wildcard-certificates","text":"Warning Provisioning a certificate per namespace only works with DNS-01 challenge. This component cannot be used with HTTP-01 challenge. The per-namespace certificate manager uses namespace labels to select which namespaces should have a certificate applied. For more details on namespace selectors, see the Kubernetes documentation . Prior to release 1.0, the fixed label networking.knative.dev/disableWildcardCert: true was used to disable certificate generation for a namespace. In 1.0 and later, other labels such as kubernetes.io/metadata.name may be used to select or restrict namespaces. To enable certificates for all namespaces except those with the networking.knative.dev/disableWildcardCert: true label, use the following command: kubectl patch --namespace knative-serving configmap config-network -p '{\"data\": {\"namespace-wildcard-cert-selector\": \"{\\\"matchExpressions\\\": [{\\\"key\\\":\\\"networking.knative.dev/disableWildcardCert\\\", \\\"operator\\\": \\\"NotIn\\\", \\\"values\\\":[\\\"true\\\"]}]}\"}}' This selects all namespaces where the label value is not in the set \"true\" .","title":"Provisioning certificates per namespace (wildcard certificates)"},{"location":"serving/using-auto-tls/#configure-config-certmanager-configmap","text":"Update your config-certmanager ConfigMap in the knative-serving namespace to reference your new ClusterIssuer . Run the following command to edit your config-certmanager ConfigMap: kubectl edit configmap config-certmanager --namespace knative-serving Add the issuerRef within the data section: data: issuerRef: | kind: ClusterIssuer name: letsencrypt-http01-issuer Example: apiVersion: v1 kind: ConfigMap metadata: name: config-certmanager namespace: knative-serving labels: networking.knative.dev/certificate-provider: cert-manager data: issuerRef: | kind: ClusterIssuer name: letsencrypt-http01-issuer issueRef defines which ClusterIssuer will be used by Knative to issue certificates. Ensure that the file was updated successfully: kubectl get configmap config-certmanager --namespace knative-serving --output yaml","title":"Configure config-certmanager ConfigMap"},{"location":"serving/using-auto-tls/#turn-on-auto-tls","text":"Update the config-network ConfigMap in the knative-serving namespace to enable auto-tls and specify how HTTP requests are handled: Run the following command to edit your config-network ConfigMap: kubectl edit configmap config-network --namespace knative-serving Add the auto-tls: Enabled attribute under the data section: data: auto-tls: Enabled Example: apiVersion: v1 kind: ConfigMap metadata: name: config-network namespace: knative-serving data: ... auto-tls: Enabled ... Configure how HTTP and HTTPS requests are handled in the http-protocol attribute. By default, Knative ingress is configured to serve HTTP traffic ( http-protocol: Enabled ). Now that your cluster is configured to use TLS certificates and handle HTTPS traffic, you can specify whether or not any HTTP traffic is allowed. Supported http-protocol values: Enabled : Serve HTTP traffic. Disabled : Rejects all HTTP traffic. Redirected : Responds to HTTP request with a 302 redirect to ask the clients to use HTTPS. data: http-protocol: Redirected Example: apiVersion: v1 kind: ConfigMap metadata: name: config-network namespace: knative-serving data: ... auto-tls: Enabled http-protocol: Redirected ... Note When using HTTP-01 challenge, http-protocol field has to be set to Enabled to make sure HTTP-01 challenge requests can be accepted by the cluster. Ensure that the file was updated successfully: kubectl get configmap config-network --namespace knative-serving --output yaml Congratulations! Knative is now configured to obtain and renew TLS certificates. When your TLS certificate is active on your cluster, your Knative services will be able to handle HTTPS traffic.","title":"Turn on Auto TLS"},{"location":"serving/using-auto-tls/#verify-auto-tls","text":"Run the following comand to create a Knative Service: kubectl apply -f https://raw.githubusercontent.com/knative/docs/main/docs/serving/autoscaling/autoscale-go/service.yaml When the certificate is provisioned (which could take up to several minutes depending on the challenge type), you should see something like: NAME URL LATESTCREATED LATESTREADY READY REASON autoscale-go https://autoscale-go.default.{custom-domain} autoscale-go-6jf85 autoscale-go-6jf85 True Note that the URL will be https in this case.","title":"Verify Auto TLS"},{"location":"serving/using-auto-tls/#disable-auto-tls-per-service-or-route","text":"If you have Auto TLS enabled in your cluster, you can choose to disable Auto TLS for individual services or routes by adding the annotation networking.knative.dev/disable-auto-tls: true . Using the previous autoscale-go example: Edit the service using kubectl edit service.serving.knative.dev/autoscale-go -n default and add the annotation: apiVersion : serving.knative.dev/v1 kind : Service metadata : annotations : ... networking.knative.dev/disable-auto-tls : \"true\" ... The service URL should now be http , indicating that AutoTLS is disabled: NAME URL LATEST AGE CONDITIONS READY REASON autoscale-go http://autoscale-go.default.1.arenault.dev autoscale-go-dd42t 8m17s 3 OK / 3 True","title":"Disable Auto TLS per service or route"},{"location":"serving/webhook-customizations/","text":"Exclude namespaces from the Knative webhook \u00b6 The Knative webhook examines resources that are created, read, updated, or deleted. This includes system namespaces, which can cause issues during an upgrade if the webhook becomes non-responsive. Cluster administrators may want to disable the Knative webhook on system namespaces to prevent issues during upgrades. You can configure the label webhooks.knative.dev/exclude to allow namespaces to bypass the Knative webhook. apiVersion : v1 kind : Namespace metadata : name : knative-dev labels : webhooks.knative.dev/exclude : \"true\"","title":"Exclude namespaces from the Knative webhook"},{"location":"serving/webhook-customizations/#exclude-namespaces-from-the-knative-webhook","text":"The Knative webhook examines resources that are created, read, updated, or deleted. This includes system namespaces, which can cause issues during an upgrade if the webhook becomes non-responsive. Cluster administrators may want to disable the Knative webhook on system namespaces to prevent issues during upgrades. You can configure the label webhooks.knative.dev/exclude to allow namespaces to bypass the Knative webhook. apiVersion : v1 kind : Namespace metadata : name : knative-dev labels : webhooks.knative.dev/exclude : \"true\"","title":"Exclude namespaces from the Knative webhook"},{"location":"serving/autoscaling/","text":"\u81ea\u52a8\u6269\u7f29\u5bb9 \u00b6 Knative Serving\u4e3a\u5e94\u7528\u63d0\u4f9b\u81ea\u52a8\u6269\u7f29\u80fd\u529b\uff0c \u9ed8\u8ba4\u901a\u8fc7 Knative Pod Autoscaler\uff08KPA\uff09\u5b9e\u73b0\u3002 \u4f8b\u5982\uff0c\u5982\u679c\u4e00\u4e2a\u5e94\u7528\u6ca1\u6709\u63a5\u6536\u5230\u6d41\u91cf\u5e76\u4e14\u542f\u7528\u4e86\u7f29\u5bb9\u5230\u96f6\uff0cKnative\u4f1a\u5c06\u6539\u5e94\u7528\u7684\u526f\u672c\u6570\u7f29\u5bb9\u5230\u96f6\u3002\u5982\u679c\u7f29\u5bb9\u5230\u96f6\u6ca1\u6709\u542f\u7528\uff0c\u5219\u5e94\u7528\u526f\u672c\u6570\u4f1a\u7f29\u5bb9\u5230\u6307\u5b9a\u7684\u6700\u5c0f\u526f\u672c\u6570\u3002 \u5f53\u7136\uff0c\u5e94\u7528\u4e5f\u53ef\u4ee5\u81ea\u52a8\u6269\u5bb9\u4ee5\u5e94\u5bf9\u6d41\u91cf\u7684\u589e\u957f\u3002 \u5982\u679c\u60a8\u5177\u6709\u96c6\u7fa4\u7ba1\u7406\u5458\u6743\u9650\uff0c\u60a8\u53ef\u4ee5\u4e3a\u96c6\u7fa4\u542f\u7528\u548c\u7981\u7528\u7f29\u5bb9\u5230\u96f6\u7684\u529f\u80fd\u3002\u53c2\u9605 \u914d\u7f6e\u7f29\u5bb9\u5230\u96f6 \u3002 \u4e3a\u4e86\u81ea\u52a8\u6269\u7f29\u4f60\u7684\u5e94\u7528\uff0c\u4f60\u8fd8\u9700\u8981\u914d\u7f6e \u5e76\u53d1 \u548c \u6269\u7f29\u5bb9\u4e0a\u4e0b\u754c \u3002 \u5176\u5b83\u8d44\u6599 \u00b6 \u5c1d\u8bd5 Go\u81ea\u52a8\u6269\u7f29\u793a\u4f8b\u5e94\u7528 . \u5c06Knative deployment\u914d\u7f6e\u6210\u4f7f\u7528Kubernetes\u7684Pod\u6c34\u5e73\u81ea\u52a8\u6269\u7f29(Horizontal Pod Autoscaler\uff0cHPA) \u800c\u4e0d\u662f\u9ed8\u8ba4\u7684KPA. \u5173\u4e8e\u5982\u4f55\u5b89\u88c5HPA, \u53c2\u9605 \u5b89\u88c5\u53ef\u9009\u7684Serving\u6269\u5c55 . \u914d\u7f6e\u81ea\u52a8\u6269\u7f29(Autoscaler)\u4f7f\u7528\u7684 \u6307\u6807\u7684\u7c7b\u578b . \u914d\u7f6eKnative Service\u4f7f\u7528 container-freezer , \u5b83\u4f1a\u5728 pod \u7684\u6d41\u91cf\u964d\u81f3\u96f6\u65f6\u51bb\u7ed3\u6b63\u5728\u8fd0\u884c\u7684\u8fdb\u7a0b\u3002\u5176\u597d\u5904\u662f\u51cf\u5c11\u4e86\u6b64\u914d\u7f6e\u4e2d\u7684\u51b7\u542f\u52a8\u65f6\u95f4\u3002","title":"\u5173\u4e8e\u81ea\u52a8\u6269\u7f29"},{"location":"serving/autoscaling/#_1","text":"Knative Serving\u4e3a\u5e94\u7528\u63d0\u4f9b\u81ea\u52a8\u6269\u7f29\u80fd\u529b\uff0c \u9ed8\u8ba4\u901a\u8fc7 Knative Pod Autoscaler\uff08KPA\uff09\u5b9e\u73b0\u3002 \u4f8b\u5982\uff0c\u5982\u679c\u4e00\u4e2a\u5e94\u7528\u6ca1\u6709\u63a5\u6536\u5230\u6d41\u91cf\u5e76\u4e14\u542f\u7528\u4e86\u7f29\u5bb9\u5230\u96f6\uff0cKnative\u4f1a\u5c06\u6539\u5e94\u7528\u7684\u526f\u672c\u6570\u7f29\u5bb9\u5230\u96f6\u3002\u5982\u679c\u7f29\u5bb9\u5230\u96f6\u6ca1\u6709\u542f\u7528\uff0c\u5219\u5e94\u7528\u526f\u672c\u6570\u4f1a\u7f29\u5bb9\u5230\u6307\u5b9a\u7684\u6700\u5c0f\u526f\u672c\u6570\u3002 \u5f53\u7136\uff0c\u5e94\u7528\u4e5f\u53ef\u4ee5\u81ea\u52a8\u6269\u5bb9\u4ee5\u5e94\u5bf9\u6d41\u91cf\u7684\u589e\u957f\u3002 \u5982\u679c\u60a8\u5177\u6709\u96c6\u7fa4\u7ba1\u7406\u5458\u6743\u9650\uff0c\u60a8\u53ef\u4ee5\u4e3a\u96c6\u7fa4\u542f\u7528\u548c\u7981\u7528\u7f29\u5bb9\u5230\u96f6\u7684\u529f\u80fd\u3002\u53c2\u9605 \u914d\u7f6e\u7f29\u5bb9\u5230\u96f6 \u3002 \u4e3a\u4e86\u81ea\u52a8\u6269\u7f29\u4f60\u7684\u5e94\u7528\uff0c\u4f60\u8fd8\u9700\u8981\u914d\u7f6e \u5e76\u53d1 \u548c \u6269\u7f29\u5bb9\u4e0a\u4e0b\u754c \u3002","title":"\u81ea\u52a8\u6269\u7f29\u5bb9"},{"location":"serving/autoscaling/#_2","text":"\u5c1d\u8bd5 Go\u81ea\u52a8\u6269\u7f29\u793a\u4f8b\u5e94\u7528 . \u5c06Knative deployment\u914d\u7f6e\u6210\u4f7f\u7528Kubernetes\u7684Pod\u6c34\u5e73\u81ea\u52a8\u6269\u7f29(Horizontal Pod Autoscaler\uff0cHPA) \u800c\u4e0d\u662f\u9ed8\u8ba4\u7684KPA. \u5173\u4e8e\u5982\u4f55\u5b89\u88c5HPA, \u53c2\u9605 \u5b89\u88c5\u53ef\u9009\u7684Serving\u6269\u5c55 . \u914d\u7f6e\u81ea\u52a8\u6269\u7f29(Autoscaler)\u4f7f\u7528\u7684 \u6307\u6807\u7684\u7c7b\u578b . \u914d\u7f6eKnative Service\u4f7f\u7528 container-freezer , \u5b83\u4f1a\u5728 pod \u7684\u6d41\u91cf\u964d\u81f3\u96f6\u65f6\u51bb\u7ed3\u6b63\u5728\u8fd0\u884c\u7684\u8fdb\u7a0b\u3002\u5176\u597d\u5904\u662f\u51cf\u5c11\u4e86\u6b64\u914d\u7f6e\u4e2d\u7684\u51b7\u542f\u52a8\u65f6\u95f4\u3002","title":"\u5176\u5b83\u8d44\u6599"},{"location":"serving/autoscaling/autoscaler-types/","text":"\u652f\u6301\u7684\u81ea\u52a8\u6269\u7f29\u7c7b\u578b \u00b6 Knative Serving \u652f\u6301 Knative Pod Autoscaler (KPA) \u548c Kubernetes\u7684Pod\u6c34\u5e73\u81ea\u52a8\u6269\u7f29 (HPA)\u3002 \u672c\u9875\u9762\u5217\u51fa\u8fd9\u4e9b\u81ea\u52a8\u6269\u7f29\u7c7b\u578b\u7684\u7279\u70b9\u548c\u9650\u5236\uff0c\u4ee5\u53ca\u5982\u4f55\u914d\u7f6e\u5b83\u4eec\u3002 \u91cd\u8981 \u5982\u679c\u4f60\u60f3\u8981\u7684\u4f7f\u7528HPA, \u4f60\u5fc5\u987b\u5728\u5b89\u88c5Knative Serving\u4e4b\u540e\u5b89\u88c5\u5b83. \u5173\u4e8e\u5982\u4f55\u5b89\u88c5HPA, \u53c2\u9605 \u5b89\u88c5\u53ef\u9009\u7684Serving\u6269\u5c55 . Knative Pod Autoscaler (KPA) \u00b6 Knative Serving \u6838\u5fc3\u7684\u4e00\u90e8\u5206\uff0c\u5b89\u88c5 Knative Serving \u540e\u9ed8\u8ba4\u542f\u7528\u3002 \u652f\u6301\u7f29\u5bb9\u5230\u96f6\u7684\u529f\u80fd\u3002 \u4e0d\u652f\u6301\u57fa\u4e8e CPU \u7684\u81ea\u52a8\u6269\u7f29\u3002 Pod\u6c34\u5e73\u81ea\u52a8\u6269\u7f29 (HPA) \u00b6 \u4e0d\u662f Knative Serving \u6838\u5fc3\u7684\u4e00\u90e8\u5206\uff0c\u60a8\u5fc5\u987b\u5148\u5b89\u88c5 Knative Serving\u3002 \u4e0d\u652f\u6301\u7f29\u5bb9\u5230\u96f6\u7684\u529f\u80fd\u3002 \u652f\u6301\u57fa\u4e8e CPU \u7684\u81ea\u52a8\u6269\u7f29\u3002 \u914d\u7f6e\u4f7f\u7528\u54ea\u79cd\u7c7b\u578b\u7684\u81ea\u52a8\u6269\u7f29 \u00b6 \u81ea\u52a8\u6269\u7f29\u7684\u7c7b\u578b(KPA or HPA) \u53ef\u4ee5\u901a\u8fc7 class \u914d\u7f6e\u3002 \u5168\u5c40\u8bbe\u7f6e\u4f7f\u7528\u952e: pod-autoscaler-class \u5bf9\u6bcf\u4e2a\u7248\u672c\u8bbe\u7f6e\u4f7f\u7528\u952e: autoscaling.knative.dev/class \u53ef\u9009\u503c: \"kpa.autoscaling.knative.dev\" or \"hpa.autoscaling.knative.dev\" \u9ed8\u8ba4: \"kpa.autoscaling.knative.dev\" \u4f8b\u5b50: \u5bf9\u6bcf\u4e2a\u7248\u672c\u8bbe\u7f6e \u5168\u5c40\u8bbe\u7f6e (ConfigMap) \u5168\u5c40\u8bbe\u7f6e (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/class : \"kpa.autoscaling.knative.dev\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : pod-autoscaler-class : \"kpa.autoscaling.knative.dev\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : pod-autoscaler-class : \"kpa.autoscaling.knative.dev\" Global versus per-revision settings \u00b6 Configuring for autoscaling in Knative can be set using either global or per-revision settings. If no per-revision autoscaling settings are specified, the global settings will be used. If per-revision settings are specified, these will override the global settings when both types of settings exist. Global settings \u00b6 Global settings for autoscaling are configured using the config-autoscaler ConfigMap. If you installed Knative Serving using the Operator, you can set global configuration settings in the spec.config.autoscaler ConfigMap, located in the KnativeServing custom resource (CR). Example of the default autoscaling ConfigMap \u00b6 apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : container-concurrency-target-default : \"100\" container-concurrency-target-percentage : \"0.7\" enable-scale-to-zero : \"true\" max-scale-up-rate : \"1000\" max-scale-down-rate : \"2\" panic-window-percentage : \"10\" panic-threshold-percentage : \"200\" scale-to-zero-grace-period : \"30s\" scale-to-zero-pod-retention-period : \"0s\" stable-window : \"60s\" target-burst-capacity : \"200\" requests-per-second-target-default : \"200\" Per-revision settings \u00b6 Per-revision settings for autoscaling are configured by adding annotations to a revision. Example: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target : \"70\" Important If you are creating revisions by using a service or configuration, you must set the annotations in the revision template so that any modifications will be applied to each revision as they are created. Setting annotations in the top level metadata of a single revision will not propagate the changes to other revisions and will not apply changes to the autoscaling configuration for your application.","title":"\u652f\u6301\u7684\u81ea\u52a8\u6269\u7f29\u7c7b\u578b"},{"location":"serving/autoscaling/autoscaler-types/#_1","text":"Knative Serving \u652f\u6301 Knative Pod Autoscaler (KPA) \u548c Kubernetes\u7684Pod\u6c34\u5e73\u81ea\u52a8\u6269\u7f29 (HPA)\u3002 \u672c\u9875\u9762\u5217\u51fa\u8fd9\u4e9b\u81ea\u52a8\u6269\u7f29\u7c7b\u578b\u7684\u7279\u70b9\u548c\u9650\u5236\uff0c\u4ee5\u53ca\u5982\u4f55\u914d\u7f6e\u5b83\u4eec\u3002 \u91cd\u8981 \u5982\u679c\u4f60\u60f3\u8981\u7684\u4f7f\u7528HPA, \u4f60\u5fc5\u987b\u5728\u5b89\u88c5Knative Serving\u4e4b\u540e\u5b89\u88c5\u5b83. \u5173\u4e8e\u5982\u4f55\u5b89\u88c5HPA, \u53c2\u9605 \u5b89\u88c5\u53ef\u9009\u7684Serving\u6269\u5c55 .","title":"\u652f\u6301\u7684\u81ea\u52a8\u6269\u7f29\u7c7b\u578b"},{"location":"serving/autoscaling/autoscaler-types/#knative-pod-autoscaler-kpa","text":"Knative Serving \u6838\u5fc3\u7684\u4e00\u90e8\u5206\uff0c\u5b89\u88c5 Knative Serving \u540e\u9ed8\u8ba4\u542f\u7528\u3002 \u652f\u6301\u7f29\u5bb9\u5230\u96f6\u7684\u529f\u80fd\u3002 \u4e0d\u652f\u6301\u57fa\u4e8e CPU \u7684\u81ea\u52a8\u6269\u7f29\u3002","title":"Knative Pod Autoscaler (KPA)"},{"location":"serving/autoscaling/autoscaler-types/#pod-hpa","text":"\u4e0d\u662f Knative Serving \u6838\u5fc3\u7684\u4e00\u90e8\u5206\uff0c\u60a8\u5fc5\u987b\u5148\u5b89\u88c5 Knative Serving\u3002 \u4e0d\u652f\u6301\u7f29\u5bb9\u5230\u96f6\u7684\u529f\u80fd\u3002 \u652f\u6301\u57fa\u4e8e CPU \u7684\u81ea\u52a8\u6269\u7f29\u3002","title":"Pod\u6c34\u5e73\u81ea\u52a8\u6269\u7f29 (HPA)"},{"location":"serving/autoscaling/autoscaler-types/#_2","text":"\u81ea\u52a8\u6269\u7f29\u7684\u7c7b\u578b(KPA or HPA) \u53ef\u4ee5\u901a\u8fc7 class \u914d\u7f6e\u3002 \u5168\u5c40\u8bbe\u7f6e\u4f7f\u7528\u952e: pod-autoscaler-class \u5bf9\u6bcf\u4e2a\u7248\u672c\u8bbe\u7f6e\u4f7f\u7528\u952e: autoscaling.knative.dev/class \u53ef\u9009\u503c: \"kpa.autoscaling.knative.dev\" or \"hpa.autoscaling.knative.dev\" \u9ed8\u8ba4: \"kpa.autoscaling.knative.dev\" \u4f8b\u5b50: \u5bf9\u6bcf\u4e2a\u7248\u672c\u8bbe\u7f6e \u5168\u5c40\u8bbe\u7f6e (ConfigMap) \u5168\u5c40\u8bbe\u7f6e (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/class : \"kpa.autoscaling.knative.dev\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : pod-autoscaler-class : \"kpa.autoscaling.knative.dev\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : pod-autoscaler-class : \"kpa.autoscaling.knative.dev\"","title":"\u914d\u7f6e\u4f7f\u7528\u54ea\u79cd\u7c7b\u578b\u7684\u81ea\u52a8\u6269\u7f29"},{"location":"serving/autoscaling/autoscaler-types/#global-versus-per-revision-settings","text":"Configuring for autoscaling in Knative can be set using either global or per-revision settings. If no per-revision autoscaling settings are specified, the global settings will be used. If per-revision settings are specified, these will override the global settings when both types of settings exist.","title":"Global versus per-revision settings"},{"location":"serving/autoscaling/autoscaler-types/#global-settings","text":"Global settings for autoscaling are configured using the config-autoscaler ConfigMap. If you installed Knative Serving using the Operator, you can set global configuration settings in the spec.config.autoscaler ConfigMap, located in the KnativeServing custom resource (CR).","title":"Global settings"},{"location":"serving/autoscaling/autoscaler-types/#example-of-the-default-autoscaling-configmap","text":"apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : container-concurrency-target-default : \"100\" container-concurrency-target-percentage : \"0.7\" enable-scale-to-zero : \"true\" max-scale-up-rate : \"1000\" max-scale-down-rate : \"2\" panic-window-percentage : \"10\" panic-threshold-percentage : \"200\" scale-to-zero-grace-period : \"30s\" scale-to-zero-pod-retention-period : \"0s\" stable-window : \"60s\" target-burst-capacity : \"200\" requests-per-second-target-default : \"200\"","title":"Example of the default autoscaling ConfigMap"},{"location":"serving/autoscaling/autoscaler-types/#per-revision-settings","text":"Per-revision settings for autoscaling are configured by adding annotations to a revision. Example: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target : \"70\" Important If you are creating revisions by using a service or configuration, you must set the annotations in the revision template so that any modifications will be applied to each revision as they are created. Setting annotations in the top level metadata of a single revision will not propagate the changes to other revisions and will not apply changes to the autoscaling configuration for your application.","title":"Per-revision settings"},{"location":"serving/autoscaling/autoscaling-metrics/","text":"Metrics \u00b6 The metric configuration defines which metric type is watched by the Autoscaler. Setting metrics per revision \u00b6 For per-revision configuration, this is determined using the autoscaling.knative.dev/metric annotation. The possible metric types that can be configured per revision depend on the type of Autoscaler implementation you are using: The default KPA Autoscaler supports the concurrency and rps metrics. The HPA Autoscaler supports the cpu metric. For more information about KPA and HPA, see the documentation on Supported Autoscaler types . Per-revision annotation key: autoscaling.knative.dev/metric Possible values: \"concurrency\" , \"rps\" , \"cpu\" , \"memory\" or any custom metric name, depending on your Autoscaler type. The \"cpu\" , \"memory\" , and \"custom\" metrics are only supported on revisions that use the HPA class. Default: \"concurrency\" Per-revision concurrency configuration Per-revision rps configuration Per-revision cpu configuration Per-revision memory configuration Per-revision custom metric configuration apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/metric : \"concurrency\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/metric : \"rps\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/class : \"hpa.autoscaling.knative.dev\" autoscaling.knative.dev/metric : \"cpu\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/class : \"hpa.autoscaling.knative.dev\" autoscaling.knative.dev/metric : \"memory\" You can create an HPA to scale the revision by a metric that you specify. The HPA will be configured to use the average value of your metric over all the Pods of the revision. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/class : \"hpa.autoscaling.knative.dev\" autoscaling.knative.dev/metric : \"<metric-name>\" Where <metric-name> is your custom metric. Next steps \u00b6 Configure concurrency targets for applications Configure requests per second targets for replicas of an application","title":"\u914d\u7f6e metrics"},{"location":"serving/autoscaling/autoscaling-metrics/#metrics","text":"The metric configuration defines which metric type is watched by the Autoscaler.","title":"Metrics"},{"location":"serving/autoscaling/autoscaling-metrics/#setting-metrics-per-revision","text":"For per-revision configuration, this is determined using the autoscaling.knative.dev/metric annotation. The possible metric types that can be configured per revision depend on the type of Autoscaler implementation you are using: The default KPA Autoscaler supports the concurrency and rps metrics. The HPA Autoscaler supports the cpu metric. For more information about KPA and HPA, see the documentation on Supported Autoscaler types . Per-revision annotation key: autoscaling.knative.dev/metric Possible values: \"concurrency\" , \"rps\" , \"cpu\" , \"memory\" or any custom metric name, depending on your Autoscaler type. The \"cpu\" , \"memory\" , and \"custom\" metrics are only supported on revisions that use the HPA class. Default: \"concurrency\" Per-revision concurrency configuration Per-revision rps configuration Per-revision cpu configuration Per-revision memory configuration Per-revision custom metric configuration apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/metric : \"concurrency\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/metric : \"rps\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/class : \"hpa.autoscaling.knative.dev\" autoscaling.knative.dev/metric : \"cpu\" apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/class : \"hpa.autoscaling.knative.dev\" autoscaling.knative.dev/metric : \"memory\" You can create an HPA to scale the revision by a metric that you specify. The HPA will be configured to use the average value of your metric over all the Pods of the revision. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/class : \"hpa.autoscaling.knative.dev\" autoscaling.knative.dev/metric : \"<metric-name>\" Where <metric-name> is your custom metric.","title":"Setting metrics per revision"},{"location":"serving/autoscaling/autoscaling-metrics/#next-steps","text":"Configure concurrency targets for applications Configure requests per second targets for replicas of an application","title":"Next steps"},{"location":"serving/autoscaling/autoscaling-targets/","text":"Targets \u00b6 Configuring a target provide the Autoscaler with a value that it tries to maintain for the configured metric for a revision. See the metrics documentation for more information about configurable metric types. The target annotation, used to configure per-revision targets, is metric agnostic . This means the target is simply an integer value, which can be applied for any metric type. Configuring targets \u00b6 Global settings key: container-concurrency-target-default . For more information, see the documentation on metrics . Per-revision annotation key: autoscaling.knative.dev/target Possible values: An integer (metric agnostic). Default: \"100\" for container-concurrency-target-default . There is no default value set for the target annotation. Target annotation - Per-revision Concurrency target - Global (ConfigMap) Concurrency target - Container Global (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target : \"50\" apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : container-concurrency-target-default : \"200\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : container-concurrency-target-default : \"200\"","title":"\u914d\u7f6e targets"},{"location":"serving/autoscaling/autoscaling-targets/#targets","text":"Configuring a target provide the Autoscaler with a value that it tries to maintain for the configured metric for a revision. See the metrics documentation for more information about configurable metric types. The target annotation, used to configure per-revision targets, is metric agnostic . This means the target is simply an integer value, which can be applied for any metric type.","title":"Targets"},{"location":"serving/autoscaling/autoscaling-targets/#configuring-targets","text":"Global settings key: container-concurrency-target-default . For more information, see the documentation on metrics . Per-revision annotation key: autoscaling.knative.dev/target Possible values: An integer (metric agnostic). Default: \"100\" for container-concurrency-target-default . There is no default value set for the target annotation. Target annotation - Per-revision Concurrency target - Global (ConfigMap) Concurrency target - Container Global (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target : \"50\" apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : container-concurrency-target-default : \"200\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : container-concurrency-target-default : \"200\"","title":"Configuring targets"},{"location":"serving/autoscaling/concurrency/","text":"Configuring concurrency \u00b6 Concurrency determines the number of simultaneous requests that can be processed by each replica of an application at any given time. For per-revision concurrency, you must configure both autoscaling.knative.dev/metric and autoscaling.knative.dev/target for a soft limit , or containerConcurrency for a hard limit . For global concurrency, you can set the container-concurrency-target-default value. Soft versus hard concurrency limits \u00b6 It is possible to set either a soft or hard concurrency limit. Note If both a soft and a hard limit are specified, the smaller of the two values will be used. This prevents the Autoscaler from having a target value that is not permitted by the hard limit value. The soft limit is a targeted limit rather than a strictly enforced bound. In some situations, particularly if there is a sudden burst of requests, this value can be exceeded. The hard limit is an enforced upper bound. If concurrency reaches the hard limit, surplus requests will be buffered and must wait until enough capacity is free to execute the requests. Warning Using a hard limit configuration is only recommended if there is a clear use case for it with your application. Having a low hard limit specified may have a negative impact on the throughput and latency of an application, and may cause additional cold starts. Soft limit \u00b6 Global key: container-concurrency-target-default Per-revision annotation key: autoscaling.knative.dev/target Possible values: An integer. Default: \"100\" Example: Per Revision Global (ConfigMap) Global (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target : \"200\" apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : container-concurrency-target-default : \"200\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : container-concurrency-target-default : \"200\" Hard limit \u00b6 The hard limit is specified per Revision using the containerConcurrency field on the Revision spec. This setting is not an annotation. There is no global setting for the hard limit in the autoscaling ConfigMap, because containerConcurrency has implications outside of autoscaling, such as on buffering and queuing of requests. However, a default value can be set for the Revision's containerConcurrency field in config-defaults.yaml . The default value is 0 , meaning that there is no limit on the number of requests that are allowed to flow into the revision. A value greater than 0 specifies the exact number of requests that are allowed to flow to the replica at any one time. Global key: container-concurrency (in config-defaults.yaml ) Per-revision spec key: containerConcurrency Possible values: integer Default: 0 , meaning no limit Example: Per Revision Global (Defaults ConfigMap) Global (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containerConcurrency : 50 apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : container-concurrency : \"50\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : defaults : container-concurrency : \"50\" Target utilization \u00b6 In addition to the literal settings explained previously, concurrency values can be further adjusted by using a target utilization value . This value specifies what percentage of the previously specified target should actually be targeted by the Autoscaler. This is also known as specifying the hotness at which a replica runs, which causes the Autoscaler to scale up before the defined hard limit is reached. For example, if containerConcurrency is set to 10, and the target utilization value is set to 70 (percent), the Autoscaler will create a new replica when the average number of concurrent requests across all existing replicas reaches 7. Requests numbered 7 to 10 will still be sent to the existing replicas, but this allows for additional replicas to be started in anticipation of being needed when the containerConcurrency limit is reached. Global key: container-concurrency-target-percentage Per-revision annotation key: autoscaling.knative.dev/target-utilization-percentage Possible values: float Default: 70 Example: Per Revision Global (ConfigMap) Global (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target-utilization-percentage : \"80\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : container-concurrency-target-percentage : \"80\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : container-concurrency-target-percentage : \"80\"","title":"\u914d\u7f6e\u5e76\u53d1"},{"location":"serving/autoscaling/concurrency/#configuring-concurrency","text":"Concurrency determines the number of simultaneous requests that can be processed by each replica of an application at any given time. For per-revision concurrency, you must configure both autoscaling.knative.dev/metric and autoscaling.knative.dev/target for a soft limit , or containerConcurrency for a hard limit . For global concurrency, you can set the container-concurrency-target-default value.","title":"Configuring concurrency"},{"location":"serving/autoscaling/concurrency/#soft-versus-hard-concurrency-limits","text":"It is possible to set either a soft or hard concurrency limit. Note If both a soft and a hard limit are specified, the smaller of the two values will be used. This prevents the Autoscaler from having a target value that is not permitted by the hard limit value. The soft limit is a targeted limit rather than a strictly enforced bound. In some situations, particularly if there is a sudden burst of requests, this value can be exceeded. The hard limit is an enforced upper bound. If concurrency reaches the hard limit, surplus requests will be buffered and must wait until enough capacity is free to execute the requests. Warning Using a hard limit configuration is only recommended if there is a clear use case for it with your application. Having a low hard limit specified may have a negative impact on the throughput and latency of an application, and may cause additional cold starts.","title":"Soft versus hard concurrency limits"},{"location":"serving/autoscaling/concurrency/#soft-limit","text":"Global key: container-concurrency-target-default Per-revision annotation key: autoscaling.knative.dev/target Possible values: An integer. Default: \"100\" Example: Per Revision Global (ConfigMap) Global (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target : \"200\" apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : container-concurrency-target-default : \"200\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : container-concurrency-target-default : \"200\"","title":"Soft limit"},{"location":"serving/autoscaling/concurrency/#hard-limit","text":"The hard limit is specified per Revision using the containerConcurrency field on the Revision spec. This setting is not an annotation. There is no global setting for the hard limit in the autoscaling ConfigMap, because containerConcurrency has implications outside of autoscaling, such as on buffering and queuing of requests. However, a default value can be set for the Revision's containerConcurrency field in config-defaults.yaml . The default value is 0 , meaning that there is no limit on the number of requests that are allowed to flow into the revision. A value greater than 0 specifies the exact number of requests that are allowed to flow to the replica at any one time. Global key: container-concurrency (in config-defaults.yaml ) Per-revision spec key: containerConcurrency Possible values: integer Default: 0 , meaning no limit Example: Per Revision Global (Defaults ConfigMap) Global (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containerConcurrency : 50 apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : container-concurrency : \"50\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : defaults : container-concurrency : \"50\"","title":"Hard limit"},{"location":"serving/autoscaling/concurrency/#target-utilization","text":"In addition to the literal settings explained previously, concurrency values can be further adjusted by using a target utilization value . This value specifies what percentage of the previously specified target should actually be targeted by the Autoscaler. This is also known as specifying the hotness at which a replica runs, which causes the Autoscaler to scale up before the defined hard limit is reached. For example, if containerConcurrency is set to 10, and the target utilization value is set to 70 (percent), the Autoscaler will create a new replica when the average number of concurrent requests across all existing replicas reaches 7. Requests numbered 7 to 10 will still be sent to the existing replicas, but this allows for additional replicas to be started in anticipation of being needed when the containerConcurrency limit is reached. Global key: container-concurrency-target-percentage Per-revision annotation key: autoscaling.knative.dev/target-utilization-percentage Possible values: float Default: 70 Example: Per Revision Global (ConfigMap) Global (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target-utilization-percentage : \"80\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : container-concurrency-target-percentage : \"80\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : container-concurrency-target-percentage : \"80\"","title":"Target utilization"},{"location":"serving/autoscaling/container-freezer/","text":"Configuring container-freezer \u00b6 When container-freezer is enabled, queue-proxy calls an endpoint API when its traffic drops to zero or scales up from zero. Within the community-maintained endpoint API implementation container-freezer, the running process is frozen when the pod's traffic drops to zero, and resumed when the pod's traffic scales up from zero. However, users can also run their own implementation instead (for example, as a billing component to log when requests are being handled). Configure min-scale \u00b6 To use container-freezer, the value of per-revision annotation key autoscaling.knative.dev/min-scale must be greater than zero. Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/min-scale : \"3\" spec : containers : - image : gcr.io/knative-samples/helloworld-go Configure the endpoint API address \u00b6 Queue-proxy calls the endpoint API address when container-freezer is enabled, so you need to configure the API address. Open the config-deployment ConfigMap by running the command: kubectl edit configmap config-deployment -n knative-serving Edit the file to configure the endpoint API address, for example: apiVersion : v1 kind : ConfigMap metadata : name : config-deployment namespace : knative-serving data : concurrency-state-endpoint : \"http://$HOST_IP:9696\" Note If using the community-maintained implementation, use http://$HOST_IP:9696 as the value for concurrency-state-endpoint , as the community-maintained implementation is a daemonset and the appropriate value will be inserted by queue-proxy at runtime. If the user-specific endpoint API implementation is deployed a service in the cluster, use a specific service address such as http://billing.default.svc:9696 . Next \u00b6 Implement your own user-specific endpoint API, and deploy it in cluster. Use the community-maintained container-freezer implementation.","title":"\u914d\u7f6e\u5bb9\u5668-freezer"},{"location":"serving/autoscaling/container-freezer/#configuring-container-freezer","text":"When container-freezer is enabled, queue-proxy calls an endpoint API when its traffic drops to zero or scales up from zero. Within the community-maintained endpoint API implementation container-freezer, the running process is frozen when the pod's traffic drops to zero, and resumed when the pod's traffic scales up from zero. However, users can also run their own implementation instead (for example, as a billing component to log when requests are being handled).","title":"Configuring container-freezer"},{"location":"serving/autoscaling/container-freezer/#configure-min-scale","text":"To use container-freezer, the value of per-revision annotation key autoscaling.knative.dev/min-scale must be greater than zero. Example: Per Revision apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/min-scale : \"3\" spec : containers : - image : gcr.io/knative-samples/helloworld-go","title":"Configure min-scale"},{"location":"serving/autoscaling/container-freezer/#configure-the-endpoint-api-address","text":"Queue-proxy calls the endpoint API address when container-freezer is enabled, so you need to configure the API address. Open the config-deployment ConfigMap by running the command: kubectl edit configmap config-deployment -n knative-serving Edit the file to configure the endpoint API address, for example: apiVersion : v1 kind : ConfigMap metadata : name : config-deployment namespace : knative-serving data : concurrency-state-endpoint : \"http://$HOST_IP:9696\" Note If using the community-maintained implementation, use http://$HOST_IP:9696 as the value for concurrency-state-endpoint , as the community-maintained implementation is a daemonset and the appropriate value will be inserted by queue-proxy at runtime. If the user-specific endpoint API implementation is deployed a service in the cluster, use a specific service address such as http://billing.default.svc:9696 .","title":"Configure the endpoint API address"},{"location":"serving/autoscaling/container-freezer/#next","text":"Implement your own user-specific endpoint API, and deploy it in cluster. Use the community-maintained container-freezer implementation.","title":"Next"},{"location":"serving/autoscaling/kpa-specific/","text":"Additional autoscaling configuration for Knative Pod Autoscaler \u00b6 The following settings are specific to the Knative Pod Autoscaler (KPA). Modes \u00b6 The KPA acts on metrics ( concurrency or rps ) aggregated over time-based windows. These windows define the amount of historical data that the Autoscaler takes into account, and are used to smooth the data over the specified amount of time. The shorter these windows are, the more quickly the Autoscaler will react. The KPA's implementation has two modes: stable and panic . Stable mode is used for general operation, while panic mode by default has a much shorter window, and will be used to quickly scale a revision up if a burst of traffic arrives. Note When using panic mode, the Revision will not scale down to avoid churn. The Autoscaler will leave panic mode if there has been no reason to react quickly during the stable window timeframe. Stable window \u00b6 Global key: stable-window Per-revision annotation key: autoscaling.knative.dev/window Possible values: Duration, 6s <= value <= 1h Default: 60s Note When scaling to zero Replicas, the last Replica will only be removed after there has not been any traffic to the Revision for the entire duration of the stable window. Example: Per Revision Global (ConfigMap) Global (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/window : \"40s\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : stable-window : \"40s\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : stable-window : \"40s\" Panic window \u00b6 The panic window is defined as a percentage of the stable window to assure that both are relative to each other in a working way. This value indicates how the window over which historical data is evaluated will shrink upon entering panic mode. For example, a value of 10.0 means that in panic mode the window will be 10% of the stable window size. Global key: panic-window-percentage Per-revision annotation key: autoscaling.knative.dev/panic-window-percentage Possible values: float, 1.0 <= value <= 100.0 Default: 10.0 Example: Per Revision Global (ConfigMap) Global (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/panic-window-percentage : \"20.0\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : panic-window-percentage : \"20.0\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : panic-window-percentage : \"20.0\" Panic mode threshold \u00b6 This threshold defines when the Autoscaler will move from stable mode into panic mode. This value is a percentage of the traffic that the current amount of replicas can handle. Note A value of 100.0 (100 percent) means that the Autoscaler is always in panic mode, therefore the minimum value should be higher than 100.0 . The default setting of 200.0 means that panic mode will be start if traffic is twice as high as the current replica population can handle. Global key: panic-threshold-percentage Per-revision annotation key: autoscaling.knative.dev/panic-threshold-percentage Possible values: float, 110.0 <= value <= 1000.0 Default: 200.0 Example: Per Revision Global (ConfigMap) Global (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/panic-threshold-percentage : \"150.0\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : panic-threshold-percentage : \"150.0\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : panic-threshold-percentage : \"150.0\" Scale rates \u00b6 These settings control by how much the replica population can scale up or down in a single evaluation cycle. A minimal change of one replica in each direction is always permitted, so the Autoscaler can scale to +/- 1 replica at any time, regardless of the scale rates set. Scale up rate \u00b6 This setting determines the maximum ratio of desired to existing pods. For example, with a value of 2.0 , the revision can only scale from N to 2*N pods in one evaluation cycle. Global key: max-scale-up-rate Per-revision annotation key: n/a Possible values: float Default: 1000.0 Example: Global (ConfigMap) Global (Operator) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : max-scale-up-rate : \"500.0\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : max-scale-up-rate : \"500.0\" Scale down rate \u00b6 This setting determines the maximum ratio of existing to desired pods. For example, with a value of 2.0 , the revision can only scale from N to N/2 pods in one evaluation cycle. Global key: max-scale-down-rate Per-revision annotation key: n/a Possible values: float Default: 2.0 Example: Global (ConfigMap) Global (Operator) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : max-scale-down-rate : \"4.0\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : max-scale-down-rate : \"4.0\"","title":"Knative Pod Autoscaler\u7684\u989d\u5916\u914d\u7f6e"},{"location":"serving/autoscaling/kpa-specific/#additional-autoscaling-configuration-for-knative-pod-autoscaler","text":"The following settings are specific to the Knative Pod Autoscaler (KPA).","title":"Additional autoscaling configuration for Knative Pod Autoscaler"},{"location":"serving/autoscaling/kpa-specific/#modes","text":"The KPA acts on metrics ( concurrency or rps ) aggregated over time-based windows. These windows define the amount of historical data that the Autoscaler takes into account, and are used to smooth the data over the specified amount of time. The shorter these windows are, the more quickly the Autoscaler will react. The KPA's implementation has two modes: stable and panic . Stable mode is used for general operation, while panic mode by default has a much shorter window, and will be used to quickly scale a revision up if a burst of traffic arrives. Note When using panic mode, the Revision will not scale down to avoid churn. The Autoscaler will leave panic mode if there has been no reason to react quickly during the stable window timeframe.","title":"Modes"},{"location":"serving/autoscaling/kpa-specific/#stable-window","text":"Global key: stable-window Per-revision annotation key: autoscaling.knative.dev/window Possible values: Duration, 6s <= value <= 1h Default: 60s Note When scaling to zero Replicas, the last Replica will only be removed after there has not been any traffic to the Revision for the entire duration of the stable window. Example: Per Revision Global (ConfigMap) Global (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/window : \"40s\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : stable-window : \"40s\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : stable-window : \"40s\"","title":"Stable window"},{"location":"serving/autoscaling/kpa-specific/#panic-window","text":"The panic window is defined as a percentage of the stable window to assure that both are relative to each other in a working way. This value indicates how the window over which historical data is evaluated will shrink upon entering panic mode. For example, a value of 10.0 means that in panic mode the window will be 10% of the stable window size. Global key: panic-window-percentage Per-revision annotation key: autoscaling.knative.dev/panic-window-percentage Possible values: float, 1.0 <= value <= 100.0 Default: 10.0 Example: Per Revision Global (ConfigMap) Global (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/panic-window-percentage : \"20.0\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : panic-window-percentage : \"20.0\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : panic-window-percentage : \"20.0\"","title":"Panic window"},{"location":"serving/autoscaling/kpa-specific/#panic-mode-threshold","text":"This threshold defines when the Autoscaler will move from stable mode into panic mode. This value is a percentage of the traffic that the current amount of replicas can handle. Note A value of 100.0 (100 percent) means that the Autoscaler is always in panic mode, therefore the minimum value should be higher than 100.0 . The default setting of 200.0 means that panic mode will be start if traffic is twice as high as the current replica population can handle. Global key: panic-threshold-percentage Per-revision annotation key: autoscaling.knative.dev/panic-threshold-percentage Possible values: float, 110.0 <= value <= 1000.0 Default: 200.0 Example: Per Revision Global (ConfigMap) Global (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/panic-threshold-percentage : \"150.0\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : panic-threshold-percentage : \"150.0\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : panic-threshold-percentage : \"150.0\"","title":"Panic mode threshold"},{"location":"serving/autoscaling/kpa-specific/#scale-rates","text":"These settings control by how much the replica population can scale up or down in a single evaluation cycle. A minimal change of one replica in each direction is always permitted, so the Autoscaler can scale to +/- 1 replica at any time, regardless of the scale rates set.","title":"Scale rates"},{"location":"serving/autoscaling/kpa-specific/#scale-up-rate","text":"This setting determines the maximum ratio of desired to existing pods. For example, with a value of 2.0 , the revision can only scale from N to 2*N pods in one evaluation cycle. Global key: max-scale-up-rate Per-revision annotation key: n/a Possible values: float Default: 1000.0 Example: Global (ConfigMap) Global (Operator) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : max-scale-up-rate : \"500.0\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : max-scale-up-rate : \"500.0\"","title":"Scale up rate"},{"location":"serving/autoscaling/kpa-specific/#scale-down-rate","text":"This setting determines the maximum ratio of existing to desired pods. For example, with a value of 2.0 , the revision can only scale from N to N/2 pods in one evaluation cycle. Global key: max-scale-down-rate Per-revision annotation key: n/a Possible values: float Default: 2.0 Example: Global (ConfigMap) Global (Operator) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : max-scale-down-rate : \"4.0\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : max-scale-down-rate : \"4.0\"","title":"Scale down rate"},{"location":"serving/autoscaling/rps-target/","text":"Configuring the requests per second (RPS) target \u00b6 This setting specifies a target for requests-per-second per replica of an application. Global key: requests-per-second-target-default Per-revision annotation key: autoscaling.knative.dev/target (your revision must also be configured to use the rps metric annotation ) Possible values: An integer. Default: \"200\" Example: Per Revision Global (ConfigMap) Global (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target : \"150\" autoscaling.knative.dev/metric : \"rps\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : requests-per-second-target-default : \"150\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : requests-per-second-target-default : \"150\"","title":"\u914d\u7f6e\u6bcf\u79d2\u8bf7\u6c42\u6570 (RPS) target"},{"location":"serving/autoscaling/rps-target/#configuring-the-requests-per-second-rps-target","text":"This setting specifies a target for requests-per-second per replica of an application. Global key: requests-per-second-target-default Per-revision annotation key: autoscaling.knative.dev/target (your revision must also be configured to use the rps metric annotation ) Possible values: An integer. Default: \"200\" Example: Per Revision Global (ConfigMap) Global (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target : \"150\" autoscaling.knative.dev/metric : \"rps\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : requests-per-second-target-default : \"150\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : requests-per-second-target-default : \"150\"","title":"Configuring the requests per second (RPS) target"},{"location":"serving/autoscaling/scale-bounds/","text":"Configuring scale bounds \u00b6 You can configure upper and lower bounds to control autoscaling behavior. You can also specify the initial scale that a Revision is scaled to immediately after creation. This can be a default configuration for all Revisions, or for a specific Revision using an annotation. Lower bound \u00b6 This value controls the minimum number of replicas that each Revision should have. Knative will attempt to never have less than this number of replicas at any one point in time. Global key: min-scale Per-revision annotation key: autoscaling.knative.dev/min-scale Possible values: integer Default: 0 if scale-to-zero is enabled and class KPA is used, 1 otherwise Note For more information about scale-to-zero configuration, see the documentation on Configuring scale to zero . Example: Per Revision Global (ConfigMap) Global (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/min-scale : \"3\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : min-scale : \"3\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : min-scale : \"3\" Upper bound \u00b6 This value controls the maximum number of replicas that each revision should have. Knative will attempt to never have more than this number of replicas running, or in the process of being created, at any one point in time. If the max-scale-limit global key is set, Knative ensures that neither the global max scale nor the per-revision max scale for new revisions exceed this value. When max-scale-limit is set to a positive value, a revision with a max scale above that value (including 0, which means unlimited) is disallowed. Global key: max-scale Per-revision annotation key: autoscaling.knative.dev/max-scale Possible values: integer Default: 0 which means unlimited Example: Per Revision Global (ConfigMap) Global (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/max-scale : \"3\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : max-scale : \"3\" max-scale-limit : \"100\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : max-scale : \"3\" max-scale-limit : \"100\" Initial scale \u00b6 This value controls the initial target scale a Revision must reach immediately after it is created before it is marked as Ready . After the Revision has reached this scale one time, this value is ignored. This means that the Revision will scale down after the initial target scale is reached if the actual traffic received only needs a smaller scale. When the Revision is created, the larger of initial scale and lower bound is automatically chosen as the initial target scale. Global key: initial-scale in combination with allow-zero-initial-scale Per-revision annotation key: autoscaling.knative.dev/initial-scale Possible values: integer Default: 1 Example: Per Revision Global (ConfigMap) Global (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/initial-scale : \"0\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : initial-scale : \"0\" allow-zero-initial-scale : \"true\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : initial-scale : \"0\" allow-zero-initial-scale : \"true\" Scale Down Delay \u00b6 Scale Down Delay specifies a time window which must pass at reduced concurrency before a scale-down decision is applied. This can be useful, for example, to keep containers around for a configurable duration to avoid a cold start penalty if new requests come in. Unlike setting a lower bound, the revision will eventually be scaled down if reduced concurrency is maintained for the delay period. Note Only supported for the default KPA autoscaler class. Global key: scale-down-delay Per-revision annotation key: autoscaling.knative.dev/scale-down-delay Possible values: Duration, 0s <= value <= 1h Default: 0s (no delay) Example: Per Revision Global (ConfigMap) Global (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/scale-down-delay : \"15m\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : scale-down-delay : \"15m\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : scale-down-delay : \"15m\" Stable window \u00b6 The stable window defines the sliding time window over which metrics are averaged to provide the input for scaling decisions when the autoscaler is not in Panic mode . Global key: stable-window Per-revision annotation key: autoscaling.knative.dev/window Possible values: Duration, 6s <= value <= 1h Default: 60s Note During scale down, in most cases the last Replica is removed after there has been no traffic to the Revision for the entire duration of the stable window. Example: Per Revision Global (ConfigMap) Global (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/window : \"40s\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : stable-window : \"40s\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : stable-window : \"40s\"","title":"\u914d\u7f6e\u6269\u7f29\u5bb9\u4e0a\u4e0b\u754c"},{"location":"serving/autoscaling/scale-bounds/#configuring-scale-bounds","text":"You can configure upper and lower bounds to control autoscaling behavior. You can also specify the initial scale that a Revision is scaled to immediately after creation. This can be a default configuration for all Revisions, or for a specific Revision using an annotation.","title":"Configuring scale bounds"},{"location":"serving/autoscaling/scale-bounds/#lower-bound","text":"This value controls the minimum number of replicas that each Revision should have. Knative will attempt to never have less than this number of replicas at any one point in time. Global key: min-scale Per-revision annotation key: autoscaling.knative.dev/min-scale Possible values: integer Default: 0 if scale-to-zero is enabled and class KPA is used, 1 otherwise Note For more information about scale-to-zero configuration, see the documentation on Configuring scale to zero . Example: Per Revision Global (ConfigMap) Global (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/min-scale : \"3\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : min-scale : \"3\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : min-scale : \"3\"","title":"Lower bound"},{"location":"serving/autoscaling/scale-bounds/#upper-bound","text":"This value controls the maximum number of replicas that each revision should have. Knative will attempt to never have more than this number of replicas running, or in the process of being created, at any one point in time. If the max-scale-limit global key is set, Knative ensures that neither the global max scale nor the per-revision max scale for new revisions exceed this value. When max-scale-limit is set to a positive value, a revision with a max scale above that value (including 0, which means unlimited) is disallowed. Global key: max-scale Per-revision annotation key: autoscaling.knative.dev/max-scale Possible values: integer Default: 0 which means unlimited Example: Per Revision Global (ConfigMap) Global (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/max-scale : \"3\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : max-scale : \"3\" max-scale-limit : \"100\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : max-scale : \"3\" max-scale-limit : \"100\"","title":"Upper bound"},{"location":"serving/autoscaling/scale-bounds/#initial-scale","text":"This value controls the initial target scale a Revision must reach immediately after it is created before it is marked as Ready . After the Revision has reached this scale one time, this value is ignored. This means that the Revision will scale down after the initial target scale is reached if the actual traffic received only needs a smaller scale. When the Revision is created, the larger of initial scale and lower bound is automatically chosen as the initial target scale. Global key: initial-scale in combination with allow-zero-initial-scale Per-revision annotation key: autoscaling.knative.dev/initial-scale Possible values: integer Default: 1 Example: Per Revision Global (ConfigMap) Global (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/initial-scale : \"0\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : initial-scale : \"0\" allow-zero-initial-scale : \"true\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : initial-scale : \"0\" allow-zero-initial-scale : \"true\"","title":"Initial scale"},{"location":"serving/autoscaling/scale-bounds/#scale-down-delay","text":"Scale Down Delay specifies a time window which must pass at reduced concurrency before a scale-down decision is applied. This can be useful, for example, to keep containers around for a configurable duration to avoid a cold start penalty if new requests come in. Unlike setting a lower bound, the revision will eventually be scaled down if reduced concurrency is maintained for the delay period. Note Only supported for the default KPA autoscaler class. Global key: scale-down-delay Per-revision annotation key: autoscaling.knative.dev/scale-down-delay Possible values: Duration, 0s <= value <= 1h Default: 0s (no delay) Example: Per Revision Global (ConfigMap) Global (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/scale-down-delay : \"15m\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : scale-down-delay : \"15m\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : scale-down-delay : \"15m\"","title":"Scale Down Delay"},{"location":"serving/autoscaling/scale-bounds/#stable-window","text":"The stable window defines the sliding time window over which metrics are averaged to provide the input for scaling decisions when the autoscaler is not in Panic mode . Global key: stable-window Per-revision annotation key: autoscaling.knative.dev/window Possible values: Duration, 6s <= value <= 1h Default: 60s Note During scale down, in most cases the last Replica is removed after there has been no traffic to the Revision for the entire duration of the stable window. Example: Per Revision Global (ConfigMap) Global (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/window : \"40s\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : stable-window : \"40s\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : stable-window : \"40s\"","title":"Stable window"},{"location":"serving/autoscaling/scale-to-zero/","text":"\u914d\u7f6e\u7f29\u5bb9\u5230\u96f6 \u00b6 \u8b66\u544a \u53ea\u6709\u5f53\u4f60\u4f7f\u7528KnativePodAutoscaler (KPA)\u65f6\u624d\u53ef\u4ee5\u542f\u7528\u7f29\u5bb9\u5230\u96f6, \u5e76\u4e14\u53ea\u80fd\u5168\u5c40\u914d\u7f6e\u3002 \u6709\u5173\u4f7f\u7528 KPA \u6216\u5168\u5c40\u914d\u7f6e\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 \u652f\u6301\u7684\u81ea\u52a8\u6269\u7f29\u7c7b\u578b . \u542f\u7528\u7f29\u5bb9\u5230\u96f6 \u00b6 The scale to zero value controls whether Knative allows replicas to scale down to zero (if set to true ), or stop at 1 replica if set to false . Note For more information about scale bounds configuration per Revision, see the documentation on Configuring scale bounds . Global key: enable-scale-to-zero Per-revision annotation key: No per-revision setting. Possible values: boolean Default: true Example: Global (ConfigMap) Global (Operator) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : enable-scale-to-zero : \"false\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : enable-scale-to-zero : \"false\" Scale to zero grace period \u00b6 This setting specifies an upper bound time limit that the system will wait internally for scale-from-zero machinery to be in place before the last replica is removed. Warning This is a value that controls how long internal network programming is allowed to take, and should only be adjusted if you experience issues with requests being dropped while a Revision is scaling to zero Replicas. This setting does not adjust how long the last replica will be kept after traffic ends, and it does not guarantee that the replica will actually be kept for this entire duration. Global key: scale-to-zero-grace-period Per-revision annotation key: n/a Possible values: Duration Default: 30s Example: Global (ConfigMap) Global (Operator) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : scale-to-zero-grace-period : \"40s\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : scale-to-zero-grace-period : \"40s\" Scale to zero last pod retention period \u00b6 The scale-to-zero-pod-retention-period flag determines the minimum amount of time that the last pod will remain active after the Autoscaler decides to scale pods to zero. This contrasts with the scale-to-zero-grace-period flag, which determines the maximum amount of time that the last pod will remain active after the Autoscaler decides to scale pods to zero. Global key: scale-to-zero-pod-retention-period Per-revision annotation key: autoscaling.knative.dev/scale-to-zero-pod-retention-period Possible values: Non-negative duration string Default: 0s Example: Per Revision Global (ConfigMap) Global (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/scale-to-zero-pod-retention-period : \"1m5s\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : scale-to-zero-pod-retention-period : \"42s\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : scale-to-zero-pod-retention-period : \"42s\"","title":"\u914d\u7f6e\u7f29\u5bb9\u52300"},{"location":"serving/autoscaling/scale-to-zero/#_1","text":"\u8b66\u544a \u53ea\u6709\u5f53\u4f60\u4f7f\u7528KnativePodAutoscaler (KPA)\u65f6\u624d\u53ef\u4ee5\u542f\u7528\u7f29\u5bb9\u5230\u96f6, \u5e76\u4e14\u53ea\u80fd\u5168\u5c40\u914d\u7f6e\u3002 \u6709\u5173\u4f7f\u7528 KPA \u6216\u5168\u5c40\u914d\u7f6e\u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 \u652f\u6301\u7684\u81ea\u52a8\u6269\u7f29\u7c7b\u578b .","title":"\u914d\u7f6e\u7f29\u5bb9\u5230\u96f6"},{"location":"serving/autoscaling/scale-to-zero/#_2","text":"The scale to zero value controls whether Knative allows replicas to scale down to zero (if set to true ), or stop at 1 replica if set to false . Note For more information about scale bounds configuration per Revision, see the documentation on Configuring scale bounds . Global key: enable-scale-to-zero Per-revision annotation key: No per-revision setting. Possible values: boolean Default: true Example: Global (ConfigMap) Global (Operator) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : enable-scale-to-zero : \"false\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : enable-scale-to-zero : \"false\"","title":"\u542f\u7528\u7f29\u5bb9\u5230\u96f6"},{"location":"serving/autoscaling/scale-to-zero/#scale-to-zero-grace-period","text":"This setting specifies an upper bound time limit that the system will wait internally for scale-from-zero machinery to be in place before the last replica is removed. Warning This is a value that controls how long internal network programming is allowed to take, and should only be adjusted if you experience issues with requests being dropped while a Revision is scaling to zero Replicas. This setting does not adjust how long the last replica will be kept after traffic ends, and it does not guarantee that the replica will actually be kept for this entire duration. Global key: scale-to-zero-grace-period Per-revision annotation key: n/a Possible values: Duration Default: 30s Example: Global (ConfigMap) Global (Operator) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : scale-to-zero-grace-period : \"40s\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : scale-to-zero-grace-period : \"40s\"","title":"Scale to zero grace period"},{"location":"serving/autoscaling/scale-to-zero/#scale-to-zero-last-pod-retention-period","text":"The scale-to-zero-pod-retention-period flag determines the minimum amount of time that the last pod will remain active after the Autoscaler decides to scale pods to zero. This contrasts with the scale-to-zero-grace-period flag, which determines the maximum amount of time that the last pod will remain active after the Autoscaler decides to scale pods to zero. Global key: scale-to-zero-pod-retention-period Per-revision annotation key: autoscaling.knative.dev/scale-to-zero-pod-retention-period Possible values: Non-negative duration string Default: 0s Example: Per Revision Global (ConfigMap) Global (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/scale-to-zero-pod-retention-period : \"1m5s\" spec : containers : - image : gcr.io/knative-samples/helloworld-go apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : scale-to-zero-pod-retention-period : \"42s\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : scale-to-zero-pod-retention-period : \"42s\"","title":"Scale to zero last pod retention period"},{"location":"serving/autoscaling/autoscale-go/","text":"Autoscale Sample App - Go \u00b6 A demonstration of the autoscaling capabilities of a Knative Serving Revision. Prerequisites \u00b6 A Kubernetes cluster with Knative Serving ) installed. The hey load generator installed ( go get -u github.com/rakyll/hey ). Clone this repository, and move into the sample directory: git clone -b \"main\" https://github.com/knative/docs knative-docs cd knative-docs Deploy the Service \u00b6 Deploy the sample Knative Service: kubectl apply -f docs/serving/autoscaling/autoscale-go/service.yaml Obtain the URL of the service (once Ready ): $ kubectl get ksvc autoscale-go NAME URL LATESTCREATED LATESTREADY READY REASON autoscale-go http://autoscale-go.default.1.2.3.4.sslip.io autoscale-go-96dtk autoscale-go-96dtk True Load the Service \u00b6 Make a request to the autoscale app to see it consume some resources. curl \"http://autoscale-go.default.1.2.3.4.sslip.io?sleep=100&prime=10000&bloat=5\" Allocated 5 Mb of memory. The largest prime less than 10000 is 9973 . Slept for 100 .13 milliseconds. Send 30 seconds of traffic maintaining 50 in-flight requests. hey -z 30s -c 50 \\ \"http://autoscale-go.default.1.2.3.4.sslip.io?sleep=100&prime=10000&bloat=5\" \\ && kubectl get pods Summary: Total: 30 .3379 secs Slowest: 0 .7433 secs Fastest: 0 .1672 secs Average: 0 .2778 secs Requests/sec: 178 .7861 Total data: 542038 bytes Size/request: 99 bytes Response time histogram: 0 .167 [ 1 ] | 0 .225 [ 1462 ] | \u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0 0 .282 [ 1303 ] | \u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0 0 .340 [ 1894 ] | \u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0 0 .398 [ 471 ] | \u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0 0 .455 [ 159 ] | \u25a0\u25a0\u25a0 0 .513 [ 68 ] | \u25a0 0 .570 [ 18 ] | 0 .628 [ 14 ] | 0 .686 [ 21 ] | 0 .743 [ 13 ] | Latency distribution: 10 % in 0 .1805 secs 25 % in 0 .2197 secs 50 % in 0 .2801 secs 75 % in 0 .3129 secs 90 % in 0 .3596 secs 95 % in 0 .4020 secs 99 % in 0 .5457 secs Details ( average, fastest, slowest ) : DNS+dialup: 0 .0007 secs, 0 .1672 secs, 0 .7433 secs DNS-lookup: 0 .0000 secs, 0 .0000 secs, 0 .0000 secs req write: 0 .0001 secs, 0 .0000 secs, 0 .0045 secs resp wait: 0 .2766 secs, 0 .1669 secs, 0 .6633 secs resp read: 0 .0002 secs, 0 .0000 secs, 0 .0065 secs Status code distribution: [ 200 ] 5424 responses NAME READY STATUS RESTARTS AGE autoscale-go-00001-deployment-78cdc67bf4-2w4sk 3 /3 Running 0 26s autoscale-go-00001-deployment-78cdc67bf4-dd2zb 3 /3 Running 0 24s autoscale-go-00001-deployment-78cdc67bf4-pg55p 3 /3 Running 0 18s autoscale-go-00001-deployment-78cdc67bf4-q8bf9 3 /3 Running 0 1m autoscale-go-00001-deployment-78cdc67bf4-thjbq 3 /3 Running 0 26s Analysis \u00b6 Algorithm \u00b6 Knative Serving autoscaling is based on the average number of in-flight requests per pod (concurrency). The system has a default target concurrency of 100 (Search for container-concurrency-target-default) but we used 10 for our service. We loaded the service with 50 concurrent requests so the autoscaler created 5 pods ( 50 concurrent requests / target of 10 = 5 pods ) Panic \u00b6 The autoscaler calculates average concurrency over a 60 second window so it takes a minute for the system to stablize at the desired level of concurrency. However the autoscaler also calculates a 6 second panic window and will enter panic mode if that window reached 2x the target concurrency. In panic mode the autoscaler operates on the shorter, more sensitive panic window. Once the panic conditions are no longer met for 60 seconds, the autoscaler will return to the initial 60 second stable window. | Panic Target---> +-- | 20 | | | <------Panic Window | | Stable Target---> +------------------------- | -- | 10 CONCURRENCY | | | | <-----------Stable Window | | | --------------------------+-------------------------+--+ 0 120 60 0 TIME Customization \u00b6 The autoscaler supports customization through annotations. There are two autoscaler classes built into Knative: kpa.autoscaling.knative.dev which is the concurrency-based autoscaler described earlier (the default), and hpa.autoscaling.knative.dev which delegates to the Kubernetes HPA which autoscales on CPU usage. Example of a Service scaled on CPU: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : autoscale-go namespace : default spec : template : metadata : annotations : # Standard Kubernetes CPU-based autoscaling. autoscaling.knative.dev/class : hpa.autoscaling.knative.dev autoscaling.knative.dev/metric : cpu spec : containers : - image : gcr.io/knative-samples/autoscale-go:0.1 Additionally the autoscaler targets and scaling bounds can be specified in annotations. Example of a Service with custom targets and scale bounds: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : autoscale-go namespace : default spec : template : metadata : annotations : # Knative concurrency-based autoscaling (default). autoscaling.knative.dev/class : kpa.autoscaling.knative.dev autoscaling.knative.dev/metric : concurrency # Target 10 requests in-flight per pod. autoscaling.knative.dev/target : \"10\" # Disable scale to zero with a min scale of 1. autoscaling.knative.dev/min-scale : \"1\" # Limit scaling to 100 pods. autoscaling.knative.dev/max-scale : \"100\" spec : containers : - image : gcr.io/knative-samples/autoscale-go:0.1 Note For an hpa.autoscaling.knative.dev class Service, the autoscaling.knative.dev/target specifies the CPU percentage target (default \"80\" ). Demo \u00b6 View the Kubecon Demo of Knative autoscaler customization (32 minutes). Other Experiments \u00b6 Send 60 seconds of traffic maintaining 100 concurrent requests. hey -z 60s -c 100 \\ \"http://autoscale-go.default.1.2.3.4.sslip.io?sleep=100&prime=10000&bloat=5\" Send 60 seconds of traffic maintaining 100 qps with short requests (10 ms). hey -z 60s -q 100 \\ \"http://autoscale-go.default.1.2.3.4.sslip.io?sleep=10\" Send 60 seconds of traffic maintaining 100 qps with long requests (1 sec). hey -z 60s -q 100 \\ \"http://autoscale-go.default.1.2.3.4.sslip.io?sleep=1000\" Send 60 seconds of traffic with heavy CPU usage (~1 cpu/sec/request, total 100 cpus). hey -z 60s -q 100 \\ \"http://autoscale-go.default.1.2.3.4.sslip.io?prime=40000000\" Send 60 seconds of traffic with heavy memory usage (1 gb/request, total 5 gb). hey -z 60s -c 5 \\ \"http://autoscale-go.default.1.2.3.4.sslip.io?bloat=1000\" Cleanup \u00b6 kubectl delete -f docs/serving/autoscaling/autoscale-go/service.yaml Further reading \u00b6 Autoscaling Developer Documentation","title":"\u81ea\u52a8\u6269\u7f29\u793a\u4f8b\u7a0b\u5e8f - Go"},{"location":"serving/autoscaling/autoscale-go/#autoscale-sample-app-go","text":"A demonstration of the autoscaling capabilities of a Knative Serving Revision.","title":"Autoscale Sample App - Go"},{"location":"serving/autoscaling/autoscale-go/#prerequisites","text":"A Kubernetes cluster with Knative Serving ) installed. The hey load generator installed ( go get -u github.com/rakyll/hey ). Clone this repository, and move into the sample directory: git clone -b \"main\" https://github.com/knative/docs knative-docs cd knative-docs","title":"Prerequisites"},{"location":"serving/autoscaling/autoscale-go/#deploy-the-service","text":"Deploy the sample Knative Service: kubectl apply -f docs/serving/autoscaling/autoscale-go/service.yaml Obtain the URL of the service (once Ready ): $ kubectl get ksvc autoscale-go NAME URL LATESTCREATED LATESTREADY READY REASON autoscale-go http://autoscale-go.default.1.2.3.4.sslip.io autoscale-go-96dtk autoscale-go-96dtk True","title":"Deploy the Service"},{"location":"serving/autoscaling/autoscale-go/#load-the-service","text":"Make a request to the autoscale app to see it consume some resources. curl \"http://autoscale-go.default.1.2.3.4.sslip.io?sleep=100&prime=10000&bloat=5\" Allocated 5 Mb of memory. The largest prime less than 10000 is 9973 . Slept for 100 .13 milliseconds. Send 30 seconds of traffic maintaining 50 in-flight requests. hey -z 30s -c 50 \\ \"http://autoscale-go.default.1.2.3.4.sslip.io?sleep=100&prime=10000&bloat=5\" \\ && kubectl get pods Summary: Total: 30 .3379 secs Slowest: 0 .7433 secs Fastest: 0 .1672 secs Average: 0 .2778 secs Requests/sec: 178 .7861 Total data: 542038 bytes Size/request: 99 bytes Response time histogram: 0 .167 [ 1 ] | 0 .225 [ 1462 ] | \u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0 0 .282 [ 1303 ] | \u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0 0 .340 [ 1894 ] | \u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0 0 .398 [ 471 ] | \u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0\u25a0 0 .455 [ 159 ] | \u25a0\u25a0\u25a0 0 .513 [ 68 ] | \u25a0 0 .570 [ 18 ] | 0 .628 [ 14 ] | 0 .686 [ 21 ] | 0 .743 [ 13 ] | Latency distribution: 10 % in 0 .1805 secs 25 % in 0 .2197 secs 50 % in 0 .2801 secs 75 % in 0 .3129 secs 90 % in 0 .3596 secs 95 % in 0 .4020 secs 99 % in 0 .5457 secs Details ( average, fastest, slowest ) : DNS+dialup: 0 .0007 secs, 0 .1672 secs, 0 .7433 secs DNS-lookup: 0 .0000 secs, 0 .0000 secs, 0 .0000 secs req write: 0 .0001 secs, 0 .0000 secs, 0 .0045 secs resp wait: 0 .2766 secs, 0 .1669 secs, 0 .6633 secs resp read: 0 .0002 secs, 0 .0000 secs, 0 .0065 secs Status code distribution: [ 200 ] 5424 responses NAME READY STATUS RESTARTS AGE autoscale-go-00001-deployment-78cdc67bf4-2w4sk 3 /3 Running 0 26s autoscale-go-00001-deployment-78cdc67bf4-dd2zb 3 /3 Running 0 24s autoscale-go-00001-deployment-78cdc67bf4-pg55p 3 /3 Running 0 18s autoscale-go-00001-deployment-78cdc67bf4-q8bf9 3 /3 Running 0 1m autoscale-go-00001-deployment-78cdc67bf4-thjbq 3 /3 Running 0 26s","title":"Load the Service"},{"location":"serving/autoscaling/autoscale-go/#analysis","text":"","title":"Analysis"},{"location":"serving/autoscaling/autoscale-go/#algorithm","text":"Knative Serving autoscaling is based on the average number of in-flight requests per pod (concurrency). The system has a default target concurrency of 100 (Search for container-concurrency-target-default) but we used 10 for our service. We loaded the service with 50 concurrent requests so the autoscaler created 5 pods ( 50 concurrent requests / target of 10 = 5 pods )","title":"Algorithm"},{"location":"serving/autoscaling/autoscale-go/#panic","text":"The autoscaler calculates average concurrency over a 60 second window so it takes a minute for the system to stablize at the desired level of concurrency. However the autoscaler also calculates a 6 second panic window and will enter panic mode if that window reached 2x the target concurrency. In panic mode the autoscaler operates on the shorter, more sensitive panic window. Once the panic conditions are no longer met for 60 seconds, the autoscaler will return to the initial 60 second stable window. | Panic Target---> +-- | 20 | | | <------Panic Window | | Stable Target---> +------------------------- | -- | 10 CONCURRENCY | | | | <-----------Stable Window | | | --------------------------+-------------------------+--+ 0 120 60 0 TIME","title":"Panic"},{"location":"serving/autoscaling/autoscale-go/#customization","text":"The autoscaler supports customization through annotations. There are two autoscaler classes built into Knative: kpa.autoscaling.knative.dev which is the concurrency-based autoscaler described earlier (the default), and hpa.autoscaling.knative.dev which delegates to the Kubernetes HPA which autoscales on CPU usage. Example of a Service scaled on CPU: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : autoscale-go namespace : default spec : template : metadata : annotations : # Standard Kubernetes CPU-based autoscaling. autoscaling.knative.dev/class : hpa.autoscaling.knative.dev autoscaling.knative.dev/metric : cpu spec : containers : - image : gcr.io/knative-samples/autoscale-go:0.1 Additionally the autoscaler targets and scaling bounds can be specified in annotations. Example of a Service with custom targets and scale bounds: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : autoscale-go namespace : default spec : template : metadata : annotations : # Knative concurrency-based autoscaling (default). autoscaling.knative.dev/class : kpa.autoscaling.knative.dev autoscaling.knative.dev/metric : concurrency # Target 10 requests in-flight per pod. autoscaling.knative.dev/target : \"10\" # Disable scale to zero with a min scale of 1. autoscaling.knative.dev/min-scale : \"1\" # Limit scaling to 100 pods. autoscaling.knative.dev/max-scale : \"100\" spec : containers : - image : gcr.io/knative-samples/autoscale-go:0.1 Note For an hpa.autoscaling.knative.dev class Service, the autoscaling.knative.dev/target specifies the CPU percentage target (default \"80\" ).","title":"Customization"},{"location":"serving/autoscaling/autoscale-go/#demo","text":"View the Kubecon Demo of Knative autoscaler customization (32 minutes).","title":"Demo"},{"location":"serving/autoscaling/autoscale-go/#other-experiments","text":"Send 60 seconds of traffic maintaining 100 concurrent requests. hey -z 60s -c 100 \\ \"http://autoscale-go.default.1.2.3.4.sslip.io?sleep=100&prime=10000&bloat=5\" Send 60 seconds of traffic maintaining 100 qps with short requests (10 ms). hey -z 60s -q 100 \\ \"http://autoscale-go.default.1.2.3.4.sslip.io?sleep=10\" Send 60 seconds of traffic maintaining 100 qps with long requests (1 sec). hey -z 60s -q 100 \\ \"http://autoscale-go.default.1.2.3.4.sslip.io?sleep=1000\" Send 60 seconds of traffic with heavy CPU usage (~1 cpu/sec/request, total 100 cpus). hey -z 60s -q 100 \\ \"http://autoscale-go.default.1.2.3.4.sslip.io?prime=40000000\" Send 60 seconds of traffic with heavy memory usage (1 gb/request, total 5 gb). hey -z 60s -c 5 \\ \"http://autoscale-go.default.1.2.3.4.sslip.io?bloat=1000\"","title":"Other Experiments"},{"location":"serving/autoscaling/autoscale-go/#cleanup","text":"kubectl delete -f docs/serving/autoscaling/autoscale-go/service.yaml","title":"Cleanup"},{"location":"serving/autoscaling/autoscale-go/#further-reading","text":"Autoscaling Developer Documentation","title":"Further reading"},{"location":"serving/configuration/config-defaults/","text":"Configuring the Defaults ConfigMap \u00b6 The config-defaults ConfigMap, known as the Defaults ConfigMap, contains settings that determine how Knative sets default values for resources. This ConfigMap is located in the knative-serving namespace. You can view the current config-defaults ConfigMap by running the following command: kubectl get configmap -n knative-serving config-defaults -oyaml Example config-defaults ConfigMap \u00b6 apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-timeout-seconds : \"300\" max-revision-timeout-seconds : \"600\" revision-cpu-request : \"400m\" revision-memory-request : \"100M\" revision-ephemeral-storage-request : \"500M\" revision-cpu-limit : \"1000m\" revision-memory-limit : \"200M\" revision-ephemeral-storage-limit : \"750M\" container-name-template : \"user-container\" container-concurrency : \"0\" container-concurrency-max-limit : \"1000\" allow-container-concurrency-zero : \"true\" enable-service-links : \"false\" See below for a description of each property. Properties \u00b6 Revision Timeout Seconds \u00b6 revision-timeout-seconds contains the default number of seconds to use for the revision's per-request timeout, if none is specified. Key : revision-timeout-seconds Default : \"300\" (5 minutes) Example: apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-timeout-seconds : \"300\" # 5 minutes Max Revision Timeout Seconds \u00b6 max-revision-timeout-seconds contains the maximum number of seconds that can be used for revision-timeout-seconds. This value must be greater than or equal to revision-timeout-seconds. If omitted, the system default is used (600 seconds). If this value is increased, the activator's terminationGraceTimeSeconds should also be increased to prevent in-flight requests being disrupted. Key : max-revision-timeout-seconds Default : \"600\" (10 minutes) Example: apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : max-revision-timeout-seconds : \"600\" # 10 minutes Revision Cpu Request \u00b6 revision-cpu-request contains the cpu allocation to assign to revisions by default. If omitted, no value is specified and the system default is used. Below is an example of setting revision-cpu-request. By default, it is not set by Knative. Key : revision-cpu-request Default : \"400m\" (0.4 of a CPU (aka 400 milli-CPU)) Example: apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-cpu-request : \"400m\" # 0.4 of a CPU (aka 400 milli-CPU) Revision Memory Request \u00b6 revision-memory-request contains the memory allocation to assign to revisions by default. If omitted, no value is specified and the system default is used. Below is an example of setting revision-memory-request. By default, it is not set by Knative. Key : revision-memory-request Default : \"100M\" (100 megabytes of memory) Example: apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-memory-request : \"100M\" # 100 megabytes of memory Revision Ephemeral Storage Request \u00b6 revision-ephemeral-storage-request contains the ephemeral storage allocation to assign to revisions by default. If omitted, no value is specified and the system default is used. Key : revision-ephemeral-storage-request Default : \"500M\" (500 megabytes of storage) Example: apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-ephemeral-storage-request : \"500M\" # 500 megabytes of storage Revision Cpu Limit \u00b6 revision-cpu-limit contains the cpu allocation to limit revisions to by default. If omitted, no value is specified and the system default is used. Below is an example of setting revision-cpu-limit. By default, it is not set by Knative. Key : revision-cpu-limit Default : \"1000m\" (1 CPU (aka 1000 milli-CPU)) Example: apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-cpu-limit : \"1000m\" # 1 CPU (aka 1000 milli-CPU) Revision Memory Limit \u00b6 revision-memory-limit contains the memory allocation to limit revisions to by default. If omitted, no value is specified and the system default is used. Below is an example of setting revision-memory-limit. By default, it is not set by Knative. Key : revision-memory-limit Default : \"200M\" (200 megabytes of memory) Example: apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-memory-limit : \"200M\" # 200 megabytes of memory Revision Ephemeral Storage Limit \u00b6 revision-ephemeral-storage-limit contains the ephemeral storage allocation to limit revisions to by default. If omitted, no value is specified and the system default is used. Key : revision-ephemeral-storage-limit Default : \"750M\" (750 megabytes of storage) Example: apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-ephemeral-storage-limit : \"750M\" # 750 megabytes of storage Container Name Template \u00b6 container-name-template contains a template for the default container name, if none is specified. This field supports Go templating and is supplied with the ObjectMeta of the enclosing Service or Configuration, so values such as {{.Name}} are also valid. Key : container-name-template Default : \"user-container\" Example: apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : container-name-template : \"user-container\" Container Concurrency \u00b6 container-concurrency specifies the maximum number of requests the Container can handle at once, and requests above this threshold are queued. Setting a value of zero disables this throttling and lets through as many requests as the pod receives. Key : container-concurrency Default : \"0\" Example: apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : container-concurrency : \"0\" Container Concurrency Max Limit \u00b6 The container concurrency max limit is an operator setting ensuring that the individual revisions cannot have arbitrary large concurrency values, or autoscaling targets. container-concurrency default setting must be at or below this value. Must be greater than 1. Note Even with this set, a user can choose a containerConcurrency value of 0 (unbounded) unless allow-container-concurrency-zero is set to \"false\". Key : container-concurrency-max-limit Default : \"1000\" Example: apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : container-concurrency-max-limit : \"1000\" Allow Container Concurrency Zero \u00b6 allow-container-concurrency-zero controls whether users can specify 0 (i.e. unbounded) for containerConcurrency. Key : allow-container-concurrency-zero Default : \"true\" Example: apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : allow-container-concurrency-zero : \"true\" Enable Service Links \u00b6 enable-service-links specifies the default value used for the enableServiceLinks field of the PodSpec, when it is omitted by the user. See the Kubernetes Documentation for the enableServiceLinks Feature . This is a tri-state flag with possible values of (true|false|default). In environments with large number of services it is suggested to set this value to false . See serving#8498 . Key : enable-service-links Default : \"false\" Example: apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : enable-service-links : \"false\"","title":"Configure the Defaults ConfigMap"},{"location":"serving/configuration/config-defaults/#configuring-the-defaults-configmap","text":"The config-defaults ConfigMap, known as the Defaults ConfigMap, contains settings that determine how Knative sets default values for resources. This ConfigMap is located in the knative-serving namespace. You can view the current config-defaults ConfigMap by running the following command: kubectl get configmap -n knative-serving config-defaults -oyaml","title":"Configuring the Defaults ConfigMap"},{"location":"serving/configuration/config-defaults/#example-config-defaults-configmap","text":"apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-timeout-seconds : \"300\" max-revision-timeout-seconds : \"600\" revision-cpu-request : \"400m\" revision-memory-request : \"100M\" revision-ephemeral-storage-request : \"500M\" revision-cpu-limit : \"1000m\" revision-memory-limit : \"200M\" revision-ephemeral-storage-limit : \"750M\" container-name-template : \"user-container\" container-concurrency : \"0\" container-concurrency-max-limit : \"1000\" allow-container-concurrency-zero : \"true\" enable-service-links : \"false\" See below for a description of each property.","title":"Example config-defaults ConfigMap"},{"location":"serving/configuration/config-defaults/#properties","text":"","title":"Properties"},{"location":"serving/configuration/config-defaults/#revision-timeout-seconds","text":"revision-timeout-seconds contains the default number of seconds to use for the revision's per-request timeout, if none is specified. Key : revision-timeout-seconds Default : \"300\" (5 minutes) Example: apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-timeout-seconds : \"300\" # 5 minutes","title":"Revision Timeout Seconds"},{"location":"serving/configuration/config-defaults/#max-revision-timeout-seconds","text":"max-revision-timeout-seconds contains the maximum number of seconds that can be used for revision-timeout-seconds. This value must be greater than or equal to revision-timeout-seconds. If omitted, the system default is used (600 seconds). If this value is increased, the activator's terminationGraceTimeSeconds should also be increased to prevent in-flight requests being disrupted. Key : max-revision-timeout-seconds Default : \"600\" (10 minutes) Example: apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : max-revision-timeout-seconds : \"600\" # 10 minutes","title":"Max Revision Timeout Seconds"},{"location":"serving/configuration/config-defaults/#revision-cpu-request","text":"revision-cpu-request contains the cpu allocation to assign to revisions by default. If omitted, no value is specified and the system default is used. Below is an example of setting revision-cpu-request. By default, it is not set by Knative. Key : revision-cpu-request Default : \"400m\" (0.4 of a CPU (aka 400 milli-CPU)) Example: apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-cpu-request : \"400m\" # 0.4 of a CPU (aka 400 milli-CPU)","title":"Revision Cpu Request"},{"location":"serving/configuration/config-defaults/#revision-memory-request","text":"revision-memory-request contains the memory allocation to assign to revisions by default. If omitted, no value is specified and the system default is used. Below is an example of setting revision-memory-request. By default, it is not set by Knative. Key : revision-memory-request Default : \"100M\" (100 megabytes of memory) Example: apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-memory-request : \"100M\" # 100 megabytes of memory","title":"Revision Memory Request"},{"location":"serving/configuration/config-defaults/#revision-ephemeral-storage-request","text":"revision-ephemeral-storage-request contains the ephemeral storage allocation to assign to revisions by default. If omitted, no value is specified and the system default is used. Key : revision-ephemeral-storage-request Default : \"500M\" (500 megabytes of storage) Example: apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-ephemeral-storage-request : \"500M\" # 500 megabytes of storage","title":"Revision Ephemeral Storage Request"},{"location":"serving/configuration/config-defaults/#revision-cpu-limit","text":"revision-cpu-limit contains the cpu allocation to limit revisions to by default. If omitted, no value is specified and the system default is used. Below is an example of setting revision-cpu-limit. By default, it is not set by Knative. Key : revision-cpu-limit Default : \"1000m\" (1 CPU (aka 1000 milli-CPU)) Example: apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-cpu-limit : \"1000m\" # 1 CPU (aka 1000 milli-CPU)","title":"Revision Cpu Limit"},{"location":"serving/configuration/config-defaults/#revision-memory-limit","text":"revision-memory-limit contains the memory allocation to limit revisions to by default. If omitted, no value is specified and the system default is used. Below is an example of setting revision-memory-limit. By default, it is not set by Knative. Key : revision-memory-limit Default : \"200M\" (200 megabytes of memory) Example: apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-memory-limit : \"200M\" # 200 megabytes of memory","title":"Revision Memory Limit"},{"location":"serving/configuration/config-defaults/#revision-ephemeral-storage-limit","text":"revision-ephemeral-storage-limit contains the ephemeral storage allocation to limit revisions to by default. If omitted, no value is specified and the system default is used. Key : revision-ephemeral-storage-limit Default : \"750M\" (750 megabytes of storage) Example: apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : revision-ephemeral-storage-limit : \"750M\" # 750 megabytes of storage","title":"Revision Ephemeral Storage Limit"},{"location":"serving/configuration/config-defaults/#container-name-template","text":"container-name-template contains a template for the default container name, if none is specified. This field supports Go templating and is supplied with the ObjectMeta of the enclosing Service or Configuration, so values such as {{.Name}} are also valid. Key : container-name-template Default : \"user-container\" Example: apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : container-name-template : \"user-container\"","title":"Container Name Template"},{"location":"serving/configuration/config-defaults/#container-concurrency","text":"container-concurrency specifies the maximum number of requests the Container can handle at once, and requests above this threshold are queued. Setting a value of zero disables this throttling and lets through as many requests as the pod receives. Key : container-concurrency Default : \"0\" Example: apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : container-concurrency : \"0\"","title":"Container Concurrency"},{"location":"serving/configuration/config-defaults/#container-concurrency-max-limit","text":"The container concurrency max limit is an operator setting ensuring that the individual revisions cannot have arbitrary large concurrency values, or autoscaling targets. container-concurrency default setting must be at or below this value. Must be greater than 1. Note Even with this set, a user can choose a containerConcurrency value of 0 (unbounded) unless allow-container-concurrency-zero is set to \"false\". Key : container-concurrency-max-limit Default : \"1000\" Example: apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : container-concurrency-max-limit : \"1000\"","title":"Container Concurrency Max Limit"},{"location":"serving/configuration/config-defaults/#allow-container-concurrency-zero","text":"allow-container-concurrency-zero controls whether users can specify 0 (i.e. unbounded) for containerConcurrency. Key : allow-container-concurrency-zero Default : \"true\" Example: apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : allow-container-concurrency-zero : \"true\"","title":"Allow Container Concurrency Zero"},{"location":"serving/configuration/config-defaults/#enable-service-links","text":"enable-service-links specifies the default value used for the enableServiceLinks field of the PodSpec, when it is omitted by the user. See the Kubernetes Documentation for the enableServiceLinks Feature . This is a tri-state flag with possible values of (true|false|default). In environments with large number of services it is suggested to set this value to false . See serving#8498 . Key : enable-service-links Default : \"false\" Example: apiVersion : v1 kind : ConfigMap metadata : name : config-defaults namespace : knative-serving data : enable-service-links : \"false\"","title":"Enable Service Links"},{"location":"serving/configuration/deployment/","text":"Configure Deployment resources \u00b6 The config-deployment ConfigMap, known as the Deployment ConfigMap, contains settings that determine how Kubernetes Deployment resources, which back Knative services, are configured. This ConfigMap is located in the knative-serving namespace. You can view the current config-deployment ConfigMap by running the following command: kubectl get configmap -n knative-serving config-deployment -oyaml Example config-deployment ConfigMap \u00b6 apiVersion : v1 kind : ConfigMap metadata : name : config-deployment namespace : knative-serving labels : serving.knative.dev/release : devel annotations : knative.dev/example-checksum : \"fa67b403\" data : # This is the Go import path for the binary that is containerized # and substituted here. queue-sidecar-image : ko://knative.dev/serving/cmd/queue # List of repositories for which tag to digest resolving should be skipped registries-skipping-tag-resolving : \"kind.local,ko.local,dev.local\" # digest-resolution-timeout is the maximum time allowed for an image's # digests to be resolved. digest-resolution-timeout : \"10s\" # progress-deadline is the duration we wait for the deployment to # be ready before considering it failed. progress-deadline : \"600s\" # queue-sidecar-cpu-request is the requests.cpu to set for the queue proxy sidecar container. # If omitted, a default value (currently \"25m\"), is used. queue-sidecar-cpu-request : \"25m\" # queue-sidecar-cpu-limit is the limits.cpu to set for the queue proxy sidecar container. # If omitted, no value is specified and the system default is used. queue-sidecar-cpu-limit : \"1000m\" # queue-sidecar-memory-request is the requests.memory to set for the queue proxy container. # If omitted, no value is specified and the system default is used. queue-sidecar-memory-request : \"400Mi\" # queue-sidecar-memory-limit is the limits.memory to set for the queue proxy container. # If omitted, no value is specified and the system default is used. queue-sidecar-memory-limit : \"800Mi\" # queue-sidecar-ephemeral-storage-request is the requests.ephemeral-storage to # set for the queue proxy sidecar container. # If omitted, no value is specified and the system default is used. queue-sidecar-ephemeral-storage-request : \"512Mi\" # queue-sidecar-ephemeral-storage-limit is the limits.ephemeral-storage to set # for the queue proxy sidecar container. # If omitted, no value is specified and the system default is used. queue-sidecar-ephemeral-storage-limit : \"1024Mi\" # concurrency-state-endpoint is the endpoint that queue-proxy calls when its traffic drops to zero or # scales up from zero. concurrency-state-endpoint : \"\" Configuring progress deadlines \u00b6 Configuring progress deadline settings allows you to specify the maximum time, either in seconds or minutes, that you will wait for your Deployment to progress before the system reports back that the Deployment has failed progressing for the Knative Revision. The default progress deadline is 600 seconds. This value is expressed as a Golang time.Duration string representation, and must be rounded to a second precision. The Knative Autoscaler component scales the revision to 0, and the Knative service enters a terminal Failed state, if the initial scale cannot be achieved within the time limit defined by this setting. You may want to configure this setting as a higher value if any of the following issues occur in your Knative deployment: It takes a long time to pull the Service image, due to the size of the image. It takes a long time for the Service to become READY , due to priming of the initial cache state. The cluster is relies on cluster autoscaling to allocate resources for new pods. See the Kubernetes documentation for more information. The following example shows a snippet of an example Deployment Config Map that sets this value to 10 minutes: apiVersion : v1 kind : ConfigMap metadata : name : config-deployment namespace : knative-serving labels : serving.knative.dev/release : devel annotations : knative.dev/example-checksum : \"fa67b403\" data : progress-deadline : \"10m\" Skipping tag resolution \u00b6 You can configure Knative Serving to skip tag resolution for Deployments by modifying the registries-skipping-tag-resolving ConfigMap setting. The following example shows how to disable tag resolution for registry.example.com : apiVersion : v1 kind : ConfigMap metadata : name : config-deployment namespace : knative-serving labels : serving.knative.dev/release : devel annotations : knative.dev/example-checksum : \"fa67b403\" data : # List of repositories for which tag to digest resolving should be skipped registries-skipping-tag-resolving : registry.example.com Enable container-freezer service \u00b6 You can configure queue-proxy to pause pods when not in use by enabling the container-freezer service. It calls a stand-alone service (via a user-specified endpoint) when a pod's traffic drops to zero or scales up from zero. To enable it, set concurrency-state-endpoint to a non-empty value. With this configuration, you can achieve some features like freezing running processes in pods or billing based on the time it takes to process the requests. Before you configure this, you need to implement the endpoint API. The official implementation is container-freezer. You can install it by following the installation instructions in the container-freezer README . The following example shows how to enable the container-freezer service. When using $HOST_IP , the container-freezer service inserts the appropriate value for each node at runtime: apiVersion : v1 kind : ConfigMap metadata : name : config-deployment namespace : knative-serving labels : serving.knative.dev/release : devel annotations : knative.dev/example-checksum : \"fa67b403\" data : concurrency-state-endpoint : \"http://$HOST_IP:9696\"","title":"Configure Deployment resources"},{"location":"serving/configuration/deployment/#configure-deployment-resources","text":"The config-deployment ConfigMap, known as the Deployment ConfigMap, contains settings that determine how Kubernetes Deployment resources, which back Knative services, are configured. This ConfigMap is located in the knative-serving namespace. You can view the current config-deployment ConfigMap by running the following command: kubectl get configmap -n knative-serving config-deployment -oyaml","title":"Configure Deployment resources"},{"location":"serving/configuration/deployment/#example-config-deployment-configmap","text":"apiVersion : v1 kind : ConfigMap metadata : name : config-deployment namespace : knative-serving labels : serving.knative.dev/release : devel annotations : knative.dev/example-checksum : \"fa67b403\" data : # This is the Go import path for the binary that is containerized # and substituted here. queue-sidecar-image : ko://knative.dev/serving/cmd/queue # List of repositories for which tag to digest resolving should be skipped registries-skipping-tag-resolving : \"kind.local,ko.local,dev.local\" # digest-resolution-timeout is the maximum time allowed for an image's # digests to be resolved. digest-resolution-timeout : \"10s\" # progress-deadline is the duration we wait for the deployment to # be ready before considering it failed. progress-deadline : \"600s\" # queue-sidecar-cpu-request is the requests.cpu to set for the queue proxy sidecar container. # If omitted, a default value (currently \"25m\"), is used. queue-sidecar-cpu-request : \"25m\" # queue-sidecar-cpu-limit is the limits.cpu to set for the queue proxy sidecar container. # If omitted, no value is specified and the system default is used. queue-sidecar-cpu-limit : \"1000m\" # queue-sidecar-memory-request is the requests.memory to set for the queue proxy container. # If omitted, no value is specified and the system default is used. queue-sidecar-memory-request : \"400Mi\" # queue-sidecar-memory-limit is the limits.memory to set for the queue proxy container. # If omitted, no value is specified and the system default is used. queue-sidecar-memory-limit : \"800Mi\" # queue-sidecar-ephemeral-storage-request is the requests.ephemeral-storage to # set for the queue proxy sidecar container. # If omitted, no value is specified and the system default is used. queue-sidecar-ephemeral-storage-request : \"512Mi\" # queue-sidecar-ephemeral-storage-limit is the limits.ephemeral-storage to set # for the queue proxy sidecar container. # If omitted, no value is specified and the system default is used. queue-sidecar-ephemeral-storage-limit : \"1024Mi\" # concurrency-state-endpoint is the endpoint that queue-proxy calls when its traffic drops to zero or # scales up from zero. concurrency-state-endpoint : \"\"","title":"Example config-deployment ConfigMap"},{"location":"serving/configuration/deployment/#configuring-progress-deadlines","text":"Configuring progress deadline settings allows you to specify the maximum time, either in seconds or minutes, that you will wait for your Deployment to progress before the system reports back that the Deployment has failed progressing for the Knative Revision. The default progress deadline is 600 seconds. This value is expressed as a Golang time.Duration string representation, and must be rounded to a second precision. The Knative Autoscaler component scales the revision to 0, and the Knative service enters a terminal Failed state, if the initial scale cannot be achieved within the time limit defined by this setting. You may want to configure this setting as a higher value if any of the following issues occur in your Knative deployment: It takes a long time to pull the Service image, due to the size of the image. It takes a long time for the Service to become READY , due to priming of the initial cache state. The cluster is relies on cluster autoscaling to allocate resources for new pods. See the Kubernetes documentation for more information. The following example shows a snippet of an example Deployment Config Map that sets this value to 10 minutes: apiVersion : v1 kind : ConfigMap metadata : name : config-deployment namespace : knative-serving labels : serving.knative.dev/release : devel annotations : knative.dev/example-checksum : \"fa67b403\" data : progress-deadline : \"10m\"","title":"Configuring progress deadlines"},{"location":"serving/configuration/deployment/#skipping-tag-resolution","text":"You can configure Knative Serving to skip tag resolution for Deployments by modifying the registries-skipping-tag-resolving ConfigMap setting. The following example shows how to disable tag resolution for registry.example.com : apiVersion : v1 kind : ConfigMap metadata : name : config-deployment namespace : knative-serving labels : serving.knative.dev/release : devel annotations : knative.dev/example-checksum : \"fa67b403\" data : # List of repositories for which tag to digest resolving should be skipped registries-skipping-tag-resolving : registry.example.com","title":"Skipping tag resolution"},{"location":"serving/configuration/deployment/#enable-container-freezer-service","text":"You can configure queue-proxy to pause pods when not in use by enabling the container-freezer service. It calls a stand-alone service (via a user-specified endpoint) when a pod's traffic drops to zero or scales up from zero. To enable it, set concurrency-state-endpoint to a non-empty value. With this configuration, you can achieve some features like freezing running processes in pods or billing based on the time it takes to process the requests. Before you configure this, you need to implement the endpoint API. The official implementation is container-freezer. You can install it by following the installation instructions in the container-freezer README . The following example shows how to enable the container-freezer service. When using $HOST_IP , the container-freezer service inserts the appropriate value for each node at runtime: apiVersion : v1 kind : ConfigMap metadata : name : config-deployment namespace : knative-serving labels : serving.knative.dev/release : devel annotations : knative.dev/example-checksum : \"fa67b403\" data : concurrency-state-endpoint : \"http://$HOST_IP:9696\"","title":"Enable container-freezer service"},{"location":"serving/configuration/feature-flags/","text":"Feature and extension flags \u00b6 The Knative API is designed to be portable, and abstracts away specific implementation details for user deployments. The intention of the API is to empower users to surface extra features and extensions that are possible within their platform of choice. This document introduces two concepts: Feature A way to stage the introduction of features to the Knative API. Extension A way to extend Knative beyond the portable concepts of the Knative API. Configuring flags \u00b6 Features and extensions are controlled by flags . You can define flags in the config-features ConfigMap in the knative-serving namespace. Flags can have the following values: Enabled The feature or extension is enabled and currently in use. Allowed The feature or extension is enabled and can be used, for example, by using an additional annotation or spec configuration for a resource. Disabled The feature cannot be used. Lifecyle \u00b6 When features and extensions are introduced to Knative, they follow a lifecycle of three stages: Alpha stage Might contain bugs. Support for the feature might be dropped at any time without notice. The API might change in a later software release in ways that make it incompatible with older releases without notice. Recommended for use only in short-lived testing clusters, due to increased risk of bugs and lack of long-term support. Beta stage The feature is well tested and safe to enable. Support for the overall feature will not be dropped, though details might change. The schema and semantics of objects might change in incompatible ways in a subsequent beta or stable release. If this happens, instructions are provided for migrating to the next version. These types of changes might require you to delete, modify, or re-create API objects, and might require downtime for applications that rely on the feature. Recommended for only non-business-critical uses because of the potential for incompatible changes in subsequent releases. If you have multiple clusters that can be upgraded independently, you might be able to relax this restriction. General Availability (GA) stage Stable versions of the feature or extension are included in official, stable Knative releases. Feature lifecycle stages \u00b6 Features use flags to safely introduce new changes to the Knative API. The following definitions explain the default implementation for features at different stages: Alpha stage The feature is disabled by default, but you can manually enable it. Beta stage The feature is enabled by default, but you can manually disable it. GA stage The feature is always enabled; you cannot disable it. The corresponding feature flag is no longer needed and is removed from Knative. Extension lifecycle stages \u00b6 An extension surfaces details of a specific Knative implementation, or features of the underlying environment. Note Extensions are never included in the core Knative API due to their lack of portability. Each extension is always controlled by a flag and is never enabled by default. Alpha stage The feature is disabled by default, but you can manually enable it. Beta stage The feature is allowed by default. GA stage The feature is allowed by default. Available Flags \u00b6 Multiple containers \u00b6 Type : Feature ConfigMap key: multi-container This flag allows specifying multiple user containers in a Knative Service spec. Only one container can handle requests, so exactly one container must have a port specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : containers : - name : first-container image : gcr.io/knative-samples/helloworld-go ports : - containerPort : 8080 - name : second-container image : gcr.io/knative-samples/helloworld-java Kubernetes EmptyDir Volume \u00b6 Type : Extension ConfigMap key: kubernetes.podspec-volumes-emptydir This extension controls whether emptyDir volumes can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : containers : ... volumeMounts : - name : cache mountPath : /cache volumes : - name : cache emptyDir : {} Kubernetes PersistentVolumeClaim (PVC) \u00b6 Type : Extension ConfigMap keys: kubernetes.podspec-persistent-volume-claim kubernetes.podspec-persistent-volume-write This extension controls whether PersistentVolumeClaim (PVC) can be specified and whether write access is allowed for the corresponding volume. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : containers : ... volumeMounts : - mountPath : /data name : mydata readOnly : true volumes : - name : mydata persistentVolumeClaim : claimName : minio-pv-claim readOnly : true Kubernetes node affinity \u00b6 Type : Extension ConfigMap key: kubernetes.podspec-affinity This extension controls whether node affinity can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : affinity : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : kubernetes.io/e2e-az-name operator : In values : - e2e-az1 - e2e-az2 Kubernetes host aliases \u00b6 Type : Extension ConfigMap key: kubernetes.podspec-hostaliases This flag controls whether host aliases can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : hostAliases : - ip : \"127.0.0.1\" hostnames : - \"foo.local\" - \"bar.local\" Kubernetes node selector \u00b6 Type : Extension ConfigMap key: kubernetes.podspec-nodeselector This flag controls whether node selector can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : nodeSelector : labelName : labelValue Kubernetes toleration \u00b6 Type : Extension ConfigMap key: kubernetes.podspec-tolerations This flag controls whether tolerations can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : tolerations : - key : \"example-key\" operator : \"Exists\" effect : \"NoSchedule\" Kubernetes Downward API \u00b6 Type : Extension ConfigMap key: kubernetes.podspec-fieldref This flag controls whether the Downward API (environment variable based) can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : containers : - name : user-container image : gcr.io/knative-samples/helloworld-go env : - name : MY_NODE_NAME valueFrom : fieldRef : fieldPath : spec.nodeName Kubernetes priority class name \u00b6 Type : extension ConfigMap key: kubernetes.podspec-priorityclassname This flag controls whether the priorityClassName can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : priorityClassName : high-priority ... Kubernetes dry run \u00b6 Type : Extension ConfigMap key: kubernetes.podspec-dryrun This flag controls whether Knative attempts to validate the Pod spec derived from a Knative Service spec, by using the Kubernetes API server before accepting the object. When this extension is enabled , the server always runs this validation. When this extension is allowed , the server does not run this validation by default. When this extension is allowed , you can run this validation for individual Services, by adding the features.knative.dev/podspec-dryrun\":\"enabled\" annotation: apiVersion : serving.knative.dev/v1 kind : Service metadata : annotations : features.knative.dev/podspec-dryrun\":\"enabled\" ... Kubernetes runtime class \u00b6 Type : Extension ConfigMap key: kubernetes.podspec-runtimeclass This flag controls whether the runtime class can be used. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : runtimeClassName : myclass ... Kubernetes security context \u00b6 Type : Extension ConfigMap key: kubernetes.podspec-securitycontext This flag controls whether a subset of the security context can be used. When set to enabled or allowed , the following PodSecurityContext properties are permitted: FSGroup RunAsGroup RunAsNonRoot SupplementalGroups RunAsUser When set to enabled or allowed , the following container SecurityContext properties are permitted: RunAsNonRoot (also allowed without this flag only when set to true) RunAsGroup RunAsUser (already allowed without this flag) Warning Use this flag with caution. PodSecurityContext properties can affect non-user sidecar containers that come from Knative or your service mesh. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : securityContext : runAsUser : 1000 ... Kubernetes security context capabilities \u00b6 Type : Extension ConfigMap key : kubernetes.containerspec-addcapabilities This flag controls whether users can add capabilities on the securityContext of the container. When set to enabled or allowed it allows Linux capabilities to be added to the container. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go env : - name : TARGET value : \"Go Sample v1\" securityContext : capabilities : add : - NET_BIND_SERVICE Tag header based routing \u00b6 Type : Extension ConfigMap key: tag-header-based-routing This flags controls whether tag header based routing is enabled. Kubernetes init containers \u00b6 Type : Extension ConfigMap key: kubernetes.podspec-init-containers This flag controls whether init containers can be used. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : ... initContainers : - name : init-myservice image : busybox command : [ 'sh' , '-c' , \"service_setup.sh\" ] ...","title":"Feature and extension flags"},{"location":"serving/configuration/feature-flags/#feature-and-extension-flags","text":"The Knative API is designed to be portable, and abstracts away specific implementation details for user deployments. The intention of the API is to empower users to surface extra features and extensions that are possible within their platform of choice. This document introduces two concepts: Feature A way to stage the introduction of features to the Knative API. Extension A way to extend Knative beyond the portable concepts of the Knative API.","title":"Feature and extension flags"},{"location":"serving/configuration/feature-flags/#configuring-flags","text":"Features and extensions are controlled by flags . You can define flags in the config-features ConfigMap in the knative-serving namespace. Flags can have the following values: Enabled The feature or extension is enabled and currently in use. Allowed The feature or extension is enabled and can be used, for example, by using an additional annotation or spec configuration for a resource. Disabled The feature cannot be used.","title":"Configuring flags"},{"location":"serving/configuration/feature-flags/#lifecyle","text":"When features and extensions are introduced to Knative, they follow a lifecycle of three stages: Alpha stage Might contain bugs. Support for the feature might be dropped at any time without notice. The API might change in a later software release in ways that make it incompatible with older releases without notice. Recommended for use only in short-lived testing clusters, due to increased risk of bugs and lack of long-term support. Beta stage The feature is well tested and safe to enable. Support for the overall feature will not be dropped, though details might change. The schema and semantics of objects might change in incompatible ways in a subsequent beta or stable release. If this happens, instructions are provided for migrating to the next version. These types of changes might require you to delete, modify, or re-create API objects, and might require downtime for applications that rely on the feature. Recommended for only non-business-critical uses because of the potential for incompatible changes in subsequent releases. If you have multiple clusters that can be upgraded independently, you might be able to relax this restriction. General Availability (GA) stage Stable versions of the feature or extension are included in official, stable Knative releases.","title":"Lifecyle"},{"location":"serving/configuration/feature-flags/#feature-lifecycle-stages","text":"Features use flags to safely introduce new changes to the Knative API. The following definitions explain the default implementation for features at different stages: Alpha stage The feature is disabled by default, but you can manually enable it. Beta stage The feature is enabled by default, but you can manually disable it. GA stage The feature is always enabled; you cannot disable it. The corresponding feature flag is no longer needed and is removed from Knative.","title":"Feature lifecycle stages"},{"location":"serving/configuration/feature-flags/#extension-lifecycle-stages","text":"An extension surfaces details of a specific Knative implementation, or features of the underlying environment. Note Extensions are never included in the core Knative API due to their lack of portability. Each extension is always controlled by a flag and is never enabled by default. Alpha stage The feature is disabled by default, but you can manually enable it. Beta stage The feature is allowed by default. GA stage The feature is allowed by default.","title":"Extension lifecycle stages"},{"location":"serving/configuration/feature-flags/#available-flags","text":"","title":"Available Flags"},{"location":"serving/configuration/feature-flags/#multiple-containers","text":"Type : Feature ConfigMap key: multi-container This flag allows specifying multiple user containers in a Knative Service spec. Only one container can handle requests, so exactly one container must have a port specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : containers : - name : first-container image : gcr.io/knative-samples/helloworld-go ports : - containerPort : 8080 - name : second-container image : gcr.io/knative-samples/helloworld-java","title":"Multiple containers"},{"location":"serving/configuration/feature-flags/#kubernetes-emptydir-volume","text":"Type : Extension ConfigMap key: kubernetes.podspec-volumes-emptydir This extension controls whether emptyDir volumes can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : containers : ... volumeMounts : - name : cache mountPath : /cache volumes : - name : cache emptyDir : {}","title":"Kubernetes EmptyDir Volume"},{"location":"serving/configuration/feature-flags/#kubernetes-persistentvolumeclaim-pvc","text":"Type : Extension ConfigMap keys: kubernetes.podspec-persistent-volume-claim kubernetes.podspec-persistent-volume-write This extension controls whether PersistentVolumeClaim (PVC) can be specified and whether write access is allowed for the corresponding volume. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : containers : ... volumeMounts : - mountPath : /data name : mydata readOnly : true volumes : - name : mydata persistentVolumeClaim : claimName : minio-pv-claim readOnly : true","title":"Kubernetes PersistentVolumeClaim (PVC)"},{"location":"serving/configuration/feature-flags/#kubernetes-node-affinity","text":"Type : Extension ConfigMap key: kubernetes.podspec-affinity This extension controls whether node affinity can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : affinity : nodeAffinity : requiredDuringSchedulingIgnoredDuringExecution : nodeSelectorTerms : - matchExpressions : - key : kubernetes.io/e2e-az-name operator : In values : - e2e-az1 - e2e-az2","title":"Kubernetes node affinity"},{"location":"serving/configuration/feature-flags/#kubernetes-host-aliases","text":"Type : Extension ConfigMap key: kubernetes.podspec-hostaliases This flag controls whether host aliases can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : hostAliases : - ip : \"127.0.0.1\" hostnames : - \"foo.local\" - \"bar.local\"","title":"Kubernetes host aliases"},{"location":"serving/configuration/feature-flags/#kubernetes-node-selector","text":"Type : Extension ConfigMap key: kubernetes.podspec-nodeselector This flag controls whether node selector can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : nodeSelector : labelName : labelValue","title":"Kubernetes node selector"},{"location":"serving/configuration/feature-flags/#kubernetes-toleration","text":"Type : Extension ConfigMap key: kubernetes.podspec-tolerations This flag controls whether tolerations can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : tolerations : - key : \"example-key\" operator : \"Exists\" effect : \"NoSchedule\"","title":"Kubernetes toleration"},{"location":"serving/configuration/feature-flags/#kubernetes-downward-api","text":"Type : Extension ConfigMap key: kubernetes.podspec-fieldref This flag controls whether the Downward API (environment variable based) can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : containers : - name : user-container image : gcr.io/knative-samples/helloworld-go env : - name : MY_NODE_NAME valueFrom : fieldRef : fieldPath : spec.nodeName","title":"Kubernetes Downward API"},{"location":"serving/configuration/feature-flags/#kubernetes-priority-class-name","text":"Type : extension ConfigMap key: kubernetes.podspec-priorityclassname This flag controls whether the priorityClassName can be specified. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : priorityClassName : high-priority ...","title":"Kubernetes priority class name"},{"location":"serving/configuration/feature-flags/#kubernetes-dry-run","text":"Type : Extension ConfigMap key: kubernetes.podspec-dryrun This flag controls whether Knative attempts to validate the Pod spec derived from a Knative Service spec, by using the Kubernetes API server before accepting the object. When this extension is enabled , the server always runs this validation. When this extension is allowed , the server does not run this validation by default. When this extension is allowed , you can run this validation for individual Services, by adding the features.knative.dev/podspec-dryrun\":\"enabled\" annotation: apiVersion : serving.knative.dev/v1 kind : Service metadata : annotations : features.knative.dev/podspec-dryrun\":\"enabled\" ...","title":"Kubernetes dry run"},{"location":"serving/configuration/feature-flags/#kubernetes-runtime-class","text":"Type : Extension ConfigMap key: kubernetes.podspec-runtimeclass This flag controls whether the runtime class can be used. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : runtimeClassName : myclass ...","title":"Kubernetes runtime class"},{"location":"serving/configuration/feature-flags/#kubernetes-security-context","text":"Type : Extension ConfigMap key: kubernetes.podspec-securitycontext This flag controls whether a subset of the security context can be used. When set to enabled or allowed , the following PodSecurityContext properties are permitted: FSGroup RunAsGroup RunAsNonRoot SupplementalGroups RunAsUser When set to enabled or allowed , the following container SecurityContext properties are permitted: RunAsNonRoot (also allowed without this flag only when set to true) RunAsGroup RunAsUser (already allowed without this flag) Warning Use this flag with caution. PodSecurityContext properties can affect non-user sidecar containers that come from Knative or your service mesh. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : securityContext : runAsUser : 1000 ...","title":"Kubernetes security context"},{"location":"serving/configuration/feature-flags/#kubernetes-security-context-capabilities","text":"Type : Extension ConfigMap key : kubernetes.containerspec-addcapabilities This flag controls whether users can add capabilities on the securityContext of the container. When set to enabled or allowed it allows Linux capabilities to be added to the container. apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go env : - name : TARGET value : \"Go Sample v1\" securityContext : capabilities : add : - NET_BIND_SERVICE","title":"Kubernetes security context capabilities"},{"location":"serving/configuration/feature-flags/#tag-header-based-routing","text":"Type : Extension ConfigMap key: tag-header-based-routing This flags controls whether tag header based routing is enabled.","title":"Tag header based routing"},{"location":"serving/configuration/feature-flags/#kubernetes-init-containers","text":"Type : Extension ConfigMap key: kubernetes.podspec-init-containers This flag controls whether init containers can be used. apiVersion : serving.knative.dev/v1 kind : Service ... spec : template : spec : ... initContainers : - name : init-myservice image : busybox command : [ 'sh' , '-c' , \"service_setup.sh\" ] ...","title":"Kubernetes init containers"},{"location":"serving/configuration/revision-gc/","text":"Revision garbage collection \u00b6 The config-gc ConfigMap contains settings that determine in-active revisions are cleaned up. This ConfigMap is located in the knative-serving namespace. Cluster-wide configuration \u00b6 The following properties allow you to configure revision garbage collection: Name Description retain-since-create-time Duration since creation before considering a revision for GC or \"disabled\" retain-since-last-active-time Duration since active before considering a revision for GC or \"disabled\" min-non-active-revisions Minimum number of non-active revisions to retain. max-non-active-revisions Maximum number of non-active revisions to retain or \"disabled\" to disable any maximum limit. Revisions are retained if they belong to any one of the following categories: Is active and is being reference by a route Created within retain-since-create-time Last referenced by a route within retain-since-last-active-time There are fewer than min-non-active-revisions Examples \u00b6 Immediately collect any inactive revision \u00b6 apiVersion : v1 kind : ConfigMap metadata : name : config-gc namespace : knative-serving data : min-non-active-revisions : \"0\" max-non-active-revisions : \"0\" retain-since-create-time : \"disabled\" retain-since-last-active-time : \"disabled\" Keep around the last ten non-active revisions \u00b6 apiVersion : v1 kind : ConfigMap metadata : name : config-gc namespace : knative-serving data : retain-since-create-time : \"disabled\" retain-since-last-active-time : \"disabled\" max-non-active-revisions : \"10\" Disable garbage collection \u00b6 apiVersion : v1 kind : ConfigMap metadata : name : config-gc namespace : knative-serving data : retain-since-create-time : \"disabled\" retain-since-last-active-time : \"disabled\" max-non-active-revisions : \"disabled\" Complex example \u00b6 The following example configuration keeps recently deployed or active revisions, always maintains the last two revisions in case of rollback, and prevents burst activity from exploding the count of old revisions: apiVersion : v1 kind : ConfigMap metadata : name : config-gc namespace : knative-serving data : retain-since-create-time : \"48h\" retain-since-last-active-time : \"15h\" min-non-active-revisions : \"2\" max-non-active-revisions : \"1000\" Per-revision options \u00b6 You can configure a revision so that it is never garbage collected by adding the serving.knative.dev/no-gc: \"true\" annotation: apiVersion : serving.knative.dev/v1 kind : Revision metadata : annotations : serving.knative.dev/no-gc : \"true\" spec : ...","title":"Revision garbage collection"},{"location":"serving/configuration/revision-gc/#revision-garbage-collection","text":"The config-gc ConfigMap contains settings that determine in-active revisions are cleaned up. This ConfigMap is located in the knative-serving namespace.","title":"Revision garbage collection"},{"location":"serving/configuration/revision-gc/#cluster-wide-configuration","text":"The following properties allow you to configure revision garbage collection: Name Description retain-since-create-time Duration since creation before considering a revision for GC or \"disabled\" retain-since-last-active-time Duration since active before considering a revision for GC or \"disabled\" min-non-active-revisions Minimum number of non-active revisions to retain. max-non-active-revisions Maximum number of non-active revisions to retain or \"disabled\" to disable any maximum limit. Revisions are retained if they belong to any one of the following categories: Is active and is being reference by a route Created within retain-since-create-time Last referenced by a route within retain-since-last-active-time There are fewer than min-non-active-revisions","title":"Cluster-wide configuration"},{"location":"serving/configuration/revision-gc/#examples","text":"","title":"Examples"},{"location":"serving/configuration/revision-gc/#immediately-collect-any-inactive-revision","text":"apiVersion : v1 kind : ConfigMap metadata : name : config-gc namespace : knative-serving data : min-non-active-revisions : \"0\" max-non-active-revisions : \"0\" retain-since-create-time : \"disabled\" retain-since-last-active-time : \"disabled\"","title":"Immediately collect any inactive revision"},{"location":"serving/configuration/revision-gc/#keep-around-the-last-ten-non-active-revisions","text":"apiVersion : v1 kind : ConfigMap metadata : name : config-gc namespace : knative-serving data : retain-since-create-time : \"disabled\" retain-since-last-active-time : \"disabled\" max-non-active-revisions : \"10\"","title":"Keep around the last ten non-active revisions"},{"location":"serving/configuration/revision-gc/#disable-garbage-collection","text":"apiVersion : v1 kind : ConfigMap metadata : name : config-gc namespace : knative-serving data : retain-since-create-time : \"disabled\" retain-since-last-active-time : \"disabled\" max-non-active-revisions : \"disabled\"","title":"Disable garbage collection"},{"location":"serving/configuration/revision-gc/#complex-example","text":"The following example configuration keeps recently deployed or active revisions, always maintains the last two revisions in case of rollback, and prevents burst activity from exploding the count of old revisions: apiVersion : v1 kind : ConfigMap metadata : name : config-gc namespace : knative-serving data : retain-since-create-time : \"48h\" retain-since-last-active-time : \"15h\" min-non-active-revisions : \"2\" max-non-active-revisions : \"1000\"","title":"Complex example"},{"location":"serving/configuration/revision-gc/#per-revision-options","text":"You can configure a revision so that it is never garbage collected by adding the serving.knative.dev/no-gc: \"true\" annotation: apiVersion : serving.knative.dev/v1 kind : Revision metadata : annotations : serving.knative.dev/no-gc : \"true\" spec : ...","title":"Per-revision options"},{"location":"serving/configuration/rolling-out-latest-revision-configmap/","text":"Configuring gradual rollout of traffic to Revisions \u00b6 If your traffic configuration points to a Configuration target instead of a Revision target, when a new Revision is created and ready, 100% of the traffic from the target is immediately shifted to the new Revision. This might make the request queue too long, either at the QP or Activator, and cause the requests to expire or be rejected by the QP. Knative provides a rollout-duration parameter, which can be used to gradually shift traffic to the latest Revision, preventing requests from being queued or rejected. Affected Configuration targets are rolled out to 1% of traffic first, and then in equal incremental steps for the rest of the assigned traffic. Note rollout-duration is time-based, and does not interact with the autoscaling subsystem. This feature is available for tagged and untagged traffic targets, configured for either Knative Services or Routes without a service. Procedure \u00b6 You can configure the rollout-duration parameter by modifying the config-network ConfigMap, or by using the Operator. ConfigMap configuration Operator configuration apiVersion : v1 kind : ConfigMap metadata : name : config-network namespace : knative-serving data : rollout-duration : \"380s\" # Value in seconds. apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : network : rollout-duration : \"380s\" Route status updates \u00b6 During a rollout, the system updates the Route and Knative Service status conditions. Both the traffic and conditions status parameters are affected. For example, for the following traffic configuration: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 55 configurationName : config # Pinned to latest ready Revision - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. Initially 1% of the traffic is rolled out to the Revisions: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 54 revisionName : config-00008 - percent : 1 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. Then the rest of the traffic is rolled out in increments of 18%: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 36 revisionName : config-00008 - percent : 19 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. The rollout continues until the target traffic configuration is reached: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 55 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. During the rollout, the Route and Knative Service status conditions are as follows: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... status : conditions : ... - lastTransitionTime : \"...\" message : A gradual rollout of the latest revision(s) is in progress. reason : RolloutInProgress status : Unknown type : Ready Multiple rollouts \u00b6 If a new revision is created while a rollout is in progress, the system begins to shift traffic immediately to the newest Revision, and drains the incomplete rollouts from newest to oldest.","title":"Configuring gradual rollout of traffic to Revisions"},{"location":"serving/configuration/rolling-out-latest-revision-configmap/#configuring-gradual-rollout-of-traffic-to-revisions","text":"If your traffic configuration points to a Configuration target instead of a Revision target, when a new Revision is created and ready, 100% of the traffic from the target is immediately shifted to the new Revision. This might make the request queue too long, either at the QP or Activator, and cause the requests to expire or be rejected by the QP. Knative provides a rollout-duration parameter, which can be used to gradually shift traffic to the latest Revision, preventing requests from being queued or rejected. Affected Configuration targets are rolled out to 1% of traffic first, and then in equal incremental steps for the rest of the assigned traffic. Note rollout-duration is time-based, and does not interact with the autoscaling subsystem. This feature is available for tagged and untagged traffic targets, configured for either Knative Services or Routes without a service.","title":"Configuring gradual rollout of traffic to Revisions"},{"location":"serving/configuration/rolling-out-latest-revision-configmap/#procedure","text":"You can configure the rollout-duration parameter by modifying the config-network ConfigMap, or by using the Operator. ConfigMap configuration Operator configuration apiVersion : v1 kind : ConfigMap metadata : name : config-network namespace : knative-serving data : rollout-duration : \"380s\" # Value in seconds. apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : network : rollout-duration : \"380s\"","title":"Procedure"},{"location":"serving/configuration/rolling-out-latest-revision-configmap/#route-status-updates","text":"During a rollout, the system updates the Route and Knative Service status conditions. Both the traffic and conditions status parameters are affected. For example, for the following traffic configuration: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 55 configurationName : config # Pinned to latest ready Revision - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. Initially 1% of the traffic is rolled out to the Revisions: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 54 revisionName : config-00008 - percent : 1 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. Then the rest of the traffic is rolled out in increments of 18%: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 36 revisionName : config-00008 - percent : 19 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. The rollout continues until the target traffic configuration is reached: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... traffic : - percent : 55 revisionName : config-00009 - percent : 45 revisionName : config-00005 # Pinned to a specific Revision. During the rollout, the Route and Knative Service status conditions are as follows: apiVersion : serving.knative.dev/v1 kind : Service metadata : ... spec : ... status : conditions : ... - lastTransitionTime : \"...\" message : A gradual rollout of the latest revision(s) is in progress. reason : RolloutInProgress status : Unknown type : Ready","title":"Route status updates"},{"location":"serving/configuration/rolling-out-latest-revision-configmap/#multiple-rollouts","text":"If a new revision is created while a rollout is in progress, the system begins to shift traffic immediately to the newest Revision, and drains the incomplete rollouts from newest to oldest.","title":"Multiple rollouts"},{"location":"serving/load-balancing/","text":"Load balancing \u00b6 You can turn on Knative load balancing, by placing the Activator service in the request path to act as a load balancer. To do this, you must first ensure that individual pod addressability is enabled. Activator pod selection \u00b6 Activator pods are scaled horizontally, so there may be multiple Activators in a deployment. In general, the system will perform best if the number of revision pods is larger than the number of Activator pods, and those numbers divide equally. Knative assigns a subset of Activators for each revision, depending on the revision size. More revision pods will mean a greater number of Activators for that revision. The Activator load balancing algorithm works as follows: If concurrency is unlimited, the request is sent to the better of two random choices. If concurrency is set to a value less or equal than 3, the Activator will send the request to the first pod that has capacity. Otherwise, requests will be balanced in a round robin fashion, with respect to container concurrency. For more information, see the documentation on concurrency . Configuring target burst capacity \u00b6 Target burst capacity is mainly responsible for determining whether the Activator is in the request path outside of scale-from-zero scenarios. Target burst capacity can be configured using a combination of the following parameters: Setting the targeted concurrency limits for the revision. See concurrency . Setting the target utilization parameters. See target utilization . Setting the target burst capacity. You can configure target burst capacity using the target-burst-capacity key in the config-autoscaler ConfigMap. See Setting the target burst capacity . Setting the Activator capacity by using the config-autoscaler ConfigMap. See Setting the Activator capacity .","title":"About load balancing"},{"location":"serving/load-balancing/#load-balancing","text":"You can turn on Knative load balancing, by placing the Activator service in the request path to act as a load balancer. To do this, you must first ensure that individual pod addressability is enabled.","title":"Load balancing"},{"location":"serving/load-balancing/#activator-pod-selection","text":"Activator pods are scaled horizontally, so there may be multiple Activators in a deployment. In general, the system will perform best if the number of revision pods is larger than the number of Activator pods, and those numbers divide equally. Knative assigns a subset of Activators for each revision, depending on the revision size. More revision pods will mean a greater number of Activators for that revision. The Activator load balancing algorithm works as follows: If concurrency is unlimited, the request is sent to the better of two random choices. If concurrency is set to a value less or equal than 3, the Activator will send the request to the first pod that has capacity. Otherwise, requests will be balanced in a round robin fashion, with respect to container concurrency. For more information, see the documentation on concurrency .","title":"Activator pod selection"},{"location":"serving/load-balancing/#configuring-target-burst-capacity","text":"Target burst capacity is mainly responsible for determining whether the Activator is in the request path outside of scale-from-zero scenarios. Target burst capacity can be configured using a combination of the following parameters: Setting the targeted concurrency limits for the revision. See concurrency . Setting the target utilization parameters. See target utilization . Setting the target burst capacity. You can configure target burst capacity using the target-burst-capacity key in the config-autoscaler ConfigMap. See Setting the target burst capacity . Setting the Activator capacity by using the config-autoscaler ConfigMap. See Setting the Activator capacity .","title":"Configuring target burst capacity"},{"location":"serving/load-balancing/activator-capacity/","text":"Configuring Activator capacity \u00b6 If there is more than one Activator in the system, Knative puts as many Activators on the request path as required to handle the current request load plus the target burst capacity. If the target burst capacity is 0, Knative only puts the Activator into the request path if the Revision is scaled to zero. Knative uses at least two Activators to enable high availability if possible. The actual number of Activators is calculated taking the Activator capacity into account, by using the formula (replicas * target + target-burst-capacity)/activator-capacity . This means that there are enough Activators in the routing path to handle the theoretical capacity of the existing application, including any additional target burst capacity. Setting the Activator capacity \u00b6 Global key: activator-capacity Possible values: int (at least 1) Default: 100 Example: Global (ConfigMap) Global (Operator) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : activator-capacity : \"200\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : activator-capacity : \"200\"","title":"Configuring Activator capacity"},{"location":"serving/load-balancing/activator-capacity/#configuring-activator-capacity","text":"If there is more than one Activator in the system, Knative puts as many Activators on the request path as required to handle the current request load plus the target burst capacity. If the target burst capacity is 0, Knative only puts the Activator into the request path if the Revision is scaled to zero. Knative uses at least two Activators to enable high availability if possible. The actual number of Activators is calculated taking the Activator capacity into account, by using the formula (replicas * target + target-burst-capacity)/activator-capacity . This means that there are enough Activators in the routing path to handle the theoretical capacity of the existing application, including any additional target burst capacity.","title":"Configuring Activator capacity"},{"location":"serving/load-balancing/activator-capacity/#setting-the-activator-capacity","text":"Global key: activator-capacity Possible values: int (at least 1) Default: 100 Example: Global (ConfigMap) Global (Operator) apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : activator-capacity : \"200\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : activator-capacity : \"200\"","title":"Setting the Activator capacity"},{"location":"serving/load-balancing/target-burst-capacity/","text":"Configuring target burst capacity \u00b6 Target burst capacity is a global and per-revision integer setting that determines the size of traffic burst a Knative application can handle without buffering. If a traffic burst is too large for the application to handle, the Activator service will be placed in the request path to protect the revision and optimize request load balancing. The Activator service is responsible for receiving and buffering requests for inactive revisions, or for revisions where a traffic burst is larger than the limits of what can be handled without buffering for that revision. It can also quickly spin up additional pods for capacity, and throttle how quickly requests are sent to pods. Target burst capacity can be configured using a combination of the following parameters: Setting the targeted concurrency limits for the revision. See concurrency . Setting the target utilization parameters. See target utilization . Setting the target burst capacity. You can configure target burst capacity using the target-burst-capacity annotation key in the config-autoscaler ConfigMap. See Setting the target burst capacity . Setting the target burst capacity \u00b6 Global key: target-burst-capacity Per-revision annotation key: autoscaling.knative.dev/target-burst-capacity Possible values: float ( 0 means the Activator is only in path when scaled to 0, -1 means the Activator is always in path) Default: 200 Example: Per Revision Global (ConfigMap) Global (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : annotations : name : <service_name> namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target-burst-capacity : \"200\" apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : target-burst-capacity : \"200\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : target-burst-capacity : \"200\" Note Ingress gateway load balancing requires additional configuration. For more information about load balancing using an ingress gateway, see the Serving API documentation. If autoscaling.knative.dev/target-burst-capacity is set to 0 , the Activator is only added to the request path during scale from zero scenarios, and ingress load balancing will be applied. If autoscaling.knative.dev/target-burst-capacity is set to -1 , the Activator is always in the request path, regardless of the revision size. If autoscaling.knative.dev/target-burst-capacity is set to another integer, the Activator may be in the path, depending on the revision scale and load.","title":"Configuring target burst capacity"},{"location":"serving/load-balancing/target-burst-capacity/#configuring-target-burst-capacity","text":"Target burst capacity is a global and per-revision integer setting that determines the size of traffic burst a Knative application can handle without buffering. If a traffic burst is too large for the application to handle, the Activator service will be placed in the request path to protect the revision and optimize request load balancing. The Activator service is responsible for receiving and buffering requests for inactive revisions, or for revisions where a traffic burst is larger than the limits of what can be handled without buffering for that revision. It can also quickly spin up additional pods for capacity, and throttle how quickly requests are sent to pods. Target burst capacity can be configured using a combination of the following parameters: Setting the targeted concurrency limits for the revision. See concurrency . Setting the target utilization parameters. See target utilization . Setting the target burst capacity. You can configure target burst capacity using the target-burst-capacity annotation key in the config-autoscaler ConfigMap. See Setting the target burst capacity .","title":"Configuring target burst capacity"},{"location":"serving/load-balancing/target-burst-capacity/#setting-the-target-burst-capacity","text":"Global key: target-burst-capacity Per-revision annotation key: autoscaling.knative.dev/target-burst-capacity Possible values: float ( 0 means the Activator is only in path when scaled to 0, -1 means the Activator is always in path) Default: 200 Example: Per Revision Global (ConfigMap) Global (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : annotations : name : <service_name> namespace : default spec : template : metadata : annotations : autoscaling.knative.dev/target-burst-capacity : \"200\" apiVersion : v1 kind : ConfigMap metadata : name : config-autoscaler namespace : knative-serving data : target-burst-capacity : \"200\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : autoscaler : target-burst-capacity : \"200\" Note Ingress gateway load balancing requires additional configuration. For more information about load balancing using an ingress gateway, see the Serving API documentation. If autoscaling.knative.dev/target-burst-capacity is set to 0 , the Activator is only added to the request path during scale from zero scenarios, and ingress load balancing will be applied. If autoscaling.knative.dev/target-burst-capacity is set to -1 , the Activator is always in the request path, regardless of the revision size. If autoscaling.knative.dev/target-burst-capacity is set to another integer, the Activator may be in the path, depending on the revision scale and load.","title":"Setting the target burst capacity"},{"location":"serving/observability/logging/collecting-logs/","text":"Logging \u00b6 You can use Fluent Bit , a log processor and forwarder, to collect Kubernetes logs in a central directory. This is not required to run Knative, but can be helpful with Knative Serving , which automatically deletes pods and associated logs when they are no longer needed. Fluent Bit supports exporting to a number of other log providers. If you already have an existing log provider, for example, Splunk, Datadog, ElasticSearch, or Stackdriver, you can follow the FluentBit documentation to configure log forwarders. Setting up logging components \u00b6 Setting up log collection requires two steps: Running a log forwarding DaemonSet on each node. Running a collector somewhere in the cluster. Tip In the following example, a StatefulSet is used, which stores logs on a Kubernetes PersistentVolumeClaim, but you can also use a HostPath. Setting up the collector \u00b6 The fluent-bit-collector.yaml file defines a StatefulSet, as well as a Kubernetes Service which allows accessing and reading the logs from within the cluster. The supplied configuration will create the monitoring configuration in a namespace called logging . Important Set up the collector before the forwarders. You will need the address of the collector when configuring the forwarders, and the forwarders may queue logs until the collector is ready. Procedure \u00b6 Apply the configuration by entering the command: kubectl apply -f https://github.com/knative/docs/raw/main/docs/serving/observability/logging/fluent-bit-collector.yaml The default configuration will classify logs into: Knative services, or pods with an app=Knative label. Non-Knative apps. Note Logs default to logging with the pod name; this can be changed by updating the log-collector-config ConfigMap before or after installation. Warning After the ConfigMap is updated, you must restart Fluent Bit. You can do this by deleting the pod and letting the StatefulSet recreate it. To access the logs through your web browser, enter the command: kubectl port-forward --namespace logging service/log-collector 8080 :80 Navigate to http://localhost:8080/ . Optional: You can open a shell in the nginx pod and search the logs using Unix tools, by entering the command: kubectl exec --namespace logging --stdin --tty --container nginx log-collector-0 Setting up the forwarders \u00b6 See the Fluent Bit documentation to set up a Fluent Bit DaemonSet that forwards logs to ElasticSearch by default. When you create a ConfigMap during the installation steps, you must: Replace the ElasticSearch configuration with the fluent-bit-configmap.yaml , or Add the following block to the ConfigMap, and update the @INCLUDE output-elasticsearch.conf to be @INCLUDE output-forward.conf : output-forward.conf : | [OUTPUT] Name forward Host log-collector.logging Port 24224 Require_ack_response True Setting up a local collector \u00b6 Warning This procedure describes a development environment setup and is not suitable for production use. If you are using a local Kubernetes cluster for development, you can create a hostPath PersistentVolume to store the logs on your desktop operating system. This allows you to use your usual desktop tools on the files without needing Kubernetes-specific tools. The PersistentVolumeClaim will look similar to the following: apiVersion : v1 kind : PersistentVolume metadata : name : shared-logs labels : app : logs-collector spec : accessModes : - \"ReadWriteOnce\" storageClassName : manual claimRef : apiVersion : v1 kind : PersistentVolumeClaim name : logs-log-collector-0 namespace : logging capacity : storage : 5Gi hostPath : path : <see below> Note The hostPath will vary based on your Kubernetes software and host operating system. You must update the StatefulSet volumeClaimTemplates to reference the shared-logs volume, as shown in the following example: volumeClaimTemplates : metadata : name : logs spec : accessModes : [ \"ReadWriteOnce\" ] volumeName : shared-logs Kind \u00b6 When creating your cluster, you must use a kind-config.yaml and specify extraMounts for each node, as shown in the following example: apiversion : kind.x-k8s.io/v1alpha4 kind : Cluster nodes : - role : control-plane extraMounts : - hostPath : ./logs containerPath : /shared/logs - role : worker extraMounts : - hostPath : ./logs containerPath : /shared/logs You can then use /shared/logs as the spec.hostPath.path in your PersistentVolume. Note that the directory path ./logs is relative to the directory that the Kind cluster was created in. Docker Desktop \u00b6 Docker desktop automatically creates some shared mounts between the host and the guest operating systems, so you only need to know the path to your home directory. The following are some examples for different operating systems: Host OS hostPath Mac OS /Users/${USER} Windows /run/desktop/mnt/host/c/Users/${USER}/ Linux /home/${USER} Minikube \u00b6 Minikube requires an explicit command to mount a directory into the virtual machine (VM) running Kubernetes. The following command mounts the logs directory inside the current directory onto /mnt/logs in the VM: minikube mount ./logs:/mnt/logs You must also reference /mnt/logs as the hostPath.path in the PersistentVolume.","title":"Collecting logs"},{"location":"serving/observability/logging/collecting-logs/#logging","text":"You can use Fluent Bit , a log processor and forwarder, to collect Kubernetes logs in a central directory. This is not required to run Knative, but can be helpful with Knative Serving , which automatically deletes pods and associated logs when they are no longer needed. Fluent Bit supports exporting to a number of other log providers. If you already have an existing log provider, for example, Splunk, Datadog, ElasticSearch, or Stackdriver, you can follow the FluentBit documentation to configure log forwarders.","title":"Logging"},{"location":"serving/observability/logging/collecting-logs/#setting-up-logging-components","text":"Setting up log collection requires two steps: Running a log forwarding DaemonSet on each node. Running a collector somewhere in the cluster. Tip In the following example, a StatefulSet is used, which stores logs on a Kubernetes PersistentVolumeClaim, but you can also use a HostPath.","title":"Setting up logging components"},{"location":"serving/observability/logging/collecting-logs/#setting-up-the-collector","text":"The fluent-bit-collector.yaml file defines a StatefulSet, as well as a Kubernetes Service which allows accessing and reading the logs from within the cluster. The supplied configuration will create the monitoring configuration in a namespace called logging . Important Set up the collector before the forwarders. You will need the address of the collector when configuring the forwarders, and the forwarders may queue logs until the collector is ready.","title":"Setting up the collector"},{"location":"serving/observability/logging/collecting-logs/#procedure","text":"Apply the configuration by entering the command: kubectl apply -f https://github.com/knative/docs/raw/main/docs/serving/observability/logging/fluent-bit-collector.yaml The default configuration will classify logs into: Knative services, or pods with an app=Knative label. Non-Knative apps. Note Logs default to logging with the pod name; this can be changed by updating the log-collector-config ConfigMap before or after installation. Warning After the ConfigMap is updated, you must restart Fluent Bit. You can do this by deleting the pod and letting the StatefulSet recreate it. To access the logs through your web browser, enter the command: kubectl port-forward --namespace logging service/log-collector 8080 :80 Navigate to http://localhost:8080/ . Optional: You can open a shell in the nginx pod and search the logs using Unix tools, by entering the command: kubectl exec --namespace logging --stdin --tty --container nginx log-collector-0","title":"Procedure"},{"location":"serving/observability/logging/collecting-logs/#setting-up-the-forwarders","text":"See the Fluent Bit documentation to set up a Fluent Bit DaemonSet that forwards logs to ElasticSearch by default. When you create a ConfigMap during the installation steps, you must: Replace the ElasticSearch configuration with the fluent-bit-configmap.yaml , or Add the following block to the ConfigMap, and update the @INCLUDE output-elasticsearch.conf to be @INCLUDE output-forward.conf : output-forward.conf : | [OUTPUT] Name forward Host log-collector.logging Port 24224 Require_ack_response True","title":"Setting up the forwarders"},{"location":"serving/observability/logging/collecting-logs/#setting-up-a-local-collector","text":"Warning This procedure describes a development environment setup and is not suitable for production use. If you are using a local Kubernetes cluster for development, you can create a hostPath PersistentVolume to store the logs on your desktop operating system. This allows you to use your usual desktop tools on the files without needing Kubernetes-specific tools. The PersistentVolumeClaim will look similar to the following: apiVersion : v1 kind : PersistentVolume metadata : name : shared-logs labels : app : logs-collector spec : accessModes : - \"ReadWriteOnce\" storageClassName : manual claimRef : apiVersion : v1 kind : PersistentVolumeClaim name : logs-log-collector-0 namespace : logging capacity : storage : 5Gi hostPath : path : <see below> Note The hostPath will vary based on your Kubernetes software and host operating system. You must update the StatefulSet volumeClaimTemplates to reference the shared-logs volume, as shown in the following example: volumeClaimTemplates : metadata : name : logs spec : accessModes : [ \"ReadWriteOnce\" ] volumeName : shared-logs","title":"Setting up a local collector"},{"location":"serving/observability/logging/collecting-logs/#kind","text":"When creating your cluster, you must use a kind-config.yaml and specify extraMounts for each node, as shown in the following example: apiversion : kind.x-k8s.io/v1alpha4 kind : Cluster nodes : - role : control-plane extraMounts : - hostPath : ./logs containerPath : /shared/logs - role : worker extraMounts : - hostPath : ./logs containerPath : /shared/logs You can then use /shared/logs as the spec.hostPath.path in your PersistentVolume. Note that the directory path ./logs is relative to the directory that the Kind cluster was created in.","title":"Kind"},{"location":"serving/observability/logging/collecting-logs/#docker-desktop","text":"Docker desktop automatically creates some shared mounts between the host and the guest operating systems, so you only need to know the path to your home directory. The following are some examples for different operating systems: Host OS hostPath Mac OS /Users/${USER} Windows /run/desktop/mnt/host/c/Users/${USER}/ Linux /home/${USER}","title":"Docker Desktop"},{"location":"serving/observability/logging/collecting-logs/#minikube","text":"Minikube requires an explicit command to mount a directory into the virtual machine (VM) running Kubernetes. The following command mounts the logs directory inside the current directory onto /mnt/logs in the VM: minikube mount ./logs:/mnt/logs You must also reference /mnt/logs as the hostPath.path in the PersistentVolume.","title":"Minikube"},{"location":"serving/observability/logging/config-logging/","text":"Configuring Log Settings \u00b6 Log configuration for all Knative components is managed through the config-logging ConfigMap in the corresponding namespace. For example, Serving components are configured through config-logging in the knative-serving namespace and Eventing components are configured through config-logging in the knative-eventing namespace, etc. Knative components use the zap logging library; options are documented in more detail in that project . In addition to zap-logger-config , which is a general key that applies to all components in that namespace, the config-logging ConfigMap supports overriding the log level for individual components. ConfigMap key Description zap-logger-config A JSON object container for a zap logger configuration. Key fields are highlighted below. zap-logger-config.level The default logging level for components. Messages at or above this severity level will be logged. zap-logger-config.encoding The log encoding format for component logs (defaults to JSON). zap-logger-config.encoderConfig A zap EncoderConfig used to customize record contents. loglevel.<component> Overrides logging level for the given component only. Messages at or above this severity level will be logged. Log levels supported by Zap are: debug - fine-grained debugging info - normal logging warn - unexpected but non-critical errors error - critical errors; unexpected during normal operation dpanic - in debug mode, trigger a panic (crash) panic - trigger a panic (crash) fatal - immediately exit with exit status 1 (failure)","title":"Configuring logging"},{"location":"serving/observability/logging/config-logging/#configuring-log-settings","text":"Log configuration for all Knative components is managed through the config-logging ConfigMap in the corresponding namespace. For example, Serving components are configured through config-logging in the knative-serving namespace and Eventing components are configured through config-logging in the knative-eventing namespace, etc. Knative components use the zap logging library; options are documented in more detail in that project . In addition to zap-logger-config , which is a general key that applies to all components in that namespace, the config-logging ConfigMap supports overriding the log level for individual components. ConfigMap key Description zap-logger-config A JSON object container for a zap logger configuration. Key fields are highlighted below. zap-logger-config.level The default logging level for components. Messages at or above this severity level will be logged. zap-logger-config.encoding The log encoding format for component logs (defaults to JSON). zap-logger-config.encoderConfig A zap EncoderConfig used to customize record contents. loglevel.<component> Overrides logging level for the given component only. Messages at or above this severity level will be logged. Log levels supported by Zap are: debug - fine-grained debugging info - normal logging warn - unexpected but non-critical errors error - critical errors; unexpected during normal operation dpanic - in debug mode, trigger a panic (crash) panic - trigger a panic (crash) fatal - immediately exit with exit status 1 (failure)","title":"Configuring Log Settings"},{"location":"serving/observability/metrics/collecting-metrics/","text":"Collecting Metrics in Knative \u00b6 Knative supports different popular tools for collecting metrics: Prometheus OpenTelemetry Collector Grafana dashboards are available for metrics collected directly with Prometheus. You can also set up the OpenTelemetry Collector to receive metrics from Knative components and distribute them to other metrics providers that support OpenTelemetry. Warning You can't use OpenTelemetry Collector and Prometheus at the same time. The default metrics backend is Prometheus. You will need to remove metrics.backend-destination and metrics.request-metrics-backend-destination keys from the config-observability Configmap to enable Prometheus metrics. About Prometheus \u00b6 Prometheus is an open-source tool for collecting, aggregating timeseries metrics and alerting. It can also be used to scrape the OpenTelemetry Collector that is demonstrated below when Prometheus is used. Setting up Prometheus \u00b6 Install the Prometheus Operator by using Helm : helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm install prometheus prometheus-community/kube-prometheus-stack -n default -f values.yaml # values.yaml contains at minimum the configuration below Caution You will need to ensure that the helm chart has following values configured, otherwise the ServiceMonitors/Podmonitors will not work. kube-state-metrics : metricLabelsAllowlist : - pods=[*] - deployments=[app.kubernetes.io/name,app.kubernetes.io/component,app.kubernetes.io/instance] prometheus : prometheusSpec : serviceMonitorSelectorNilUsesHelmValues : false podMonitorSelectorNilUsesHelmValues : false Apply the ServiceMonitors/PodMonitors to collect metrics from Knative. kubectl apply -f https://raw.githubusercontent.com/knative-sandbox/monitoring/main/servicemonitor.yaml Grafana dashboards can be imported from the knative-sandbox repository . If you are using the Grafana Helm Chart with the Dashboard Sidecar enabled, you can load the dashboards by applying the following configmaps. kubectl apply -f https://raw.githubusercontent.com/knative-sandbox/monitoring/main/grafana/dashboards.yaml Access the Prometheus instance locally \u00b6 By default, the Prometheus instance is only exposed on a private service named prometheus-operated . To access the console in your web browser: Enter the command: kubectl port-forward -n default svc/prometheus-operated 9090 Access the console in your browser via http://localhost:9090 . About OpenTelemetry \u00b6 OpenTelemetry is a CNCF observability framework for cloud-native software, which provides a collection of tools, APIs, and SDKs. You can use OpenTelemetry to instrument, generate, collect, and export telemetry data. This data includes metrics, logs, and traces, that you can analyze to understand the performance and behavior of Knative components. OpenTelemetry allows you to easily export metrics to multiple monitoring services without needing to rebuild or reconfigure the Knative binaries. Understanding the collector \u00b6 The collector provides a location where various Knative components can push metrics to be retained and collected by a monitoring service. In the following example, you can configure a single collector instance using a ConfigMap and a Deployment. Tip For more complex deployments, you can automate some of these steps by using the OpenTelemetry Operator . Caution The Grafana dashboards at https://github.com/knative-sandbox/monitoring/tree/main/grafana don't work with metrics scraped from OpenTelemetry Collector. Set up the collector \u00b6 Create a namespace for the collector to run in, by entering the following command: kubectl create namespace metrics The next step uses the metrics namespace for creating the collector. Create a Deployment, Service, and ConfigMap for the collector by entering the following command: kubectl apply -f https://raw.githubusercontent.com/knative/docs/main/docs/serving/observability/metrics/collector.yaml Update the config-observability ConfigMaps in the Knative Serving and Eventing namespaces, by entering the follow command: kubectl patch --namespace knative-serving configmap/config-observability \\ --type merge \\ --patch '{\"data\":{\"metrics.backend-destination\":\"opencensus\",\"metrics.request-metrics-backend-destination\":\"opencensus\",\"metrics.opencensus-address\":\"otel-collector.metrics:55678\"}}' kubectl patch --namespace knative-eventing configmap/config-observability \\ --type merge \\ --patch '{\"data\":{\"metrics.backend-destination\":\"opencensus\",\"metrics.opencensus-address\":\"otel-collector.metrics:55678\"}}' Verify the collector setup \u00b6 You can check that metrics are being forwarded by loading the Prometheus export port on the collector, by entering the following command: kubectl port-forward --namespace metrics deployment/otel-collector 8889 Fetch http://localhost:8889/metrics to see the exported metrics.","title":"Collecting metrics"},{"location":"serving/observability/metrics/collecting-metrics/#collecting-metrics-in-knative","text":"Knative supports different popular tools for collecting metrics: Prometheus OpenTelemetry Collector Grafana dashboards are available for metrics collected directly with Prometheus. You can also set up the OpenTelemetry Collector to receive metrics from Knative components and distribute them to other metrics providers that support OpenTelemetry. Warning You can't use OpenTelemetry Collector and Prometheus at the same time. The default metrics backend is Prometheus. You will need to remove metrics.backend-destination and metrics.request-metrics-backend-destination keys from the config-observability Configmap to enable Prometheus metrics.","title":"Collecting Metrics in Knative"},{"location":"serving/observability/metrics/collecting-metrics/#about-prometheus","text":"Prometheus is an open-source tool for collecting, aggregating timeseries metrics and alerting. It can also be used to scrape the OpenTelemetry Collector that is demonstrated below when Prometheus is used.","title":"About Prometheus"},{"location":"serving/observability/metrics/collecting-metrics/#setting-up-prometheus","text":"Install the Prometheus Operator by using Helm : helm repo add prometheus-community https://prometheus-community.github.io/helm-charts helm repo update helm install prometheus prometheus-community/kube-prometheus-stack -n default -f values.yaml # values.yaml contains at minimum the configuration below Caution You will need to ensure that the helm chart has following values configured, otherwise the ServiceMonitors/Podmonitors will not work. kube-state-metrics : metricLabelsAllowlist : - pods=[*] - deployments=[app.kubernetes.io/name,app.kubernetes.io/component,app.kubernetes.io/instance] prometheus : prometheusSpec : serviceMonitorSelectorNilUsesHelmValues : false podMonitorSelectorNilUsesHelmValues : false Apply the ServiceMonitors/PodMonitors to collect metrics from Knative. kubectl apply -f https://raw.githubusercontent.com/knative-sandbox/monitoring/main/servicemonitor.yaml Grafana dashboards can be imported from the knative-sandbox repository . If you are using the Grafana Helm Chart with the Dashboard Sidecar enabled, you can load the dashboards by applying the following configmaps. kubectl apply -f https://raw.githubusercontent.com/knative-sandbox/monitoring/main/grafana/dashboards.yaml","title":"Setting up Prometheus"},{"location":"serving/observability/metrics/collecting-metrics/#access-the-prometheus-instance-locally","text":"By default, the Prometheus instance is only exposed on a private service named prometheus-operated . To access the console in your web browser: Enter the command: kubectl port-forward -n default svc/prometheus-operated 9090 Access the console in your browser via http://localhost:9090 .","title":"Access the Prometheus instance locally"},{"location":"serving/observability/metrics/collecting-metrics/#about-opentelemetry","text":"OpenTelemetry is a CNCF observability framework for cloud-native software, which provides a collection of tools, APIs, and SDKs. You can use OpenTelemetry to instrument, generate, collect, and export telemetry data. This data includes metrics, logs, and traces, that you can analyze to understand the performance and behavior of Knative components. OpenTelemetry allows you to easily export metrics to multiple monitoring services without needing to rebuild or reconfigure the Knative binaries.","title":"About OpenTelemetry"},{"location":"serving/observability/metrics/collecting-metrics/#understanding-the-collector","text":"The collector provides a location where various Knative components can push metrics to be retained and collected by a monitoring service. In the following example, you can configure a single collector instance using a ConfigMap and a Deployment. Tip For more complex deployments, you can automate some of these steps by using the OpenTelemetry Operator . Caution The Grafana dashboards at https://github.com/knative-sandbox/monitoring/tree/main/grafana don't work with metrics scraped from OpenTelemetry Collector.","title":"Understanding the collector"},{"location":"serving/observability/metrics/collecting-metrics/#set-up-the-collector","text":"Create a namespace for the collector to run in, by entering the following command: kubectl create namespace metrics The next step uses the metrics namespace for creating the collector. Create a Deployment, Service, and ConfigMap for the collector by entering the following command: kubectl apply -f https://raw.githubusercontent.com/knative/docs/main/docs/serving/observability/metrics/collector.yaml Update the config-observability ConfigMaps in the Knative Serving and Eventing namespaces, by entering the follow command: kubectl patch --namespace knative-serving configmap/config-observability \\ --type merge \\ --patch '{\"data\":{\"metrics.backend-destination\":\"opencensus\",\"metrics.request-metrics-backend-destination\":\"opencensus\",\"metrics.opencensus-address\":\"otel-collector.metrics:55678\"}}' kubectl patch --namespace knative-eventing configmap/config-observability \\ --type merge \\ --patch '{\"data\":{\"metrics.backend-destination\":\"opencensus\",\"metrics.opencensus-address\":\"otel-collector.metrics:55678\"}}'","title":"Set up the collector"},{"location":"serving/observability/metrics/collecting-metrics/#verify-the-collector-setup","text":"You can check that metrics are being forwarded by loading the Prometheus export port on the collector, by entering the following command: kubectl port-forward --namespace metrics deployment/otel-collector 8889 Fetch http://localhost:8889/metrics to see the exported metrics.","title":"Verify the collector setup"},{"location":"serving/observability/metrics/serving-metrics/","text":"Knative Serving metrics \u00b6 Administrators can monitor Serving control plane based on the metrics exposed by each Serving component. Metrics are listed next. Activator \u00b6 The following metrics can help you to understand how an application responds when traffic passes through the activator. For example, when scaling from zero, high request latency might mean that requests are taking too much time to be fulfilled. Metric Name Description Type Tags Unit Status request_concurrency Concurrent requests that are routed to Activator These are requests reported by the concurrency reporter which may not be done yet. This is the average concurrency over a reporting period Gauge configuration_name container_name namespace_name pod_name revision_name service_name Dimensionless Stable request_count The number of requests that are routed to Activator. These are requests that have been fulfilled from the activator handler. Counter configuration_name container_name namespace_name pod_name response_code response_code_class revision_name service_name Dimensionless Stable request_latencies The response time in millisecond for the fulfilled routed requests Histogram configuration_name container_name namespace_name pod_name response_code response_code_class revision_name service_name Milliseconds Stable Autoscaler \u00b6 Autoscaler component exposes a number of metrics related to its decisions per revision. For example, at any given time, you can monitor the desired pods the Autoscaler wants to allocate for a Service, the average number of requests per second during the stable window, or whether autoscaler is in panic mode (KPA). Metric Name Description Type Tags Unit Status desired_pods Number of pods autoscaler wants to allocate Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable excess_burst_capacity Excess burst capacity overserved over the stable window Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable stable_request_concurrency Average of requests count per observed pod over the stable window Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable panic_request_concurrency Average of requests count per observed pod over the panic window Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable target_concurrency_per_pod The desired number of concurrent requests for each pod Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable stable_requests_per_second Average requests-per-second per observed pod over the stable window Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable panic_requests_per_second Average requests-per-second per observed pod over the panic window Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable target_requests_per_second The desired requests-per-second for each pod Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable panic_mode 1 if autoscaler is in panic mode, 0 otherwise Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable requested_pods Number of pods autoscaler requested from Kubernetes Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable actual_pods Number of pods that are allocated currently in ready state Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable not_ready_pods Number of pods that are not ready currently Gauge configuration_name= namespace_name= revision_name service_name Dimensionless Stable pending_pods Number of pods that are pending currently Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable terminating_pods Number of pods that are terminating currently Gauge configuration_name namespace_name revision_name service_name<br> Dimensionless Stable scrape_time Time autoscaler takes to scrape metrics from the service pods in milliseconds Histogram configuration_name namespace_name revision_name service_name Milliseconds Stable Controller \u00b6 The following metrics are emitted by any component that implements a controller logic. The metrics show details about the reconciliation operations and the workqueue behavior on which reconciliation requests are enqueued. Metric Name Description Type Tags Unit Status work_queue_depth Depth of the work queue Gauge reconciler Dimensionless Stable reconcile_count Number of reconcile operations Counter reconciler success Dimensionless Stable reconcile_latency Latency of reconcile operations Histogram reconciler success Milliseconds Stable workqueue_adds_total Total number of adds handled by workqueue Counter name Dimensionless Stable workqueue_depth Current depth of workqueue Gauge reconciler Dimensionless Stable workqueue_queue_latency_seconds How long in seconds an item stays in workqueue before being requested Histogram name Seconds Stable workqueue_retries_total Total number of retries handled by workqueue Counter name Dimensionless Stable workqueue_work_duration_seconds How long in seconds processing an item from a workqueue takes. Histogram name Seconds Stable workqueue_unfinished_work_seconds How long in seconds the outstanding workqueue items have been in flight (total). Histogram name Seconds Stable workqueue_longest_running_processor_seconds How long in seconds the longest outstanding workqueue item has been in flight Histogram name Seconds Stable Webhook \u00b6 Webhook metrics report useful info about operations. For example, if a large number of operations fail, this could indicate an issue with a user-created resource. Metric Name Description Type Tags Unit Status request_count The number of requests that are routed to webhook Counter admission_allowed kind_group kind_kind kind_version request_operation resource_group resource_namespace resource_resource resource_version Dimensionless Stable request_latencies The response time in milliseconds Histogram admission_allowed kind_group kind_kind kind_version request_operation resource_group resource_namespace resource_resource resource_version Milliseconds Stable Go Runtime - memstats \u00b6 Each Knative Serving control plane process emits a number of Go runtime memory statistics (shown next). As a baseline for monitoring purproses, user could start with a subset of the metrics: current allocations (go_alloc), total allocations (go_total_alloc), system memory (go_sys), mallocs (go_mallocs), frees (go_frees) and garbage collection total pause time (total_gc_pause_ns), next gc target heap size (go_next_gc) and number of garbage collection cycles (num_gc). Metric Name Description Type Tags Unit Status go_alloc The number of bytes of allocated heap objects (same as heap_alloc) Gauge name Dimensionless Stable go_total_alloc The cumulative bytes allocated for heap objects Gauge name Dimensionless Stable go_sys The total bytes of memory obtained from the OS Gauge name Dimensionless Stable go_lookups The number of pointer lookups performed by the runtime Gauge name Dimensionless Stable go_mallocs The cumulative count of heap objects allocated Gauge name Dimensionless Stable go_frees The cumulative count of heap objects freed Gauge name Dimensionless Stable go_heap_alloc The number of bytes of allocated heap objects Gauge name Dimensionless Stable go_heap_sys The number of bytes of heap memory obtained from the OS Gauge name Dimensionless Stable go_heap_idle The number of bytes in idle (unused) spans Gauge name Dimensionless Stable go_heap_in_use The number of bytes in in-use spans Gauge name Dimensionless Stable go_heap_released The number of bytes of physical memory returned to the OS Gauge name Dimensionless Stable go_heap_objects The number of allocated heap objects Gauge name Dimensionless Stable go_stack_in_use The number of bytes in stack spans Gauge name Dimensionless Stable go_stack_sys The number of bytes of stack memory obtained from the OS Gauge name Dimensionless Stable go_mspan_in_use The number of bytes of allocated mspan structures Gauge name Dimensionless Stable go_mspan_sys The number of bytes of memory obtained from the OS for mspan structures Gauge name Dimensionless Stable go_mcache_in_use The number of bytes of allocated mcache structures Gauge name Dimensionless Stable go_mcache_sys The number of bytes of memory obtained from the OS for mcache structures Gauge name Dimensionless Stable go_bucket_hash_sys The number of bytes of memory in profiling bucket hash tables. Gauge name Dimensionless Stable go_gc_sys The number of bytes of memory in garbage collection metadata Gauge name Dimensionless Stable go_other_sys The number of bytes of memory in miscellaneous off-heap runtime allocations Gauge name Dimensionless Stable go_next_gc The target heap size of the next GC cycle Gauge name Dimensionless Stable go_last_gc The time the last garbage collection finished, as nanoseconds since 1970 (the UNIX epoch) Gauge name Nanoseconds Stable go_total_gc_pause_ns The cumulative nanoseconds in GC stop-the-world pauses since the program started Gauge name Nanoseconds Stable go_num_gc The number of completed GC cycles. Gauge name Dimensionless Stable go_num_forced_gc The number of GC cycles that were forced by the application calling the GC function. Gauge name Dimensionless Stable go_gc_cpu_fraction The fraction of this program's available CPU time used by the GC since the program started Gauge name Dimensionless Stable Note The name tag is empty.","title":"Knative Serving metrics"},{"location":"serving/observability/metrics/serving-metrics/#knative-serving-metrics","text":"Administrators can monitor Serving control plane based on the metrics exposed by each Serving component. Metrics are listed next.","title":"Knative Serving metrics"},{"location":"serving/observability/metrics/serving-metrics/#activator","text":"The following metrics can help you to understand how an application responds when traffic passes through the activator. For example, when scaling from zero, high request latency might mean that requests are taking too much time to be fulfilled. Metric Name Description Type Tags Unit Status request_concurrency Concurrent requests that are routed to Activator These are requests reported by the concurrency reporter which may not be done yet. This is the average concurrency over a reporting period Gauge configuration_name container_name namespace_name pod_name revision_name service_name Dimensionless Stable request_count The number of requests that are routed to Activator. These are requests that have been fulfilled from the activator handler. Counter configuration_name container_name namespace_name pod_name response_code response_code_class revision_name service_name Dimensionless Stable request_latencies The response time in millisecond for the fulfilled routed requests Histogram configuration_name container_name namespace_name pod_name response_code response_code_class revision_name service_name Milliseconds Stable","title":"Activator"},{"location":"serving/observability/metrics/serving-metrics/#autoscaler","text":"Autoscaler component exposes a number of metrics related to its decisions per revision. For example, at any given time, you can monitor the desired pods the Autoscaler wants to allocate for a Service, the average number of requests per second during the stable window, or whether autoscaler is in panic mode (KPA). Metric Name Description Type Tags Unit Status desired_pods Number of pods autoscaler wants to allocate Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable excess_burst_capacity Excess burst capacity overserved over the stable window Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable stable_request_concurrency Average of requests count per observed pod over the stable window Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable panic_request_concurrency Average of requests count per observed pod over the panic window Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable target_concurrency_per_pod The desired number of concurrent requests for each pod Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable stable_requests_per_second Average requests-per-second per observed pod over the stable window Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable panic_requests_per_second Average requests-per-second per observed pod over the panic window Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable target_requests_per_second The desired requests-per-second for each pod Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable panic_mode 1 if autoscaler is in panic mode, 0 otherwise Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable requested_pods Number of pods autoscaler requested from Kubernetes Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable actual_pods Number of pods that are allocated currently in ready state Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable not_ready_pods Number of pods that are not ready currently Gauge configuration_name= namespace_name= revision_name service_name Dimensionless Stable pending_pods Number of pods that are pending currently Gauge configuration_name namespace_name revision_name service_name Dimensionless Stable terminating_pods Number of pods that are terminating currently Gauge configuration_name namespace_name revision_name service_name<br> Dimensionless Stable scrape_time Time autoscaler takes to scrape metrics from the service pods in milliseconds Histogram configuration_name namespace_name revision_name service_name Milliseconds Stable","title":"Autoscaler"},{"location":"serving/observability/metrics/serving-metrics/#controller","text":"The following metrics are emitted by any component that implements a controller logic. The metrics show details about the reconciliation operations and the workqueue behavior on which reconciliation requests are enqueued. Metric Name Description Type Tags Unit Status work_queue_depth Depth of the work queue Gauge reconciler Dimensionless Stable reconcile_count Number of reconcile operations Counter reconciler success Dimensionless Stable reconcile_latency Latency of reconcile operations Histogram reconciler success Milliseconds Stable workqueue_adds_total Total number of adds handled by workqueue Counter name Dimensionless Stable workqueue_depth Current depth of workqueue Gauge reconciler Dimensionless Stable workqueue_queue_latency_seconds How long in seconds an item stays in workqueue before being requested Histogram name Seconds Stable workqueue_retries_total Total number of retries handled by workqueue Counter name Dimensionless Stable workqueue_work_duration_seconds How long in seconds processing an item from a workqueue takes. Histogram name Seconds Stable workqueue_unfinished_work_seconds How long in seconds the outstanding workqueue items have been in flight (total). Histogram name Seconds Stable workqueue_longest_running_processor_seconds How long in seconds the longest outstanding workqueue item has been in flight Histogram name Seconds Stable","title":"Controller"},{"location":"serving/observability/metrics/serving-metrics/#webhook","text":"Webhook metrics report useful info about operations. For example, if a large number of operations fail, this could indicate an issue with a user-created resource. Metric Name Description Type Tags Unit Status request_count The number of requests that are routed to webhook Counter admission_allowed kind_group kind_kind kind_version request_operation resource_group resource_namespace resource_resource resource_version Dimensionless Stable request_latencies The response time in milliseconds Histogram admission_allowed kind_group kind_kind kind_version request_operation resource_group resource_namespace resource_resource resource_version Milliseconds Stable","title":"Webhook"},{"location":"serving/observability/metrics/serving-metrics/#go-runtime-memstats","text":"Each Knative Serving control plane process emits a number of Go runtime memory statistics (shown next). As a baseline for monitoring purproses, user could start with a subset of the metrics: current allocations (go_alloc), total allocations (go_total_alloc), system memory (go_sys), mallocs (go_mallocs), frees (go_frees) and garbage collection total pause time (total_gc_pause_ns), next gc target heap size (go_next_gc) and number of garbage collection cycles (num_gc). Metric Name Description Type Tags Unit Status go_alloc The number of bytes of allocated heap objects (same as heap_alloc) Gauge name Dimensionless Stable go_total_alloc The cumulative bytes allocated for heap objects Gauge name Dimensionless Stable go_sys The total bytes of memory obtained from the OS Gauge name Dimensionless Stable go_lookups The number of pointer lookups performed by the runtime Gauge name Dimensionless Stable go_mallocs The cumulative count of heap objects allocated Gauge name Dimensionless Stable go_frees The cumulative count of heap objects freed Gauge name Dimensionless Stable go_heap_alloc The number of bytes of allocated heap objects Gauge name Dimensionless Stable go_heap_sys The number of bytes of heap memory obtained from the OS Gauge name Dimensionless Stable go_heap_idle The number of bytes in idle (unused) spans Gauge name Dimensionless Stable go_heap_in_use The number of bytes in in-use spans Gauge name Dimensionless Stable go_heap_released The number of bytes of physical memory returned to the OS Gauge name Dimensionless Stable go_heap_objects The number of allocated heap objects Gauge name Dimensionless Stable go_stack_in_use The number of bytes in stack spans Gauge name Dimensionless Stable go_stack_sys The number of bytes of stack memory obtained from the OS Gauge name Dimensionless Stable go_mspan_in_use The number of bytes of allocated mspan structures Gauge name Dimensionless Stable go_mspan_sys The number of bytes of memory obtained from the OS for mspan structures Gauge name Dimensionless Stable go_mcache_in_use The number of bytes of allocated mcache structures Gauge name Dimensionless Stable go_mcache_sys The number of bytes of memory obtained from the OS for mcache structures Gauge name Dimensionless Stable go_bucket_hash_sys The number of bytes of memory in profiling bucket hash tables. Gauge name Dimensionless Stable go_gc_sys The number of bytes of memory in garbage collection metadata Gauge name Dimensionless Stable go_other_sys The number of bytes of memory in miscellaneous off-heap runtime allocations Gauge name Dimensionless Stable go_next_gc The target heap size of the next GC cycle Gauge name Dimensionless Stable go_last_gc The time the last garbage collection finished, as nanoseconds since 1970 (the UNIX epoch) Gauge name Nanoseconds Stable go_total_gc_pause_ns The cumulative nanoseconds in GC stop-the-world pauses since the program started Gauge name Nanoseconds Stable go_num_gc The number of completed GC cycles. Gauge name Dimensionless Stable go_num_forced_gc The number of GC cycles that were forced by the application calling the GC function. Gauge name Dimensionless Stable go_gc_cpu_fraction The fraction of this program's available CPU time used by the GC since the program started Gauge name Dimensionless Stable Note The name tag is empty.","title":"Go Runtime - memstats"},{"location":"serving/services/","text":"About Knative Services \u00b6 Knative Services are used to deploy an application. To create an application using Knative, you must create a YAML file that defines a Service. This YAML file specifies metadata about the application, points to the hosted image of the app, and allows the Service to be configured. Each Service is defined by a Route and a Configuration that have the same name as the service. The Configuration and Route are created by the service controller, and derive their configuration from the configuration of the Service. Each time the configuration is updated, a new Revision is created. Revisions are immutable snapshots of a particular configuration, and use underlying Kubernetes resources to scale the number of pods based on traffic. Modifying Knative services \u00b6 Any changes to specifications, metadata labels, or metadata annotations for a Service must be copied to the Route and Configuration owned by that Service. The serving.knative.dev/service label on the Route and Configuration must also be set to the name of the Service. Any additional labels or annotations on the Route and Configuration not specified earlier must be removed. The Service updates its status fields based on the corresponding status value for the owned Route and Configuration. The Service must include conditions of RoutesReady and ConfigurationsReady in addition to the generic Ready condition. Other conditions can also be present. Additional resources \u00b6 For more information about the Knative Service object, see the Resource Types documentation.","title":"\u5173\u4e8eKnative Services"},{"location":"serving/services/#about-knative-services","text":"Knative Services are used to deploy an application. To create an application using Knative, you must create a YAML file that defines a Service. This YAML file specifies metadata about the application, points to the hosted image of the app, and allows the Service to be configured. Each Service is defined by a Route and a Configuration that have the same name as the service. The Configuration and Route are created by the service controller, and derive their configuration from the configuration of the Service. Each time the configuration is updated, a new Revision is created. Revisions are immutable snapshots of a particular configuration, and use underlying Kubernetes resources to scale the number of pods based on traffic.","title":"About Knative Services"},{"location":"serving/services/#modifying-knative-services","text":"Any changes to specifications, metadata labels, or metadata annotations for a Service must be copied to the Route and Configuration owned by that Service. The serving.knative.dev/service label on the Route and Configuration must also be set to the name of the Service. Any additional labels or annotations on the Route and Configuration not specified earlier must be removed. The Service updates its status fields based on the corresponding status value for the owned Route and Configuration. The Service must include conditions of RoutesReady and ConfigurationsReady in addition to the generic Ready condition. Other conditions can also be present.","title":"Modifying Knative services"},{"location":"serving/services/#additional-resources","text":"For more information about the Knative Service object, see the Resource Types documentation.","title":"Additional resources"},{"location":"serving/services/byo-certificate/","text":"Using a custom TLS certificate for DomainMapping \u00b6 Feature Availability: beta since Knative v0.24 beta features are well-tested and enabling them is considered safe. Support for the overall feature will not be dropped, though details may change in incompatible ways. By providing the reference to an existing TLS Certificate you can instruct a DomainMapping to use that certificate to secure the mapped service. Using this feature skips autoTLS certificate creation. Prerequisites \u00b6 You have followed the steps from Configuring custom domains and now have a working DomainMapping . You must have a TLS certificate from your Certificate Authority provider or self-signed. Procedure \u00b6 Assuming you have obtained the cert and key files from your Certificate Authority provider or self-signed, create a plain Kubernetes TLS Secret by running the command: Use kubectl to create the secret: kubectl create secret tls <tls-secret-name> --cert = path/to/cert/file --key = path/to/key/file Where <tls-secret-name> is the name of the secret object being created. Update your DomainMapping YAML file to use the newly created secret as follows: apiVersion : serving.knative.dev/v1alpha1 kind : DomainMapping metadata : name : <domain-name> namespace : <namespace> spec : ref : name : <service-name> kind : Service apiVersion : serving.knative.dev/v1 # tls block specifies the secret to be used tls : secretName : <tls-secret-name> Where: <tls-secret-name> is the name of the TLS secret created in the previous step. <domain-name> is the domain name that you want to map a Service to. <namespace> is the namespace that contains both the DomainMapping and Service objects. <service-name> is the name of the Service that will be mapped to the domain. Verify the DomainMapping status: Check the status by running the command: kubectl get domainmapping <domain-name> The URL column of the status should show the mapped domain with the scheme updated to https : NAME URL READY REASON <domain-name> https://<domain-name> True If the Service is exposed publicly, verify that it is available by running: curl https://<domain-name> If the certificate is self-signed skip verification by adding the -k flag to the curl command.","title":"Using a custom TLS certificate for DomainMapping"},{"location":"serving/services/byo-certificate/#using-a-custom-tls-certificate-for-domainmapping","text":"Feature Availability: beta since Knative v0.24 beta features are well-tested and enabling them is considered safe. Support for the overall feature will not be dropped, though details may change in incompatible ways. By providing the reference to an existing TLS Certificate you can instruct a DomainMapping to use that certificate to secure the mapped service. Using this feature skips autoTLS certificate creation.","title":"Using a custom TLS certificate for DomainMapping"},{"location":"serving/services/byo-certificate/#prerequisites","text":"You have followed the steps from Configuring custom domains and now have a working DomainMapping . You must have a TLS certificate from your Certificate Authority provider or self-signed.","title":"Prerequisites"},{"location":"serving/services/byo-certificate/#procedure","text":"Assuming you have obtained the cert and key files from your Certificate Authority provider or self-signed, create a plain Kubernetes TLS Secret by running the command: Use kubectl to create the secret: kubectl create secret tls <tls-secret-name> --cert = path/to/cert/file --key = path/to/key/file Where <tls-secret-name> is the name of the secret object being created. Update your DomainMapping YAML file to use the newly created secret as follows: apiVersion : serving.knative.dev/v1alpha1 kind : DomainMapping metadata : name : <domain-name> namespace : <namespace> spec : ref : name : <service-name> kind : Service apiVersion : serving.knative.dev/v1 # tls block specifies the secret to be used tls : secretName : <tls-secret-name> Where: <tls-secret-name> is the name of the TLS secret created in the previous step. <domain-name> is the domain name that you want to map a Service to. <namespace> is the namespace that contains both the DomainMapping and Service objects. <service-name> is the name of the Service that will be mapped to the domain. Verify the DomainMapping status: Check the status by running the command: kubectl get domainmapping <domain-name> The URL column of the status should show the mapped domain with the scheme updated to https : NAME URL READY REASON <domain-name> https://<domain-name> True If the Service is exposed publicly, verify that it is available by running: curl https://<domain-name> If the certificate is self-signed skip verification by adding the -k flag to the curl command.","title":"Procedure"},{"location":"serving/services/certificate-class/","text":"Configuring a custom certificate class for a Service \u00b6 When autoTLS is enabled and Knative Services are created, a certificate class ( certificate-class ) is automatically chosen based on the value in the config-network ConfigMap located inside the knative-serving namespace. This ConfigMap is part of Knative Serving installation. If the certificate class is not specified, this defaults to cert-manager.certificate.networking.knative.dev . After certificate-class is configured, it is used for all Knative Services unless it is overridden with a certificate-class annotation. Using the certificate class annotation \u00b6 Generally it is recommended for Knative Services to use the default certificate-class . However, in scenarios where there are multiple certificate providers, you might want to specify different certificate class annotations for each Service. You can configure each Service to use a different certificate class by specifying the networking.knative.dev/certificate-class annotation. To add a certificate class annotation to a Service, run the following command: kubectl annotate kservice <service-name> networking.knative.dev/certifcate-class = <certificate-provider> Where: <service-name> is the name of the Service that you are applying the annotation to. <certificate-provider> is the type of certificate provider that is used as the certificate class for the Service.","title":"Configuring certificate class"},{"location":"serving/services/certificate-class/#configuring-a-custom-certificate-class-for-a-service","text":"When autoTLS is enabled and Knative Services are created, a certificate class ( certificate-class ) is automatically chosen based on the value in the config-network ConfigMap located inside the knative-serving namespace. This ConfigMap is part of Knative Serving installation. If the certificate class is not specified, this defaults to cert-manager.certificate.networking.knative.dev . After certificate-class is configured, it is used for all Knative Services unless it is overridden with a certificate-class annotation.","title":"Configuring a custom certificate class for a Service"},{"location":"serving/services/certificate-class/#using-the-certificate-class-annotation","text":"Generally it is recommended for Knative Services to use the default certificate-class . However, in scenarios where there are multiple certificate providers, you might want to specify different certificate class annotations for each Service. You can configure each Service to use a different certificate class by specifying the networking.knative.dev/certificate-class annotation. To add a certificate class annotation to a Service, run the following command: kubectl annotate kservice <service-name> networking.knative.dev/certifcate-class = <certificate-provider> Where: <service-name> is the name of the Service that you are applying the annotation to. <certificate-provider> is the type of certificate provider that is used as the certificate class for the Service.","title":"Using the certificate class annotation"},{"location":"serving/services/configure-requests-limits-services/","text":"Configure resource requests and limits \u00b6 You can configure resource limits and requests, specifically for CPU and memory, for individual Knative services. The following example shows how you can set the requests and limits fields for a service: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : example-service namespace : default spec : template : spec : containers : - image : docker.io/user/example-app resources : requests : cpu : 100m memory : 640M limits : cpu : 1 Additional resources \u00b6 For more information requests and limits for Kubernetes resources, see Managing Resources for Containers .","title":"Configure resource requests and limits"},{"location":"serving/services/configure-requests-limits-services/#configure-resource-requests-and-limits","text":"You can configure resource limits and requests, specifically for CPU and memory, for individual Knative services. The following example shows how you can set the requests and limits fields for a service: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : example-service namespace : default spec : template : spec : containers : - image : docker.io/user/example-app resources : requests : cpu : 100m memory : 640M limits : cpu : 1","title":"Configure resource requests and limits"},{"location":"serving/services/configure-requests-limits-services/#additional-resources","text":"For more information requests and limits for Kubernetes resources, see Managing Resources for Containers .","title":"Additional resources"},{"location":"serving/services/creating-services/","text":"Creating a Service \u00b6 You can create a Knative service by applying a YAML file or using the kn service create CLI command. Prerequisites \u00b6 To create a Knative service, you will need: A Kubernetes cluster with Knative Serving installed. For more information, see Installing Knative Serving . Optional: To use the kn service create command, you must install the kn CLI . Procedure \u00b6 Tip The following commands create a helloworld-go sample service. You can modify these commands, including the container image URL, to deploy your own application as a Knative service. Create a sample service: Apply YAML kn CLI Create a YAML file using the following example: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go env : - name : TARGET value : \"Go Sample v1\" Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. kn service create helloworld-go --image gcr.io/knative-samples/helloworld-go After the service has been created, Knative performs the following tasks: Creates a new immutable revision for this version of the app. Performs network programming to create a route, ingress, service, and load balancer for your app. Automatically scales your pods up and down based on traffic, including to zero active pods.","title":"\u521b\u5efa\u4e00\u4e2aService"},{"location":"serving/services/creating-services/#creating-a-service","text":"You can create a Knative service by applying a YAML file or using the kn service create CLI command.","title":"Creating a Service"},{"location":"serving/services/creating-services/#prerequisites","text":"To create a Knative service, you will need: A Kubernetes cluster with Knative Serving installed. For more information, see Installing Knative Serving . Optional: To use the kn service create command, you must install the kn CLI .","title":"Prerequisites"},{"location":"serving/services/creating-services/#procedure","text":"Tip The following commands create a helloworld-go sample service. You can modify these commands, including the container image URL, to deploy your own application as a Knative service. Create a sample service: Apply YAML kn CLI Create a YAML file using the following example: apiVersion : serving.knative.dev/v1 kind : Service metadata : name : helloworld-go namespace : default spec : template : spec : containers : - image : gcr.io/knative-samples/helloworld-go env : - name : TARGET value : \"Go Sample v1\" Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. kn service create helloworld-go --image gcr.io/knative-samples/helloworld-go After the service has been created, Knative performs the following tasks: Creates a new immutable revision for this version of the app. Performs network programming to create a route, ingress, service, and load balancer for your app. Automatically scales your pods up and down based on traffic, including to zero active pods.","title":"Procedure"},{"location":"serving/services/custom-domains/","text":"Configuring custom domains \u00b6 Feature Availability: beta since Knative v0.24 beta features are well-tested and enabling them is considered safe. Support for the overall feature will not be dropped, though details may change in incompatible ways. Each Knative Service is automatically assigned a default domain name when it is created. However, you can map any custom domain name that you own to a Knative Service, by using domain mapping . You can create a DomainMapping object to map a single, non-wildcard domain to a specific Knative Service. For example, if you own the domain name example.org , and you configure the domain DNS to reference your Knative cluster, you can use DomainMapping to serve a Knative Service at this domain. Note If you create a domain mapping to map to a private Knative Service , the private Knative Service is accessible from public internet with the custom domain of the domain mapping. Tip This topic instructs how to customize the domain of each service, regardless of the default domain. If you want to customize the domain template to assign the default domain name, see Changing the default domain . Prerequisites \u00b6 You must have access to a Kubernetes cluster, with Knative Serving and an Ingress implementation installed. For more information, see the Serving Installation documentation . You must have the domain mapping feature enabled on your cluster. You must have access to a Knative service that you can map a domain to. You must own or have access to a domain name to map, and be able to change the domain DNS to point to your Knative cluster by using the tools provided by your domain registrar. Procedure \u00b6 To create a DomainMapping, you must first have a ClusterDomainClaim. This ClusterDomainClaim delegates the domain name to the namespace you want to create the DomainMapping in, which enables DomainMappings in that namespace to use the domain name. Create a ClusterDomainClaim manually or configure automatic creation of ClusterDomainClaims: To create a ClusterDomainClaim manually: Create a YAML file using the following template: apiVersion : networking.internal.knative.dev/v1alpha1 kind : ClusterDomainClaim metadata : name : <domain-name> spec : namespace : <namespace> Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To create ClusterDomainClaims automatically: set the autocreate-cluster-domain-claims property to true in the config-network ConfigMap in the knative-serving namespace. This allows any user, in any namespace, to map any domain name, including ones in other namespaces or for domain names that they do not own. Create a DomainMapping object: YAML kn Create a YAML file using the following template: apiVersion : serving.knative.dev/v1alpha1 kind : DomainMapping metadata : name : <domain-name> namespace : <namespace> spec : ref : name : <service-name> kind : Service apiVersion : serving.knative.dev/v1 tls : secretName : <cert-secret> Where: <domain-name> is the domain name that you want to map a Service to. <namespace> is the namespace that contains both the DomainMapping and Service objects. <service-name> is the name of the Service that is mapped to the domain. <cert-secret> is the name of a Secret that holds the server certificate for TLS communication. If this optional tls: section is provided, the protocol is switched from HTTP to HTTPS. Tip You can also map to other targets as long as they conform to the Addressable contract and their resolved URL is of the form <name>.<namespace>.<clusterdomain> , where <name> and <namespace> are the name and namespace of a Kubernetes Service, and <clusterdomain> is the cluster domain. Examples of objects that conform to this contract include Knative Services, Routes, and Kubernetes Services. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Run the command: kn domain create <domain-name> --ref <target> --tls <tls-secret> --namespace <namespace> Where: <domain-name> is the domain name that you want to map a Service or Route to. <target> is the name of the Service or Route that is mapped to the domain. You can use the prefix ksvc: or kroute: to specify whether to map the domain to a Knative Service or Route. If no prefix is given, ksvc: is assumed. Additionally, you can use a :namespace suffix to point to a Service or Route in a different namespace. Examples: mysvc maps to a Service mysvc in the same namespace as this mapping. kroute:myroute:othernamespace maps to a Route myroute in namespace othernamespace . <tls-secret> is optional and if provided enables the TLS protocol. The value specifies the secret that holds the server certificate. <namespace> is the namespace where you want to create the DomainMapping. By default the DomainMapping is created in the current namespace. Note In addition to creating DomainMappings, you can use the kn domain command to list, describe, update, and delete existing DomainMappings. For more information about the command, run kn domain --help . Point the domain name to the IP address of your Knative cluster. Details of this step differ depending on your domain registrar.","title":"Configuring custom domains"},{"location":"serving/services/custom-domains/#configuring-custom-domains","text":"Feature Availability: beta since Knative v0.24 beta features are well-tested and enabling them is considered safe. Support for the overall feature will not be dropped, though details may change in incompatible ways. Each Knative Service is automatically assigned a default domain name when it is created. However, you can map any custom domain name that you own to a Knative Service, by using domain mapping . You can create a DomainMapping object to map a single, non-wildcard domain to a specific Knative Service. For example, if you own the domain name example.org , and you configure the domain DNS to reference your Knative cluster, you can use DomainMapping to serve a Knative Service at this domain. Note If you create a domain mapping to map to a private Knative Service , the private Knative Service is accessible from public internet with the custom domain of the domain mapping. Tip This topic instructs how to customize the domain of each service, regardless of the default domain. If you want to customize the domain template to assign the default domain name, see Changing the default domain .","title":"Configuring custom domains"},{"location":"serving/services/custom-domains/#prerequisites","text":"You must have access to a Kubernetes cluster, with Knative Serving and an Ingress implementation installed. For more information, see the Serving Installation documentation . You must have the domain mapping feature enabled on your cluster. You must have access to a Knative service that you can map a domain to. You must own or have access to a domain name to map, and be able to change the domain DNS to point to your Knative cluster by using the tools provided by your domain registrar.","title":"Prerequisites"},{"location":"serving/services/custom-domains/#procedure","text":"To create a DomainMapping, you must first have a ClusterDomainClaim. This ClusterDomainClaim delegates the domain name to the namespace you want to create the DomainMapping in, which enables DomainMappings in that namespace to use the domain name. Create a ClusterDomainClaim manually or configure automatic creation of ClusterDomainClaims: To create a ClusterDomainClaim manually: Create a YAML file using the following template: apiVersion : networking.internal.knative.dev/v1alpha1 kind : ClusterDomainClaim metadata : name : <domain-name> spec : namespace : <namespace> Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. To create ClusterDomainClaims automatically: set the autocreate-cluster-domain-claims property to true in the config-network ConfigMap in the knative-serving namespace. This allows any user, in any namespace, to map any domain name, including ones in other namespaces or for domain names that they do not own. Create a DomainMapping object: YAML kn Create a YAML file using the following template: apiVersion : serving.knative.dev/v1alpha1 kind : DomainMapping metadata : name : <domain-name> namespace : <namespace> spec : ref : name : <service-name> kind : Service apiVersion : serving.knative.dev/v1 tls : secretName : <cert-secret> Where: <domain-name> is the domain name that you want to map a Service to. <namespace> is the namespace that contains both the DomainMapping and Service objects. <service-name> is the name of the Service that is mapped to the domain. <cert-secret> is the name of a Secret that holds the server certificate for TLS communication. If this optional tls: section is provided, the protocol is switched from HTTP to HTTPS. Tip You can also map to other targets as long as they conform to the Addressable contract and their resolved URL is of the form <name>.<namespace>.<clusterdomain> , where <name> and <namespace> are the name and namespace of a Kubernetes Service, and <clusterdomain> is the cluster domain. Examples of objects that conform to this contract include Knative Services, Routes, and Kubernetes Services. Apply the YAML file by running the command: kubectl apply -f <filename>.yaml Where <filename> is the name of the file you created in the previous step. Run the command: kn domain create <domain-name> --ref <target> --tls <tls-secret> --namespace <namespace> Where: <domain-name> is the domain name that you want to map a Service or Route to. <target> is the name of the Service or Route that is mapped to the domain. You can use the prefix ksvc: or kroute: to specify whether to map the domain to a Knative Service or Route. If no prefix is given, ksvc: is assumed. Additionally, you can use a :namespace suffix to point to a Service or Route in a different namespace. Examples: mysvc maps to a Service mysvc in the same namespace as this mapping. kroute:myroute:othernamespace maps to a Route myroute in namespace othernamespace . <tls-secret> is optional and if provided enables the TLS protocol. The value specifies the secret that holds the server certificate. <namespace> is the namespace where you want to create the DomainMapping. By default the DomainMapping is created in the current namespace. Note In addition to creating DomainMappings, you can use the kn domain command to list, describe, update, and delete existing DomainMappings. For more information about the command, run kn domain --help . Point the domain name to the IP address of your Knative cluster. Details of this step differ depending on your domain registrar.","title":"Procedure"},{"location":"serving/services/http-protocol/","text":"HTTPS redirection \u00b6 Operators can force HTTPS redirection for all Services. See the http-protocol mentioned in the Turn on AutoTLS page for more details. Overriding the default HTTP behavior \u00b6 You can override the default behavior for each Service or global configuration. Global key: http-protocol Per-revision annotation key: networking.knative.dev/http-protocol Possible values: enabled \u2014 Services accept HTTP traffic. redirected \u2014 Services send a 301 redirect for all HTTP connections and ask clients to use HTTPS instead. Default: enabled Example: Per Service Global (ConfigMap) Global (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : example namespace : default annotations : networking.knative.dev/http-protocol : \"redirected\" spec : ... apiVersion : v1 kind : ConfigMap metadata : name : config-network namespace : knative-serving data : http-protocol : \"redirected\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : network : http-protocol : \"redirected\"","title":"HTTPS redirection"},{"location":"serving/services/http-protocol/#https-redirection","text":"Operators can force HTTPS redirection for all Services. See the http-protocol mentioned in the Turn on AutoTLS page for more details.","title":"HTTPS redirection"},{"location":"serving/services/http-protocol/#overriding-the-default-http-behavior","text":"You can override the default behavior for each Service or global configuration. Global key: http-protocol Per-revision annotation key: networking.knative.dev/http-protocol Possible values: enabled \u2014 Services accept HTTP traffic. redirected \u2014 Services send a 301 redirect for all HTTP connections and ask clients to use HTTPS instead. Default: enabled Example: Per Service Global (ConfigMap) Global (Operator) apiVersion : serving.knative.dev/v1 kind : Service metadata : name : example namespace : default annotations : networking.knative.dev/http-protocol : \"redirected\" spec : ... apiVersion : v1 kind : ConfigMap metadata : name : config-network namespace : knative-serving data : http-protocol : \"redirected\" apiVersion : operator.knative.dev/v1alpha1 kind : KnativeServing metadata : name : knative-serving spec : config : network : http-protocol : \"redirected\"","title":"Overriding the default HTTP behavior"},{"location":"serving/services/ingress-class/","text":"Configuring Services custom ingress class \u00b6 When a Knative Service is created an ingress class ( ingress-class ) is automatically assigned to it, based on the value in the config-network ConfigMap located inside the knative-serving namespace. This ConfigMap is part of Knative Serving installation. If the ingress class is not specified, this defaults to istio.ingress.networking.knative.dev . Once configured the ingress-class is used for all Knative Services unless it is overridden with an ingress-class annotation. Warning Changing the ingress class in config-network ConfigMap will only affect newly created Services Using the ingress class annotation \u00b6 Generally it is recommended for Knative Services to use the default ingress-class . However, in scenarios where there are multiple networking implementations, you might want to specify different ingress class annotations for each Service. You can configure each Service to use a different ingress class by specifying the networking.knative.dev/ingress-class annotation. To add an ingress class annotation to a Service, run the following command: kubectl annotate kservice <service-name> networking.knative.dev/ingress-class = <ingress-type> Where: <service-name> is the name of the Service that you are applying the annotation to. <ingress-type> is the type of ingress that is used as the ingress class for the Service. Note This annotation overrides the ingress-class value specified in the config-network ConfigMap.","title":"Configuring ingress class"},{"location":"serving/services/ingress-class/#configuring-services-custom-ingress-class","text":"When a Knative Service is created an ingress class ( ingress-class ) is automatically assigned to it, based on the value in the config-network ConfigMap located inside the knative-serving namespace. This ConfigMap is part of Knative Serving installation. If the ingress class is not specified, this defaults to istio.ingress.networking.knative.dev . Once configured the ingress-class is used for all Knative Services unless it is overridden with an ingress-class annotation. Warning Changing the ingress class in config-network ConfigMap will only affect newly created Services","title":"Configuring Services custom ingress class"},{"location":"serving/services/ingress-class/#using-the-ingress-class-annotation","text":"Generally it is recommended for Knative Services to use the default ingress-class . However, in scenarios where there are multiple networking implementations, you might want to specify different ingress class annotations for each Service. You can configure each Service to use a different ingress class by specifying the networking.knative.dev/ingress-class annotation. To add an ingress class annotation to a Service, run the following command: kubectl annotate kservice <service-name> networking.knative.dev/ingress-class = <ingress-type> Where: <service-name> is the name of the Service that you are applying the annotation to. <ingress-type> is the type of ingress that is used as the ingress class for the Service. Note This annotation overrides the ingress-class value specified in the config-network ConfigMap.","title":"Using the ingress class annotation"},{"location":"serving/services/private-services/","text":"Configuring private Services \u00b6 By default, Services deployed through Knative are published to an external IP address, making them public Services on a public IP address and with a public URL. Knative provides two ways to enable private services which are only available inside the cluster: To make all Knative Services private, change the default domain to svc.cluster.local by editing the config-domain ConfigMap . This changes all Services deployed through Knative to only be published to the cluster. To make an individual Service private, the Service or Route can be labelled with networking.knative.dev/visibility=cluster-local so that it is not published to the external gateway. Using the cluster-local label \u00b6 To configure a Knative Service so that it is only available on the cluster-local network, and not on the public internet, you can apply the networking.knative.dev/visibility=cluster-local label to a Knative Service, a route or a Kubernetes Service object. To label a Knative Service: kubectl label kservice ${ KSVC_NAME } networking.knative.dev/visibility = cluster-local By labeling the Kubernetes Service you can restrict visibility in a more fine-grained way. See Traffic management for information about tagged routes. To label a Route when the Route is used directly without a Knative Service: kubectl label route ${ ROUTE_NAME } networking.knative.dev/visibility = cluster-local To label a Kubernetes Service: kubectl label service ${ SERVICE_NAME } networking.knative.dev/visibility = cluster-local Example \u00b6 You can deploy the Hello World sample and then convert it to be an cluster-local Service by labelling the Service: kubectl label kservice helloworld-go networking.knative.dev/visibility = cluster-local You can then verify that the change has been made by verifying the URL for the helloworld-go Service: kubectl get kservice helloworld-go NAME URL LATESTCREATED LATESTREADY READY REASON helloworld-go http://helloworld-go.default.svc.cluster.local helloworld-go-2bz5l helloworld-go-2bz5l True The Service returns the a URL with the svc.cluster.local domain, indicating the Service is only available in the cluster-local network.","title":"\u914d\u7f6e\u79c1\u6709Services"},{"location":"serving/services/private-services/#configuring-private-services","text":"By default, Services deployed through Knative are published to an external IP address, making them public Services on a public IP address and with a public URL. Knative provides two ways to enable private services which are only available inside the cluster: To make all Knative Services private, change the default domain to svc.cluster.local by editing the config-domain ConfigMap . This changes all Services deployed through Knative to only be published to the cluster. To make an individual Service private, the Service or Route can be labelled with networking.knative.dev/visibility=cluster-local so that it is not published to the external gateway.","title":"Configuring private Services"},{"location":"serving/services/private-services/#using-the-cluster-local-label","text":"To configure a Knative Service so that it is only available on the cluster-local network, and not on the public internet, you can apply the networking.knative.dev/visibility=cluster-local label to a Knative Service, a route or a Kubernetes Service object. To label a Knative Service: kubectl label kservice ${ KSVC_NAME } networking.knative.dev/visibility = cluster-local By labeling the Kubernetes Service you can restrict visibility in a more fine-grained way. See Traffic management for information about tagged routes. To label a Route when the Route is used directly without a Knative Service: kubectl label route ${ ROUTE_NAME } networking.knative.dev/visibility = cluster-local To label a Kubernetes Service: kubectl label service ${ SERVICE_NAME } networking.knative.dev/visibility = cluster-local","title":"Using the cluster-local label"},{"location":"serving/services/private-services/#example","text":"You can deploy the Hello World sample and then convert it to be an cluster-local Service by labelling the Service: kubectl label kservice helloworld-go networking.knative.dev/visibility = cluster-local You can then verify that the change has been made by verifying the URL for the helloworld-go Service: kubectl get kservice helloworld-go NAME URL LATESTCREATED LATESTREADY READY REASON helloworld-go http://helloworld-go.default.svc.cluster.local helloworld-go-2bz5l helloworld-go-2bz5l True The Service returns the a URL with the svc.cluster.local domain, indicating the Service is only available in the cluster-local network.","title":"Example"},{"location":"serving/services/service-metrics/","text":"Service metrics \u00b6 Every Knative Service has a proxy container that proxies the connections to the application container. A number of metrics are reported for the queue proxy performance. Using the following metrics, you can measure if requests are queued at the proxy side (need for backpressure) and what is the actual delay in serving requests at the application side. Queue proxy metrics \u00b6 Requests endpoint. Metric Name Description Type Tags Unit Status revision_request_count The number of requests that are routed to queue-proxy Counter configuration_name container_name namespace_name pod_name response_code response_code_class revision_name service_name Dimensionless Stable revision_request_latencies The response time in millisecond Histogram configuration_name container_name namespace_name pod_name response_code response_code_class revision_name service_name Milliseconds Stable revision_app_request_count The number of requests that are routed to user-container Counter configuration_name container_name namespace_name pod_name response_code response_code_class revision_name service_name Dimensionless Stable revision_app_request_latencies The response time in millisecond Histogram configuration_name namespace_name pod_name response_code response_code_class revision_name service_name Milliseconds Stable revision_queue_depth The current number of items in the serving and waiting queue, or not reported if unlimited concurrency Gauge configuration_name event-display container_name namespace_name pod_name response_code_class revision_name service_name Dimensionless Stable","title":"Service\u6307\u6807"},{"location":"serving/services/service-metrics/#service-metrics","text":"Every Knative Service has a proxy container that proxies the connections to the application container. A number of metrics are reported for the queue proxy performance. Using the following metrics, you can measure if requests are queued at the proxy side (need for backpressure) and what is the actual delay in serving requests at the application side.","title":"Service metrics"},{"location":"serving/services/service-metrics/#queue-proxy-metrics","text":"Requests endpoint. Metric Name Description Type Tags Unit Status revision_request_count The number of requests that are routed to queue-proxy Counter configuration_name container_name namespace_name pod_name response_code response_code_class revision_name service_name Dimensionless Stable revision_request_latencies The response time in millisecond Histogram configuration_name container_name namespace_name pod_name response_code response_code_class revision_name service_name Milliseconds Stable revision_app_request_count The number of requests that are routed to user-container Counter configuration_name container_name namespace_name pod_name response_code response_code_class revision_name service_name Dimensionless Stable revision_app_request_latencies The response time in millisecond Histogram configuration_name namespace_name pod_name response_code response_code_class revision_name service_name Milliseconds Stable revision_queue_depth The current number of items in the serving and waiting queue, or not reported if unlimited concurrency Gauge configuration_name event-display container_name namespace_name pod_name response_code_class revision_name service_name Dimensionless Stable","title":"Queue proxy metrics"},{"location":"serving/troubleshooting/debugging-application-issues/","text":"Debugging application issues \u00b6 If you have deployed an application but are having issues, you can use the following steps to troubleshoot the application. Check terminal output \u00b6 Check your deploy command output to see whether it succeeded or not. If your deployment process was terminated, you should see an error message in the output that describes the reason why the deployment failed. This kind of failure is most likely due to either a misconfigured manifest or wrong command. For example, the following output says that you must configure route traffic percent to sum to 100: Error from server (InternalError): error when applying patch: {\"metadata\":{\"annotations\":{\"kubectl.kubernetes.io/last-applied-configuration\":\"{\\\"apiVersion\\\":\\\"serving.knative.dev/v1\\\",\\\"kind\\\":\\\"Route\\\",\\\"metadata\\\":{\\\"annotations\\\":{},\\\"name\\\":\\\"route-example\\\",\\\"namespace\\\":\\\"default\\\"},\\\"spec\\\":{\\\"traffic\\\":[{\\\"configurationName\\\":\\\"configuration-example\\\",\\\"percent\\\":50}]}}\\n\"}},\"spec\":{\"traffic\":[{\"configurationName\":\"configuration-example\",\"percent\":50}]}} to: &{0xc421d98240 0xc421e77490 default route-example STDIN 0xc421db0488 264682 false} for: \"STDIN\": Internal error occurred: admission webhook \"webhook.knative.dev\" denied the request: mutation failed: The route must have traffic percent sum equal to 100. ERROR: Non-zero return code '1' from command: Process exited with status 1 Check Route status \u00b6 Run the following command to get the status of the Route object with which you deployed your application: kubectl get route <route-name> --output yaml The conditions in status provide the reason if there is any failure. For details, see Knative Error Conditions and Reporting . Check Ingress/Istio routing \u00b6 To list all Ingress resources and their corresponding labels, run the following command: kubectl get ingresses.networking.internal.knative.dev -o = custom-columns = 'NAME:.metadata.name,LABELS:.metadata.labels' NAME LABELS helloworld-go map [ serving.knative.dev/route:helloworld-go serving.knative.dev/routeNamespace:default serving.knative.dev/service:helloworld-go ] The labels serving.knative.dev/route and serving.knative.dev/routeNamespace indicate the Route in which the Ingress resource resides. Your Route and Ingress should be listed. If your Ingress does not exist, the route controller believes that the Revisions targeted by your Route/Service isn't ready. Please proceed to later sections to diagnose Revision readiness status. Otherwise, run the following command to look at the ClusterIngress created for your Route kubectl get ingresses.networking.internal.knative.dev <INGRESS_NAME> --output yaml particularly, look at the status: section. If the Ingress is working correctly, we should see the condition with type=Ready to have status=True . Otherwise, there will be error messages. Now, if Ingress shows status Ready , there must be a corresponding VirtualService. Run the following command: kubectl get virtualservice -l networking.internal.knative.dev/ingress = <INGRESS_NAME> -n <INGRESS_NAMESPACE> --output yaml the network configuration in VirtualService must match that of Ingress and Route. VirtualService currently doesn't expose a Status field, so if one exists and have matching configurations with Ingress and Route, you may want to wait a little bit for those settings to propagate. If you are familar with Istio and istioctl , you may try using istioctl to look deeper using Istio guide . Check Ingress status \u00b6 Knative uses a LoadBalancer service called istio-ingressgateway Service. To check the IP address of your Ingress, use kubectl get svc -n istio-system istio-ingressgateway If there is no external IP address, use kubectl describe svc istio-ingressgateway -n istio-system to see a reason why IP addresses weren't provisioned. Most likely it is due to a quota issue. Check Revision status \u00b6 If you configure your Route with Configuration , run the following command to get the name of the Revision created for you deployment (look up the configuration name in the Route .yaml file): kubectl get configuration <configuration-name> --output jsonpath = \"{.status.latestCreatedRevisionName}\" If you configure your Route with Revision directly, look up the revision name in the Route yaml file. Then run the following command: kubectl get revision <revision-name> --output yaml A ready Revision should have the following condition in status : conditions : - reason : ServiceReady status : \"True\" type : Ready If you see this condition, check the following to continue debugging: Check Pod status Check Istio routing Tip If you see other conditions, you can look up the meaning of the conditions in Knative Error Conditions and Reporting . An alternative is to check Pod status . Check Pod status \u00b6 To get the Pod s for all your deployments: kubectl get pods This command should list all Pod s with brief status. For example: NAME READY STATUS RESTARTS AGE configuration-example-00001-deployment-659747ff99-9bvr4 2/2 Running 0 3h configuration-example-00002-deployment-5f475b7849-gxcht 1/2 CrashLoopBackOff 2 36s Choose one and use the following command to see detailed information for its status . Some useful fields are conditions and containerStatuses : kubectl get pod <pod-name> --output yaml","title":"Debugging application issues"},{"location":"serving/troubleshooting/debugging-application-issues/#debugging-application-issues","text":"If you have deployed an application but are having issues, you can use the following steps to troubleshoot the application.","title":"Debugging application issues"},{"location":"serving/troubleshooting/debugging-application-issues/#check-terminal-output","text":"Check your deploy command output to see whether it succeeded or not. If your deployment process was terminated, you should see an error message in the output that describes the reason why the deployment failed. This kind of failure is most likely due to either a misconfigured manifest or wrong command. For example, the following output says that you must configure route traffic percent to sum to 100: Error from server (InternalError): error when applying patch: {\"metadata\":{\"annotations\":{\"kubectl.kubernetes.io/last-applied-configuration\":\"{\\\"apiVersion\\\":\\\"serving.knative.dev/v1\\\",\\\"kind\\\":\\\"Route\\\",\\\"metadata\\\":{\\\"annotations\\\":{},\\\"name\\\":\\\"route-example\\\",\\\"namespace\\\":\\\"default\\\"},\\\"spec\\\":{\\\"traffic\\\":[{\\\"configurationName\\\":\\\"configuration-example\\\",\\\"percent\\\":50}]}}\\n\"}},\"spec\":{\"traffic\":[{\"configurationName\":\"configuration-example\",\"percent\":50}]}} to: &{0xc421d98240 0xc421e77490 default route-example STDIN 0xc421db0488 264682 false} for: \"STDIN\": Internal error occurred: admission webhook \"webhook.knative.dev\" denied the request: mutation failed: The route must have traffic percent sum equal to 100. ERROR: Non-zero return code '1' from command: Process exited with status 1","title":"Check terminal output"},{"location":"serving/troubleshooting/debugging-application-issues/#check-route-status","text":"Run the following command to get the status of the Route object with which you deployed your application: kubectl get route <route-name> --output yaml The conditions in status provide the reason if there is any failure. For details, see Knative Error Conditions and Reporting .","title":"Check Route status"},{"location":"serving/troubleshooting/debugging-application-issues/#check-ingressistio-routing","text":"To list all Ingress resources and their corresponding labels, run the following command: kubectl get ingresses.networking.internal.knative.dev -o = custom-columns = 'NAME:.metadata.name,LABELS:.metadata.labels' NAME LABELS helloworld-go map [ serving.knative.dev/route:helloworld-go serving.knative.dev/routeNamespace:default serving.knative.dev/service:helloworld-go ] The labels serving.knative.dev/route and serving.knative.dev/routeNamespace indicate the Route in which the Ingress resource resides. Your Route and Ingress should be listed. If your Ingress does not exist, the route controller believes that the Revisions targeted by your Route/Service isn't ready. Please proceed to later sections to diagnose Revision readiness status. Otherwise, run the following command to look at the ClusterIngress created for your Route kubectl get ingresses.networking.internal.knative.dev <INGRESS_NAME> --output yaml particularly, look at the status: section. If the Ingress is working correctly, we should see the condition with type=Ready to have status=True . Otherwise, there will be error messages. Now, if Ingress shows status Ready , there must be a corresponding VirtualService. Run the following command: kubectl get virtualservice -l networking.internal.knative.dev/ingress = <INGRESS_NAME> -n <INGRESS_NAMESPACE> --output yaml the network configuration in VirtualService must match that of Ingress and Route. VirtualService currently doesn't expose a Status field, so if one exists and have matching configurations with Ingress and Route, you may want to wait a little bit for those settings to propagate. If you are familar with Istio and istioctl , you may try using istioctl to look deeper using Istio guide .","title":"Check Ingress/Istio routing"},{"location":"serving/troubleshooting/debugging-application-issues/#check-ingress-status","text":"Knative uses a LoadBalancer service called istio-ingressgateway Service. To check the IP address of your Ingress, use kubectl get svc -n istio-system istio-ingressgateway If there is no external IP address, use kubectl describe svc istio-ingressgateway -n istio-system to see a reason why IP addresses weren't provisioned. Most likely it is due to a quota issue.","title":"Check Ingress status"},{"location":"serving/troubleshooting/debugging-application-issues/#check-revision-status","text":"If you configure your Route with Configuration , run the following command to get the name of the Revision created for you deployment (look up the configuration name in the Route .yaml file): kubectl get configuration <configuration-name> --output jsonpath = \"{.status.latestCreatedRevisionName}\" If you configure your Route with Revision directly, look up the revision name in the Route yaml file. Then run the following command: kubectl get revision <revision-name> --output yaml A ready Revision should have the following condition in status : conditions : - reason : ServiceReady status : \"True\" type : Ready If you see this condition, check the following to continue debugging: Check Pod status Check Istio routing Tip If you see other conditions, you can look up the meaning of the conditions in Knative Error Conditions and Reporting . An alternative is to check Pod status .","title":"Check Revision status"},{"location":"serving/troubleshooting/debugging-application-issues/#check-pod-status","text":"To get the Pod s for all your deployments: kubectl get pods This command should list all Pod s with brief status. For example: NAME READY STATUS RESTARTS AGE configuration-example-00001-deployment-659747ff99-9bvr4 2/2 Running 0 3h configuration-example-00002-deployment-5f475b7849-gxcht 1/2 CrashLoopBackOff 2 36s Choose one and use the following command to see detailed information for its status . Some useful fields are conditions and containerStatuses : kubectl get pod <pod-name> --output yaml","title":"Check Pod status"}]}